OTH	GLR* is a recently developed robust version of the Generalized LR Parser, that can parse almost any input sentence by ignoring unrecognizable parts of the sentence .	A-0
OTH	On a given input sentence , the parser returns a collection of parses that correspond to maximal , or close to maximal , parsable subsets of the original input .	A-1
AIM	This paper describes recent work on developing an integrated heuristic scheme for selecting the parse that is deemed `` best '' from such a collection .	A-2
OWN	We describe the heuristic measures used and their combination scheme .	A-3
OWN	Preliminary results from experiments conducted on parsing speech recognized spontaneous speech are also reported .	A-4
OTH	The Generalized LR Parser , developed by, extended the original LR parsing algorithm to the case of non-LR languages , where the parsing tables contain entries with multiple parsing actions .	S-0
OTH	's algorithm uses a Graph Structured Stack ( GSS ) in order to efficiently pursue in parallel the different parsing options that arise as a result of the multiple entries in the parsing tables .	S-1
OTH	A second data structure uses pointers to keep track of all possible parse trees throughout the parsing of the input , while sharing common subtrees of these different parses .	S-2
OTH	A process of local ambiguity packing allows the parser to pack sub-parses that are rooted in the same non-terminal into a single structure that represents them all .	S-3
OTH	The GLR parser is the syntactic engine of the Universal Parser Architecture developed at CMU.	S-4
OTH	The architecture supports grammatical specification in an LFG framework , that consists of context-free grammar rules augmented with feature bundles that are associated with the non-terminals of the rules .	S-5
OTH	Feature structure computation is , for the most part , specified and implemented via unification operations .	S-6
OTH	This allows the grammar to constrain the applicability of context-free rules .	S-7
OTH	The result of parsing an input sentence consists of both a parse tree and the computed feature structure associated with the non-terminal at the root of the tree .	S-8
BAS	GLR* is a recently developed robust version of the Generalized LR Parser , that allows the skipping of unrecognizable parts of the input sentence.	S-9
OTH	It is designed to enhance the parsability of domains such as spontaneous speech , where the input is likely to contain deviations from the grammar , due to either extra-grammaticalities or limited grammar coverage .	S-10
OTH	In cases where the complete input sentence is not covered by the grammar , the parser attempts to find a maximal subset of the input that is parsable .	S-11
OTH	In many cases , such a parse can serve as a good approximation to the true parse of the sentence .	S-12
OTH	The parser accommodates the skipping of words of the input string by allowing shift operations to be performed from inactive state nodes in the Graph Structured Stack ( GSS ) .	S-13
OTH	Shifting an input symbol from an inactive state is equivalent to skipping the words of the input that were encountered after the parser reached the inactive state and prior to the current word that is being shifted .	S-14
OTH	Since the parser is LR(0) , previous reduce operations remain valid even when words further along in the input are skipped .	S-15
OTH	Information about skipped words is maintained in the symbol nodes that represent parse sub-trees .	S-16
OTH	To guarantee runtime feasibility , the GLR* parser is coupled with a `` beam '' search heuristic , that dynamically restricts the skipping capability of the parser , so as to focus on parses of maximal and close to maximal substrings of the input .	S-17
OTH	The efficiency of the parser is also increased by an enhanced process of local ambiguity packing and pruning .	S-18
OTH	Locally ambiguous symbol nodes are compared in terms of the words skipped within them .	S-19
OTH	In cases where one phrase has more skipped words than the other , the phrase with more skipped words is discarded in favor of the more complete parsed phrase .	S-20
OTH	This operation significantly reduces the number of parses being pursued by the parser .	S-21
OTH	At the end of the process of parsing a sentence , the GLR* parser returns with a set of possible parses , each corresponding to some grammatical subset of words of the input sentence .	S-22
OTH	Due to the beam search heuristic and the ambiguity packing scheme , this set of parses is limited to maximal or close to maximal grammatical subsets .	S-23
OTH	The principle goal is then to find the maximal parsable subset of the input string ( and its parse ) .	S-24
CTR	However , in many cases there are several distinct maximal parses , each consisting of a different subset of words of the original sentence .	S-25
CTR	Furthermore , our experience has shown that in many cases , ignoring an additional one or two input words may result in a parse that is syntactically and / or semantically more coherent .	S-26
AIM	We have thus developed an evaluation heuristic that combines several different measures , in order to select the parse that is deemed overall `` best '' .	S-27
OWN	Our heuristic uses a set of features by which each of the parse candidates can be evaluated and compared .	S-28
OWN	We use features of both the candidate parse and the ignored parts of the original input sentence .	S-29
OWN	The features are designed to be general and , for the most part , grammar and domain independent .	S-30
OWN	For each parse , the heuristic computes a penalty score for each of the features .	S-31
OWN	The penalties of the different features are then combined into a single score using a linear combination .	S-32
OWN	The weights used in this scheme are adjustable , and can be optimized for a particular domain and / or grammar .	S-33
OWN	The parser then selects the parse ranked best ( i.e. the parse of lowest overall score ) .	S-34
OWN	So far , we have experimented with the following set of evaluation features :	S-35
OWN	The number and position of skipped words	S-36
OWN	The number of substituted words	S-37
OWN	The fragmentation of the parse analysis	S-38
OWN	The statistical score of the disambiguated parse tree .	S-39
OWN	The penalty scheme for skipped words is designed to prefer parses that correspond to fewer skipped words .	S-40
OWN	It assigns a penalty in the range of ( 0.95 - 1.05 ) for each word of the original sentence that was skipped .	S-41
OWN	The scheme is such that words that are skipped later in the sentence receive the slightly higher penalty .	S-42
OWN	This preference was designed to handle the phenomena of false starts , which is common in spontaneous speech .	S-43
OTH	The GLR* parser has a capability for handling common word substitutions when the parser 's input string is the output of a speech recognition system .	S-44
OTH	When the input contains a pre-determined commonly substituted word , the parser attempts to continue with both the original input word and a specified `` correct '' word .	S-45
OWN	The number of substituted words is used as an evaluation feature , so as to prefer an analysis with fewer substituted words .	S-46
OWN	The grammars we have been working with allow a single input sentence to be analyzed as several grammatical `` sentences '' or fragments .	S-47
OWN	Our experiments have indicated that , in most cases , a less fragmented analysis is more desirable .	S-48
OWN	We therefore use the sum of the number of fragments in the analysis as an additional feature .	S-49
OWN	We have recently augmented the parser with a statistical disambiguation module .	S-50
BAS	We use a framework similar to the one proposed by, in which the shift and reduce actions of the LR parsing tables are directly augmented with probabilities .	S-51
OWN	Training of the probabilities is performed on a set of disambiguated parses .	S-52
OWN	The probabilities of the parse actions induce statistical scores on alternative parse trees , which are used for disambiguation .	S-53
OWN	However , additionally , we use the statistical score of the disambiguated parse as an additional evaluation feature across parses .	S-54
OWN	The statistical score value is first converted into a confidence measure , such that more `` common '' parse trees receive a lower penalty score .	S-55
OWN	This is done using the following formula :	S-56
OWN	The penalty scores of the features are then combined by a linear combination .	S-57
OWN	The weights assigned to the features determine the way they interact .	S-58
OWN	In our experiments so far , we have fined tuned these weights manually , so as to try and optimize the results on a training set of data .	S-59
OWN	However , we plan on investigating the possibility of using some known optimization techniques for this task .	S-60
OWN	The utility of a parser such as GLR* obviously depends on the semantic coherency of the parse results that it returns .	S-61
OWN	Since the parser is designed to succeed in parsing almost any input , parsing success by itself can no longer provide a likely guarantee of such coherency .	S-62
OWN	Although we believe this task would ultimately be better handled by a domain dependent semantic analyzer that would follow the parser , we have attempted to partially handle this problem using a simple filtering scheme .	S-63
OWN	The filtering scheme 's task is to classify the parse chosen as best by the parser into one of two categories : `` good '' or `` bad '' .	S-64
OWN	Our heuristic takes into account both the actual value of the parse 's combined penalty score and a measure relative to the length of the input sentence .	S-65
OWN	Similar to the penalty score scheme , the precise thresholds are currently fine tuned to try and optimize the classification results on a training set of data .	S-66
OWN	We have recently conducted some new experiments to test the utility of the GLR* parser and our parse evaluation heuristics when parsing speech recognized spontaneous speech in the ATIS domain .	S-67
OWN	We modified an existing partial coverage syntactic grammar into a grammar for the ATIS domain , using a development set of some 300 sentences .	S-68
OWN	The resulting grammar has 458 rules , which translate into a parsing table of almost 700 states .	S-69
OWN	A list of common appearing substitutions was constructed from the development set .	S-70
OWN	The correct parses of 250 grammatical sentences were used to train the parse table statistics that are used for disambiguation and parse evaluation .	S-71
OWN	After some experimentation , the evaluation feature weights were set in the following way .	S-72
OWN	As previously described , the penalty for a skipped word ranges between 0.95 and 1.05 , depending on the word 's position in the sentence .	S-73
OWN	The penalty for a substituted word was set to 0.9 , so that substituting a word would be preferable to skipping the word .	S-74
OWN	The fragmentation feature was given a weight of 1.1 , to prefer skipping a word if it reduces the fragmentation count by at least one .	S-75
OWN	The three penalties are then summed , together with the converted statistical score of the parse .	S-76
OWN	We then used a set of 120 new sentences as a test set .	S-77
OWN	Our goal was three-fold .	S-78
OWN	First , we wanted to compare the parsing capability of the GLR* parser with that of the original GLR parser .	S-79
OWN	Second , we wished to test the effectiveness of our evaluation heuristics in selecting the best parse .	S-80
OWN	Third , we wanted to evaluate the ability of the parse quality heuristic to correctly classify GLR* parses as `` good '' or `` bad '' .	S-81
OWN	We ran the parser three times on the test set .	S-82
OWN	The first run was with skipping disabled .	S-83
OWN	This is equivalent to running the original GLR parser .	S-84
OWN	The second run was conducted with skipping enabled and full heuristics .	S-85
OWN	The third run was conducted with skipping enabled , and with a simple heuristic that prefers parses based only on the number of words skipped .	S-86
OWN	In all three runs , the single selected parse result for each sentence was manually evaluated to determine if the parser returned with a `` correct '' parse .	S-87
OWN	The results of the experiment can be seen in Table.	S-88
OWN	The results indicate that using the GLR* parser results in a significant improvement in performance .	S-89
OWN	When using the full heuristics , the percentage of sentences , for which the parser returned a parse that matched or almost matched the `` correct '' parse increased from 50 % to 75 % .	S-90
OWN	As a result of its skipping capabilities , GLR* succeeds to parse 58 sentences ( 48 % ) that were not parsable by the original GLR parser .	S-91
OWN	Fully 96 % of the test sentences ( all but 5 ) are parsable by GLR* .	S-92
OWN	However , a significant portion of these sentences ( 23 out of the 58 ) return with bad parses , due to the skipping of essential words of the input .	S-93
OWN	We looked at the effectiveness of our parse quality heuristic in identifying such bad parses .	S-94
OWN	The heuristic is successful in labeling 21 of the 25 bad parses as `` bad '' .	S-95
OWN	67 of the 90 good / close parses are labeled as `` good '' by the heuristic .	S-96
OWN	Thus , although somewhat overly harsh , the heuristic is quite effective in identifying bad parses .	S-97
OWN	Our results indicate that our full integrated heuristic scheme for selecting the best parse out-performs the simple heuristic , that considers only the number of words skipped .	S-98
OWN	With the simple heuristic , good / close parses were returned in 24 out of the 53 sentences that involved some degree of skipping .	S-99
OWN	With our integrated heuristic scheme , good / close parses were returned in 30 sentences ( 6 additional sentences ) .	S-100
OWN	Further analysis showed that only 2 sentences had parses that were better than those selected by our integrated parse evaluation heuristic .	S-101
