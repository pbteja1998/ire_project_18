AIM	We present some variations affecting the association measure and thresholding on a technique for learning Selectional Restrictions from on-line corpora .	A-0
OWN	It uses a wide-coverage noun taxonomy and a statistical measure to generalize the appropriate semantic classes .	A-1
OWN	Evaluation measures for the Selectional Restrictions learning task are discussed .	A-2
OWN	Finally , an experimental evaluation of these variations is reported .	A-3
BKG	In recent years there has been a common agreement in the NLP research community on the importance of having an extensive coverage of selectional restrictions ( SRs ) tuned to the domain to work with .	S-0
BKG	SRs can be seen as semantic type constraints that a word sense imposes on the words with which it combines in the process of semantic interpretation .	S-1
BKG	SRs may have different applications in NLP , specifically , they may help a parser with Word Sense Selection ( WSS ), with preferring certain structures out of several grammatical onesand finally with deciding the semantic role played by a syntactic complement.	S-2
BKG	Lexicography is also interested in the acquisition of SRs ( both defining in context approach and lexical semantics work) .	S-3
AIM	The aim of our work is to explore the feasibility of using an statistical method for extracting SRs from on-line corpora .	S-4
OTH	developed a method for automatically extracting class-based SRs from on-line corpora .	S-5
OTH	performed some experiments using this basic technique and drew up some limitations from the corresponding results .	S-6
AIM	In this paper we will describe some substantial modifications to the basic technique and will report the corresponding experimental evaluation .	S-7
TXT	The outline of the paper is as follows : in sectionwe summarize the basic methodology used in, analyzing its limitations ; in sectionwe explore some alternative statistical measures for ranking the hypothesized SRs ; in sectionwe propose some evaluation measures on the SRs-learning problem , and use them to test the experimental results obtained by the different techniques ; finally , in sectionwe draw up the final conclusions and establish future lines of research .	S-8
OTH	The technique functionality can be summarized as :	S-9
OTH	Input	S-10
OTH	The training set , i.e. a list of complement co-occurrence triples , ( verb-lemma , syntactic-relationship , noun-lemma ) extracted from the corpus .	S-11
OTH	Previous knowledge used	S-12
OTH	A semantic hierarchy ( WordNet ) where words are clustered in semantic classes , and semantic classes are organized hierarchically .	S-13
OTH	Polysemous words are represented as instances of different classes .	S-14
OTH	Output	S-15
OTH	A set of syntactic SRs , ( verb-lemma , syntactic-relationship , semantic-class , weight ) .	S-16
OTH	The final SRs must be mutually disjoint .	S-17
OTH	SRs are weighted according to the statistical evidence found in the corpus .	S-18
OTH	Learning process	S-19
OTH	3 stages :	S-20
OTH	Creation of the space of candidate classes .	S-21
OTH	Evaluation of the appropriateness of the candidates by means of a statistical measure .	S-22
OTH	Selection of the most appropriate subset in the candidate space to convey the SRs .	S-23
OTH	The appropriateness of a class for expressing SRs ( stage 2 ) is quantified from the strength of co-occurrence of verbs and classes of nouns in the corpus.	S-24
OTH	Given the verb v , the syntactic-relationship s and the candidate class c , the Association Score , Assoc , between v and c in s is defined :	S-25
OTH	The two terms of Assoc try to capture different properties :	S-26
OTH	Mutual information ratio ,, measures the strength of the statistical association between the given verb v and the candidate class c in the given syntactic position s.	S-27
OTH	It compares the prior distribution ,, with the posterior distribution ,.	S-28
OTH	scales up the strength of the association by the frequency of the relationship .	S-29
OTH	Probabilities are estimated by Maximum Likelihood Estimation , counting the relative frequency of events in the corpus .	S-30
OTH	However , it is not obvious how to calculate class frequencies when the training corpus is not semantically tagged as is the case .	S-31
OTH	Nevertheless , we take a simplistic approach and calculate them in the following manner :	S-32
OTH	Where w is a constant factor used to normalize the probabilities	S-33
OTH	When creating the space of candidate classes ( learning process , stage 1 ) , we use a thresholding technique to ignore as much as possible the noise introduced in the training set .	S-34
OTH	Specifically , we consider only those classes that have a higher number of occurrences than the threshold .	S-35
OTH	The selection of the most appropriate classes ( stage 3 ) is based on a global search through the candidates , in such a way that the final classes are mutually disjoint ( not related by hyperonymy ) .	S-36
OTH	reported experimental results obtained from the application of the above technique to learn SRs .	S-37
OTH	He performed an evaluation of the SRs obtained from a training set of 870,000 words of the Wall Street Journal .	S-38
TXT	In this section we summarize the results and conclusions reached in that paper .	S-39
OTH	For instance , tableshows the SRs acquired for the subject position of the verb seek .	S-40
OTH	Type indicates a manual diagnosis about the class appropriateness ( Ok : correct ;Abs : over-generalization ; Senses : due to erroneous senses ) .	S-41
OTH	Assoc corresponds to the association score ( higher values appear first ) .	S-42
OTH	Most of the induced classes are due to incorrect senses .	S-43
OTH	Thus , although suit was used in the WSJ articles only in the sense of, the algorithm not only considered the other senses as well (,,) , but the Assoc score ranked them higher than the appropriate sense .	S-44
OTH	We can also notice that theAbs class ,, seems too general for the example nouns , while one of its daughters ,seems to fit the data much better .	S-45
OTH	Analyzing the results obtained from different experimental evaluation methods ,drew up some conclusions :	S-46
OTH	The technique achieves a good coverage .	S-47
OTH	Most of the classes acquired result from the accumulation of incorrect senses .	S-48
OTH	No clear co-relation between Assoc and the manual diagnosis is found .	S-49
OTH	A slight tendency to over-generalization exists due to incorrect senses .	S-50
CTR	Although the performance of the presented technique seems to be quite good , we think that some of the detected flaws could possibly be addressed .	S-51
CTR	Noise due to polysemy of the nouns involved seems to be the main obstacle for the practicality of the technique .	S-52
CTR	It makes the association score prefer incorrect classes and jump on over-generalizations .	S-53
AIM	In this paper we are interested in exploring various ways to make the technique more robust to noise , namely ,	S-54
AIM	to experiment with variations of the association score ,	S-55
AIM	to experiment with thresholding .	S-56
TXT	In this section we consider different variations on the association score in order to make it more robust .	S-57
TXT	The different techniques are experimentally evaluated in section.	S-58
OWN	When considering the prior probability , the more independent of the context it is the better to measure actual associations .	S-59
OWN	A sensible modification of the measure would be to consideras the prior distribution :	S-60
BAS	Using the chain rule on mutual information, we can mathematically relate the different versions of Assoc ,	S-61
OWN	The first advantage of Assoc' would come from this ( information theoretical ) relationship .	S-62
OWN	Specifically , the Assoc' takes into account the preference ( selection ) of syntactic positions for particular classes .	S-63
OWN	In intuitive terms , typical subjects ( e.g. person , individual , ... ) would be preferred ( to atypical subjects as suit _ of _ clothes ) as SRs on the subject in contrast to Assoc .	S-64
OWN	The second advantage is that as long as the prior probabilities ,, involve simpler events than those used in Assoc ,, the estimation is easier and more accurate ( ameliorating data sparseness ) .	S-65
OWN	A subsequent modification would be to estimate the prior ,, from the counts of all the nouns appearing in the corpus independently of their syntactic positions ( not restricted to be heads of verbal complements ) .	S-66
OWN	In this way , the estimation ofwould be easier and more accurate .	S-67
OWN	In the global weighting technique presented in equationvery polysemous nouns provide the same amount of evidence to every sense as non-ambiguous nouns do - while less ambiguous nouns could be more informative about the correct classes as long as they do not carry ambiguity .	S-68
OWN	The weight introduced incould alternatively be found in a local manner , in such a way that more polysemous nouns would give less evidence to each one of their senses than less ambiguous ones .	S-69
OWN	Local weight could be obtained using.	S-70
OWN	Nevertheless , a good estimation of this probability seems quite problematic because of the lack of tagged training material .	S-71
OWN	In absence of a better estimator we use a rather poor one as the uniform distribution ,	S-72
OTH	also uses a local normalization technique but he normalizes by the total number of classes in the hierarchy .	S-73
CTR	This scheme seems to present two problematic features ( seefor more details ) .	S-74
CTR	First , it doesn't take dependency relationships introduced by hyperonymy into account .	S-75
CTR	Second , nouns categorized in lower levels in the taxonomy provide less weight to each class than higher nouns .	S-76
BAS	In this section we propose the application of other measures apart from Assoc for learning SRs : log-likelihood ratio, relative entropy, mutual information ratio,.	S-77
TXT	In sectiontheir experimental evaluation is presented .	S-78
BKG	The statistical measures used to detect associations on the distribution defined by two random variables X and Y work by measuring the deviation of the conditional distribution ,, from the expected distribution if both variables were considered independent , i.e. the marginal distribution ,.	S-79
BKG	Ifis a good approximation of, association measures should be low ( near zero ) , otherwise deviating significantly from zero .	S-80
OWN	Tableshows the cross-table formed by the conditional and marginal distributions in the case ofand.	S-81
OTH	Different association measures use the information provided in the cross-table to different extents .	S-82
OWN	Thus , Assoc and mutual information ratio consider only the deviation of the conditional probabilityfrom the corresponding marginal ,.	S-83
OTH	On the other hand , log-likelihood ratio andmeasure the association betweenand c considering the deviation of the four conditional cells in tablefrom the corresponding marginals .	S-84
OWN	It is plausible that the deviation of the cells not taken into account by Assoc can help on extracting useful SRs .	S-85
OWN	Finally , it would be interesting to only use the information related to the selectional behavior of, i.e. comparing the conditional probabilities of c andgivenwith the corresponding marginals .	S-86
OWN	Relative entropy ,, could do this job .	S-87
OWN	Evaluation on NLP has been crucial to fostering research in particular areas .	S-88
OWN	Evaluation of the SR learning task would provide grounds to compare different techniques that try to abstract SRs from corpus using WordNet ( e. g , section) .	S-89
OWN	It would also permit measuring the utility of the SRs obtained using WordNet in comparison with other frameworks using other kinds of knowledge .	S-90
OWN	Finally it would be a powerful tool for detecting flaws of a particular technique ( e.g ,'s analysis ) .	S-91
OWN	However , a related and crucial issue is which linguistic tasks are used as a reference .	S-92
OWN	SRs are useful for both lexicography and NLP .	S-93
OWN	On the one hand , from the point of view of lexicography , the goal of evaluation would be to measure the quality of the SRs induced , ( i.e. , how well the resulting classes correspond to the nouns as they were used in the corpus ) .	S-94
OWN	On the other hand , from the point of view of NLP , SRs should be evaluated on their utility ( i.e. , how much they help on performing the reference task ) .	S-95
OWN	As far as lexicography ( quality ) is concerned , we think the main criteria SRs acquired from corpora should meet are :	S-96
OWN	correct categorization - inferred classes should correspond to the correct senses of the words that are being generalized - ,	S-97
OWN	appropriate generalization level and	S-98
OWN	good coverage - the majority of the noun occurrences in the corpus should be successfully generalized by the induced SRs .	S-99
OWN	Some of the methods we could use for assessing experimentally the accomplishment of these criteria would be :	S-100
OWN	Introspection	S-101
OWN	A lexicographer checks if the SRs accomplish the criteriaandabove ( e.g. , the manual diagnosis in table) .	S-102
OWN	Besides the intrinsic difficulties of this approach , it does not seem appropriate when comparing across different techniques for learning SRs , because of its qualitative flavor .	S-103
OWN	Quantification of generalization level appropriateness	S-104
OWN	A possible measure would be the percentage of sense occurrences included in the induced SRs which are effectively correct ( from now on called Abstraction Ratio ) .	S-105
OWN	Hopefully , a technique with a higher abstraction ratio learns classes that fit the set of examples better .	S-106
OWN	A manual assessment of the ratio confirmed this behavior , as testing sets with a lower ratio seemed to be inducing lessAbs cases .	S-107
OWN	Quantification of coverage	S-108
OWN	It could be measured as the proportion of triples whose correct sense belongs to one of the SRs .	S-109
BKG	The NLP tasks where SRs utility could be evaluated are diverse .	S-110
TXT	Some of them have already been introduced in section.	S-111
OTH	In the recent literature,several task oriented schemes to test Selectional Restrictions ( mainly on syntactic ambiguity resolution ) have been proposed .	S-112
CTR	However , we have tested SRs on a WSS task , using the following scheme .	S-113
OWN	For every triple in the testing set the algorithm selects as most appropriate that noun-sense that has as hyperonym the SR class with highest association score .	S-114
OWN	When more than one sense belongs to the highest SR , a random selection is performed .	S-115
OWN	When no SR has been acquired , the algorithm remains undecided .	S-116
OWN	The results of this WSS procedure are checked against a testing-sample manually analyzed , and precision and recall ratios are calculated .	S-117
OWN	Precision is calculated as the ratio of manual-automatic matches / number of noun occurrences disambiguated by the procedure .	S-118
OWN	Recall is computed as the ratio of manual-automatic matches / total number of noun occurrences .	S-119
OWN	In order to evaluate the different variants on the association score and the impact of thresholding we performed several experiments .	S-120
TXT	In this section we analyze the results .	S-121
OWN	As training set we used the 870,000 words of WSJ material provided in the ACL / DCI version of the Penn Treebank .	S-122
OWN	The testing set consisted of 2,658 triples corresponding to four average common verbs in the Treebank : rise , report , seek and present .	S-123
OWN	We only considered those triples that had been correctly extracted from the Treebank and whose noun had the correct sense included in WordNet ( 2,165 triples out of the 2,658 , from now on , called the testing-sample ) .	S-124
OWN	As evaluation measures we used coverage , abstraction ratio , and recall and precision ratios on the WSS task ( section) .	S-125
OWN	In addition we performed some evaluation by hand comparing the SRs acquired by the different techniques .	S-126
OWN	Coverage for the different techniques is shown in table.	S-127
OWN	The higher the coverage , the better the technique succeeds in correctly generalizing more of the input examples .	S-128
OWN	The labels used for referring to the different techniques are as follows :corresponds to the basic association measure ( section) ,andto the techniques introduced in section,to the local normalization ( section) , and finally , log-likelihood , D ( relative entropy ) and I ( mutual information ratio ) to the techniques discussed in section.	S-129
OWN	The abstraction ratio for the different techniques is shown in table.	S-130
OWN	In principle , the higher abstraction ratio , the better the technique succeeds in filtering out incorrect senses ( lessAbs ) .	S-131
OWN	The precision and recall ratios on the noun WSS task for the different techniques are shown in table.	S-132
OWN	In principle , the higher the precision and recall ratios the better the technique succeeds in inducing appropriate SRs for the disambiguation task .	S-133
OWN	As far as the evaluation measures try to account for different phenomena the goodness of a particular technique should be quantified as a trade-off .	S-134
OWN	Most of the results are very similar ( differences are not statistically significative ) .	S-135
OWN	Therefore we should be cautious when extrapolating the results .	S-136
OWN	Some of the conclusions from the tables above are :	S-137
OWN	and I get sensibly worse results than other measures ( although abstraction is quite good ) .	S-138
OWN	The local normalizing technique using the uniform distribution does not help .	S-139
OWN	It seems that by using the local weighting we misinform the algorithm .	S-140
OWN	The problem is the reduced weight that polysemous nouns get , while they seem to be the most informative .	S-141
OWN	However , a better informed kind of local weight ( section) should improve the technique significantly .	S-142
OWN	All versions of Assoc ( except the local normalization ) get good results .	S-143
OWN	Specially the two techniques that exploit a simpler prior distribution , which seem to improve the basic technique .	S-144
OWN	log-likelihood and D seem to get slightly worse results than Assoc techniques , although the results are very similar .	S-145
OWN	We were also interested in measuring the impact of thresholding on the SRs acquired .	S-146
OWN	In figurewe can see the different evaluation measures of the basic technique when varying the threshold .	S-147
OWN	Precision and recall coincide when no candidate classes are refused ( threshold = 1 ) .	S-148
OWN	However , as it might be expected , as the threshold increases ( i.e. some cases are not classified ) the two ratios slightly diverge ( precision increases and recall diminishes ) .	S-149
OWN	Figurealso shows the impact of thresholding on coverage and abstraction ratios .	S-150
OWN	Both decrease when threshold increases , probably because when the rejecting threshold is low , small classes that fit the data well can be induced , learning over-general or incomplete SRs otherwise .	S-151
OWN	Finally , it seems that precision and abstraction ratios are in inverse co-relation ( as precision grows , abstraction decreases ) .	S-152
OWN	In terms of WSS , general classes may be performing better than classes that fit the data better .	S-153
OWN	Nevertheless , this relationship should be further explored in future work .	S-154
AIM	In this paper we have presented some variations affecting the association measure and thresholding on the basic technique for learning SRs from on-line corpora .	S-155
AIM	We proposed some evaluation measures for the SRs learning task .	S-156
OWN	Finally , experimental results on these variations were reported .	S-157
OWN	We can conclude that some of these variations seem to improve the results obtained using the basic technique .	S-158
OWN	However , although the technique still seems far from practical application to NLP tasks , it may be most useful for providing experimental insight to lexicographers .	S-159
OWN	Future lines of research will mainly concentrate on improving the local normalization technique by solving the noun sense ambiguity .	S-160
OWN	We have foreseen the application of the following techniques :	S-161
OWN	Simple techniques to decide the best sense c given the target noun n using estimates of the n-grams :,,and, obtained from supervised and un-supervised corpora .	S-162
OWN	Combining the different n-grams by means of smoothing techniques .	S-163
OWN	Calculatingcombiningand, and applying the EM Algorithmto improve the model .	S-164
OWN	Using the WordNet hierarchy as a source of backing-off knowledge , in such a way that if n-grams composed by c aren't enough to decide the best sense ( are equal to zero ) , the tri-grams of ancestor classes could be used instead .	S-165
