BKG	Infants face the difficult problem of segmenting continuous speech into words without the benefit of a fully developed lexicon .	A-0
BKG	Several sources of information in speech might help infants solve this problem , including prosody , semantic correlations and phonotactics .	A-1
CTR	Research to date has focused on determining to which of these sources infants might be sensitive , but little work has been done to determine the potential usefulness of each source .	A-2
AIM	The computer simulations reported here are a first attempt to measure the usefulness of distributional and phonotactic information in segmenting phoneme sequences .	A-3
OWN	The algorithms hypothesize different segmentations of the input into words and select the best hypothesis according to the Minimum Description Length principle .	A-4
OWN	Our results indicate that while there is some useful information in both phoneme distributions and phonotactic rules , the combination of both sources is most useful .	A-5
BKG	Infants must learn to recognize certain sound sequences as being words ; this is a difficult problem because normal speech contains no obvious acoustic divisions between words .	S-0
BKG	Two sources of information that might aid speech segmentation are : distribution -- the phoneme sequence in cat appears frequently in several contexts including thecat , cats and catnap , whereas the sequence in catn is rare and appears in restricted contexts ; and phonotactics -- cat is an acceptable syllable in English , whereas pcat is not .	S-1
CTR	While evidence exists that infants are sensitive to these information sources , we know of no measurements of their usefulness .	S-2
AIM	In this paper , we attempt to quantify the usefulness of distribution and phonotactics in segmenting speech .	S-3
OWN	We found that each source provided some useful information for speech segmentation , but the combination of sources provided substantial information .	S-4
OWN	We also found that child-directed speech was much easier to segment than adult-directed speech when using both sources .	S-5
OTH	To date , psychologists have focused on two aspects of the speech segmentation problem .	S-6
OTH	The first is the problem of parsing continuous speech into words given a developed lexicon to which incoming sounds can be matched ; both psychologists,and designers of speech-recognition systemshave examined this problem .	S-7
CTR	However , the problem we examined is different -- we want to know how infants segment speech before knowing which phonemic sequences form words .	S-8
OTH	The second aspect psychologists have focused on is the problem of determining the information sources to which infants are sensitive .	S-9
OTH	Primarily , two sources have been examined : prosody and word stress .	S-10
OTH	Results suggest that parents exaggerate prosody in child-directed speech to highlight important words,and that infants are sensitive to prosody.	S-11
OTH	Word stress in English fairly accurately predicts the location of word beginnings,;demonstrated that 9-month - olds ( but not 6-month - olds ) are sensitive to the common strong / weak word stress pattern in English .	S-12
OTH	Sensitivity to native-language phonotactics in 9-month - olds was recently reported by.	S-13
CTR	These studies demonstrated infants ' perceptive abilities without demonstrating the usefulness of infants ' perceptions .	S-14
AIM	How do children combine the information they perceive from different sources ?	S-15
OTH	speculate that infants first learn words heard in isolation , then use distribution and prosody to refine and expand their vocabulary ; however ,suggests that sound sequences learned in isolation differ too greatly from those in context to be useful .	S-16
OTH	He goes on to say , `` just how far information in the sound structure of the input can bootstrap the acquisition of other levels [ of linguistic organization ] remains to be determined . ''	S-17
AIM	In this paper , we measure the potential roles of distribution , phonotactics and their combination using a computer-simulated learning algorithm ; the simulation is based on a bootstrapping model in which phonotactic knowledge is used to constrain the distributional analysis of speech samples .	S-18
BAS	While our work is in part motivated by the above research , other developmental research supports certain assumptions we make .	S-19
BAS	The input to our system is represented as a sequence of phonemes , so we implicitly assume that infants are able to convert from acoustic input to phoneme sequences ; research bysuggests that this assumption is reasonable .	S-20
BAS	Since sentence boundaries provide information about word boundaries ( the end of a sentence is also the end of a word ) , our input contains sentence boundaries ; several studies,,,have shown that infants can perceive sentence boundaries using prosodic cues .	S-21
CTR	However ,found no evidence that prosody can accurately predict word boundaries , so the task of finding words remains .	S-22
BAS	Finally , one might question whether infants have the ability we are trying to model -- that is , whether they can identify words embedded in sentences ;found that 7 1/2-month - olds can do so .	S-23
BKG	To gain an intuitive understanding of our model , consider the following speech sample ( transcription is in IPA ) :	S-24
BKG	There are many different ways to break this sample into putative words ( each particular segmentation is called a segmentation hypothesis ) .	S-25
BKG	Two such hypotheses are :	S-26
BKG	Listing the words used by each segmentation hypothesis yields the following two lexicons :	S-27
OWN	Note that Segmentation, the correct hypothesis , yields a compact lexicon of frequent words whereas Segmentationyields a much larger lexicon of infrequent words .	S-28
OWN	Also note that a lexicon contains only the words used in the sample -- no words are known to the system a priori , nor are any carried over from one hypothesis to the next .	S-29
OWN	Given a lexicon , the sample can be encoded by replacing words with their respective indices into the lexicon :	S-30
BAS	Our simulation attempts to find the hypothesis that minimizes the combined sizes of the lexicon and encoded sample .	S-31
BAS	This approach is called the Minimum Description Length ( MDL ) paradigm and has been used recently in other domains to analyze distributional information,,,,) .	S-32
OWN	For reasons explained in the next section , the system converts these character-based representations to compact binary representations , using the number of bits in the binary string as a measure of size .	S-33
OWN	Phonotactic rules can be used to restrict the segmentation hypothesis space by preventing word boundaries at certain places ; for instance , /ktspz/ ( `` cat 's paws '' ) has six internal segmentation points ( k tspz , k tspz , etc ) , only two of which are phonotactically allowed ( kt spz and kts pz ) .	S-34
OWN	To evaluate the usefulness of phonotactic knowledge , we compared results between phonotactically constrained and unconstrained simulations .	S-35
OWN	To use the MDL principle , as introduced above , we search for the smallest-sized hypothesis .	S-36
OWN	We must have some well-defined method of measuring hypothesis sizes for this method to work .	S-37
OTH	A simple , intuitive way of measuing the size of a hypothesis is to count the number of characters used to represent it .	S-38
OTH	For example , counting the characters ( excluding spaces ) in the introductory examples , we see that Hypothesis 1 uses 48 characters and Hypothesis 2 uses 75 .	S-39
CTR	However , this simplistic method is inefficient ; for instance , the length of lexical indices are arbitrary with respect to properties of the words themselves ( e.g. , in Hypothesis 2 , there is no reason why /jul/ was assigned the index ` 10 ' -- length two -- instead of ` 9 ' -- length one ) .	S-40
OWN	Our system improves upon this simple size metric by computing sizes based on a compact representation motivated by information theory .	S-41
OWN	We imagine hypotheses represented as a string of ones and zeros .	S-42
OWN	This binary string must represent not only the lexical entries , their indices ( called code words ) and the coded sample , but also overhead information specifying the number of items coded and their arrangement in the string ( information implicitly given by spacing and spatial placement in the introductory examples ) .	S-43
OWN	Furthermore , the string and its components must be self-delimiting , so that a decoder could identify the endpoints of components by itself .	S-44
TXT	The next section describes the binary representation and the length formul derived from it in detail ; readers satisfied with the intuitive descriptions presented so far should skip ahead to the Phonotactics sub-section .	S-45
BAS	The representation scheme described below is based on information theory ( for more examples of coding systems , see, e.g.and) .	S-46
OWN	From this representation , we can derive a formula describing its length in bits .	S-47
OWN	However , the discrete form of the formula would not work well in practice for our simulations .	S-48
OWN	Instead , we use a continuous approximation of the discrete formula ; this approximation typically involves dropping the ceiling function from length computations .	S-49
OWN	For example , we sometimes use a self-delimiting representation for integers ( as described in, pp. 74 - 75 ) .	S-50
OWN	In this representation , the number of bits needed to code an integer x is given by	S-51
OWN	However , we use the following approximation :	S-52
OWN	Using the discrete formula , the difference betweenandis zero , while the difference betweenandis one bit ; using the continuous formula , the difference betweenandis 0.0156 , while the difference betweenandis 0.0155 .	S-53
OWN	We found it easier to interpret the results using a continuous function , so in the following discussion , we will only present the approximate formulae .	S-54
OWN	The lexicon lists words ( represented as phoneme sequences ) paired with their code words .	S-55
OWN	For example :	S-56
OWN	In the binary representation , the two columns are represented separately , one after the other ; the first column is called the word inventory column ; the second column is called the code word inventory column .	S-57
OWN	In the word inventory column ( see Figurefor a schematic ) , the list of lexical items is represented as a continuous string of phonemes , without separators between words ( e.g. , ktktisi . )	S-58
OWN	To mark the boundaries between lexical items , the phoneme string is preceded by a list of integers representing the lengths ( in phonemes ) of each word .	S-59
OWN	Each length is represented as a fixed-length , zero-padded binary number .	S-60
OWN	Preceding this list is a single integer denoting the length of each length field ; this integer is represented in unary , so that its length need not be known in advance .	S-61
OWN	Preceding the entire column is the number of lexical entries n coded as a self-delimiting integer .	S-62
OWN	The length of the representation of the integer n is given by the function	S-63
OWN	We defineto be the number of phonemes in word.	S-64
OWN	If there are p total unique phonemes used in the sample , then we represent each phoneme as a fixed-length bit string of length.	S-65
OWN	So , the length of the representation of a wordin the lexicon is the number of phonemes in the word times the length of a phoneme :.	S-66
OWN	The total length of all the words in the lexicon is the sum of this formula over all lexical items :	S-67
OWN	As stated above , the length fields used to divide the phoneme string are fixed-length .	S-68
OWN	In each field is an integer between one and the number of phonemes in the longest word .	S-69
OWN	Since representing integers between one and x takesbits , the length of each field is :	S-70
OWN	To be fully self-delimiting , the width of a field must be represented in a self-delimiting way ; we use a unary representation -- i.e. , write an extra field consisting of only ` 1 ' bits followed by a terminating ` 0 ' .	S-71
OWN	There are n fields ( one for each word ) , plus the unary prefix , so the combined length of the fields plus prefix ( plus terminating zero ) is :	S-72
OWN	The total length of the word inventory column representation is the sum of the terms in,and.	S-74
OWN	The code word inventory column of the lexicon ( see Figurefor a schematic ) has a nearly identical representation as the previous column except that code words are listed instead of phonemic words -- the length fields and unary prefix serve the same purpose of marking the divisions between code words .	S-75
OWN	The sample can be represented most compactly by assigning short code words to frequent words , reserving longer code words for infrequent words .	S-76
OWN	To satisfy this property , code words are assigned so that their lengths are frequency-based ; the length of the code word for a word of frequency f(w) will not be greater than :	S-77
OWN	The total length of the code word list is the sum of the code word lengths over all lexical entries :	S-78
OWN	As in the word inventory column ( described above ) , the length of each code word is represented in a fixed-length field .	S-79
OWN	Since the least frequent word will have the longest code word ( a property of the formula for) , the longest possible code word comes from a word of frequency one :	S-80
OWN	Since the fields contains integers between one and this number , we define the length of a field to be :	S-81
OWN	As above , we represent the width of a field in unary , so there are a total of n + 1 elements of this size ( n fields plus the unary representation of the field width ) .	S-82
OWN	The combined length of the fields plus prefix ( and terminating zero ) is :	S-83
OWN	The total length of the code word inventory column representation is the sum of the terms inand.	S-84
OWN	Finally , the sequence of words which form the sample ( see Figurefor a schematic ) is represented as the number of words in the sample ( m ) followed by the list of code words .	S-85
OWN	Since code words are used as compact indices into the lexicon , the original sample could be reconstructed completely by looking up each code word in this list and replacing it with its phoneme sequence from the lexicon .	S-86
OWN	The code words we assigned to lexical items are self-delimiting ( once the set of codes is known ) , so there is no need to represent the boundaries between code words .	S-87
OWN	The length of the representation of the integer m is given by the function	S-88
OWN	The length of the representation of the sample is computed by summing the lengths of the code words used to represent the sample .	S-89
OWN	We can simplify this description by noting that the combined length of all occurrences of a particular code wordissince there areoccurrences of the code word in the sample .	S-90
OWN	So , the length of the encoded sample is the sum of this formula over all words in the lexicon :	S-91
OWN	The total length of the sample is given by adding the terms inand.	S-92
OWN	The total length of the representation of the entire hypothesis is the sum of the representation lengths of the word inventory column , the code word inventory column and the sample .	S-93
OWN	This system of computing hypothesis sizes is efficient in the sense that elements are thought of as being represented compactly and that code words are assigned based on the relative frequencies of words .	S-94
OWN	The final evaluation given to a hypothesis is an estimate of the minimal number of bits required to transmit that hypothesis .	S-95
OWN	As such , it permits direct comparison between competing hypotheses ; that is , the shorter the representation of some hypothesis , the more distributional information can be extracted and , therefore , the better the hypothesis .	S-96
OWN	Phonotactic knowledge was given to the system as a list of licit initial and final consonant clusters of English words ; this list was checked against all six samples so that the list was maximally permissive ( e.g. , the underlined consonant cluster in explore could be divided as ek-splore or eks-plore ) .	S-97
OWN	In those simulations which used the phonotactic knowledge , a word boundary could not be inserted when doing so would create a word initial or final consonant cluster not on the list or would create a word without a vowel .	S-98
OWN	For example ( from an actual sample -- corresponds to the utterance , `` Want me to help baby ? '' ) :	S-99
OWN	In the second line , those word boundaries that are phonotactically legal are marked with dots .	S-100
OWN	The boundary between /w/ and /a/ is illegal because /w/ by itself is not a legal word in English ; the boundary between /a/ and /n/ is illegal because /ntm/ is not a valid word initial consonant cluster ; the boundary between /m/ and /i/ is illegal because /ntm/ is also not a valid word final consonant cluster ; the boundary between /p/ and /b/ is legal because /lp/ is a valid word final cluster and /b/ is a valid word initial cluster .	S-101
OWN	Note that using the phonotactic constraints reduces the number of potential word boundaries from fifteen to six in this example .	S-102
OWN	After the system inserts a new word boundary , it updates the list of remaining valid insertion points -- adding a point may cause nearby points to become unusable due to the restriction that every word must have a vowel .	S-103
OWN	For example ( corresponding to the utterance `` green and '' ) :	S-104
OWN	After the segmentation of /grin/ and /nd/ , the potential boundary between /i/ and /n/ becomes invalid because inserting a word boundary there would produce a word with no vowel ( /n/ ) .	S-105
OWN	Two speech samples from each of three subjects were used in the simulations ; in one sample a mother was speaking to her daughter and in the other , the same mother was speaking to the researcher .	S-106
BAS	The samples were taken from the CHILDES databasefrom studies reported in.	S-107
OWN	Each sample was checked for consistent word spellings ( e.g. , 'ts was changed to its ) , then was transcribed into an ASCII-based phonemic representation .	S-108
OWN	The transcription system was based on IPA and used one character for each consonant or vowel ; diphthongs , r-colored vowels and syllabic consonants were each represented as one character .	S-109
OWN	For example , `` boy '' was written as b7 , `` bird '' as bRd and `` label '' as lebL .	S-110
OWN	For purposes of phonotactic constraints , syllabic consonants were treated as vowels .	S-111
OWN	Sample lengths were selected to make the number of available segmentation points nearly equal ( about 1,350 ) when no phonotactic constraints were applied ; child-directed samples had 498 - 536 tokens and 153 - 166 types , adult-directed samples had 443 - 484 tokens and 196 - 205 types .	S-112
OWN	Finally , before the samples were fed to the simulations , divisions between words ( but not between sentences ) were removed .	S-113
OWN	The space of possible hypotheses is vast , so some method of finding a minimum-length hypothesis without considering all hypotheses is necessary .	S-114
OWN	We used the following method : first , evaluate the input sample with no segmentation points added ; then evaluate all hypotheses obtained by adding one or two segmentation points ; take the shortest hypothesis found in the previous step and evaluate all hypotheses obtained by adding one or two more segmentation points ; continue this way until the sample has been segmented into the smallest possible units and report the shortest hypothesis ever found .	S-115
OWN	Two variants of this simulation were used :	S-116
OWN	DIST-FREE was free of any phonotactic restrictions on the hypotheses it could form ( DIST refers to the measurement of distributional information ) , whereas	S-117
OWN	DIST-PHONO used the phonotactic restrictions described above .	S-118
OWN	Each simulation was run on each sample , for a total of twelve DIST runs .	S-119
OWN	Finally , two other simulations were run on each sample to measure chance performance :	S-120
OWN	RAND-FREE inserted random segmentation points and reported the resulting hypothesis ,	S-121
OWN	RAND-PHONO inserted random segmentation points where permitted by the phonotactic constraints .	S-122
OWN	Since the RAND simulations were given the number of segmentation points to add ( equal to the number of segmentation points needed to produce the natural English segmentation ) , their performance is an upper bound on chance performance .	S-123
OWN	In contrast , the DIST simulations must determine the number of segmentation points to add using MDL evaluations .	S-124
OWN	The results for each RAND simulation are averages over 1,000 trials on each input sample .	S-125
OWN	Each simulation was scored for the number of correct segmentation points inserted , as compared to the natural English segmentation .	S-126
OWN	From this scoring , two values were computed :	S-127
OWN	recall , the percent of all correct segmentation points that were actually found ; and	S-128
OWN	accuracy , the percent of the hypothesized segmentation points that were actually correct .	S-129
OWN	In terms of hits , false alarms and misses , we have :	S-130
OWN	Results are given in Table.	S-131
OWN	Note that there is a trade-off between recall and accuracy -- if all possible segmentation points were added , recall would be 100 % but accuracy would be low ; likewise , if only one segmentation point was added between two words , accuracy would be 100 % but recall would be low .	S-132
OWN	Since our goal is to correctly segment speech , accuracy is more important than finding every correct segmentation .	S-133
OWN	For example , deciding ` littlekitty ' is a word is less disastrous than deciding ` li ' , ` tle ' , ` ki ' and ` ty ' are all words , because assigning meaning to ` littlekitty ' is a reasonable first try at learning word-meaning pairs , whereas trying to assign separate meanings to ` li ' and ` tle ' is problematic .	S-134
OWN	The performance of DIST-PHONO on child-directed speech shows that this system goes a long way toward solving the segmentation problem .	S-135
OWN	However , comparing the average performances of simulations is also useful .	S-136
OWN	The effect of phonotactic information can be seen by comparing the average performances of RAND-FREE and RAND-PHONO , since the only difference between them is the addition of phonotactic constraints on segmentations in the latter .	S-137
OWN	Clearly phonotactic constraints are useful , as both recall and accuracy improve .	S-138
OWN	A similar comparison between RAND-FREE and DIST-FREE shows that distributional information alone also improves performance .	S-139
OWN	Note in all the results of DIST-FREE that using distributional information alone favors recall over accuracy ; in fact , the segmentation hypotheses produced by DIST-FREE have most words broken into single phoneme units with only a handful of words remaining intact .	S-140
OWN	Two comparisons are needed to show that the combination of distributional and phonotactic information performs better than either source alone : DIST-PHONO compared to RAND-PHONO , to see the effect of adding distributional analysis to phonotactic constraints , and DIST-PHONO compared to DIST-FREE , to see the effect of adding phonotactic constraints to distributional analysis .	S-141
OWN	The former comparison shows that the sources combined are more useful than phonotactic information alone .	S-142
OWN	The latter comparison is less obvious -- the trade-off between recall and accuracy seems to have reversed , with no clear winner .	S-143
OWN	Data on discovered word types helps make this comparison : DIST-FREE found 12 % of the words with 30 % accuracy and DIST-PHONO found 33 % of the words with 50 % accuracy .	S-144
OWN	Whereas the segmentation point data are inconclusive , word type data demonstrate that combining information sources is more useful than using distributional information alone .	S-145
OWN	There is no obvious difference in performance between child - and adult-directed speech , except in DIST-PHONO ( combined information sources ) in which the difference is striking : accuracy remains high and recall rate more than triples for child-directed speech .	S-146
OWN	This difference is again supported by word type data : 14 % recall with 30 % accuracy for adult-directed speech , 56 % recall with 65 % accuracy for child-directed speech .	S-147
AIM	Our technique segments continuous speech into words using only distributional and phonotactic information more effectively than one might expect -- up to 66 % recall of segmentation points with 92 % accuracy on one sample , which yields 58 % recall of word types with 67 % accuracy ( the relatively low type accuracy is mitigated by the fact that most incorrect words are meaningful concatenations of correct words -- e.g. , ` thekitty ' ) .	S-148
BAS	This finding confirms the idea that distribution and phonotactics are useful sources of information that infants might use in discovering words.	S-149
OWN	In fact , it helps explain infants ' ability to learn words from parental speech : these two sources alone are useful and infants have several others , like prosody and word stress patterns , available as well .	S-150
OWN	It also suggests that semantics and isolated words need not play as central a role as one might think ( e.g.,downplayed the utility of words in isolation ) .	S-151
OWN	It is difficult , if not impossible given currently available methods , to determine which sources of information are necessary for infants to segment speech and learn words ; only this sort of indirect evidence is available to us .	S-152
OWN	The results show a difference between adult - and child-directed speech , in that the latter is easier to segment given both distribution and phonotactics .	S-153
BAS	This lends quantitative support to research which suggests that motherese differs from normal adult speech in ways possibly useful to the language-learning infant.	S-154
OWN	In fact , the factors making motherese more learnable might be elucidated using this technique : compare the results of several different models , each containing a different factor or combination of factors , looking for those in which a substantial performance difference exists between child - and adult-directed speech .	S-155
OWN	Our model uses phonotactic constraints as absolute requirements on the structure of individual words ; this implies that phonotactics have been learned prior to attempts at segmentation .	S-156
OWN	We must therefore show that phonotactics can indeed be learned without access to a lexicon -- without such a demonstration , we are trapped in circular reasoning .	S-157
OWN	demonstrate that phonotactics can be learned with high accuracy from the same unsegmented utterances we used in our simulations .	S-158
OWN	In general , two methods exist for combining information sources in the MDL paradigm : one is to have absolute requirements on plausible hypotheses ( like our phonotactic constraints ) -- these requirements must be independently learnable ; the other method of combination is to include an information source in the internal representation of hypotheses ( like our distributional information ) -- all components of the representation are learned simultaneously ( see, for an example of multiple components in a representation ) .	S-159
OWN	We would like to extend the system by using a more detailed transcription system .	S-160
OWN	We expect that this would help the system find word boundaries for reasons detailed in-- in brief , that allophonic variation may be quite useful in predicting word boundaries .	S-161
OWN	Another simpler extension of this research will be to increase the length of the speech samples used .	S-162
OWN	Finally , we will try the current system on samples from other languages , to make sure this method generalizes appropriately .	S-163
OWN	This research program will provide complementary evidence supporting hypotheses about the sources of information infants use in learning their native languages .	S-164
AIM	Until now , research has focused on demonstrations of infants ' sensitivity to various sources ; we have begun to provide quantitative measures of the usefulness of those sources .	S-165
