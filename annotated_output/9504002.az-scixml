AIM	An experiment designed to explore the relationship between tagging accuracy and the nature of the tagset is described , using corpora in English , French and Swedish .	A-0
AIM	In particular , the question of internal versus external criteria for tagset design is considered , with the general conclusion that external ( linguistic ) criteria should be followed .	A-1
OWN	Some problems associated with tagging unknown words in inflected languages are briefly considered .	A-2
BKG	Tagging by means of a Hidden Markov Model ( HMM ) is widely recognised as an effective technique for assigning parts of speech to a corpus in a robust and efficient manner .	S-0
BKG	An attractive feature of the technique is that the algorithm itself is independent of the ( natural ) language to which it is applied .	S-1
BKG	All of the `` knowledge engineering '' is localised in the choice of tagset and the method of training .	S-2
BKG	Typically , training makes use of a manually tagged corpus , or an untagged corpus with some initial bootstrapping probabilities .	S-3
BKG	Some attention has been given to how to make such techniques effective ; for examplesuggest ways of training trigram taggers , andandconsider the amount and quality of the seeding data needed to construct an accurate tagger .	S-4
BKG	In training a tagger for a given language , a major part of the knowledge engineering required can therefore be localised in the choice of the tagset .	S-5
BKG	The design of an appropriate tagset is subject to both external and internal criteria .	S-6
BKG	The external criterion is that the tagset must be capable of making the linguistic ( for example , syntactic or morphological ) distinctions required in the output corpora .	S-7
OTH	Tagsets used in the past have included varying amounts of detail .	S-8
OTH	For example , the Penn treebank tagsetomits a number of the distinctions which are made in the LOB and Brown tagsets on which it is based,in cases where the surface form of the words allows the distinctions to be recovered if they are needed .	S-9
OTH	Thus , the auxiliary verbs be , do and have have the same tags as other verbs in Penn , but are each separated out in the LOB tagset .	S-10
BKG	A second design criterion on tagsets is the internal one of making the tagging as effective as possible .	S-11
BKG	As an example , one of the most common errors made by taggers with the LOB and Brown tagsets is mistagging a word as a subordinating conjunction ( CS ) rather than a preposition ( IN ) , or vice-versa.	S-12
BKG	A higher level of syntactic analysis indicating the phrasal structure would be required to predict which tag is correct , and this information is not available to fixed-context taggers .	S-13
BKG	The Penn treebank therefore uses a single tag for both cases , leaving the resolution - if required - to some other process .	S-14
BKG	Similarly , most tagsets do not distinguish transitive and intransitive verbs , since taggers which use a context of only two or three words will generally not be able to make the right predictions .	S-15
BKG	Distinctions of this sort are usually found only in corpora such as Susanne which are parsed as well as tagged .	S-16
BKG	The problem of tagset design becomes particularly important for highly inflected languages , such as Greek or Hungarian .	S-17
BKG	If all of the syntactic variations which are realised in the inflectional system were represented in the tagset , there would be a huge number of tags , and it would be practically impossible to implement or train a simple tagger .	S-18
BKG	Note in passing that this may not as serious a problem as it first appears .	S-19
BKG	If the language is very highly inflected , it may be be possible to do all ( or a large part ) of the work of a tagger with a word-by-word morphological analysis instead .	S-20
BKG	Nevertheless , there are many languages which have enough ambiguity that tagging is useful , but a rich enough tagset that the criteria on which it is designed must be given careful consideration .	S-21
AIM	In this paper , I report two experiments which address the internal design criterion , by looking at how tagging accuracy varies as the tagset is modified , in English , French and Swedish .	S-22
OWN	Although the choice of language was dictated by the corpora which were available , they represent three different degrees of complexity in their inflectional systems .	S-23
BKG	English has a very limited system , marking little more than plurality on nouns and a restricted range of verb properties .	S-24
BKG	French has a little more complexity , with gender , number and person marked , while Swedish has more detailed marking for gender , number , definiteness and case .	S-25
AIM	As a subsidiary issue , we will also look at how the tagger performs on unknown words , i.e. ones not seen in the training data .	S-26
OTH	The usual approach here is to hypothesise all tags in the tagset for an unknown word , other than ones where all the words that may have the tag can be enumerated in advance ( closed class tags ) .	S-27
CTR	HMM taggers often perform poorly on unknown words .	S-28
OWN	Alternative tagsets were derived by taking the initial tagset for each corpus ( from manual tagging of the corpus ) and condensing sets of tags which represent a grammatical distinction such as gender into single tags .	S-29
OWN	The changes were then applied to the training corpus .	S-30
OWN	This allows us to effectively produce a corpus tagged according to a different scheme without having to manually re-tag the corpus .	S-31
OWN	The changes in the tagsets were motivated purely by grammatical considerations , and did not take the errors actually observed into account .	S-32
OWN	In general what we will look at in the results is how the tagging accuracy changes as the size of the tagset changes .	S-33
OWN	This is a deliberately naive approach , and it is adopted with the goal of continuing in the relatively `` knowledge-free '' tradition of work in HMM tagging .	S-34
AIM	The aim of the experiment is to determine , crudely , whether a bigger tagset is better than a smaller one , or whether external criteria requiring human intervention should be used to choose the best tagset .	S-35
OWN	The results for the three languages turn out to be quite different , and the general conclusion ( which is the overall contribution of the paper ) will be that the external criterion should be the one to dominate tagset design : there is a limit to how knowledge-free we can be .	S-36
BKG	As a preliminary to this work , note that it is hard to reason about the effect of changing the tagset .	S-37
BKG	It can be argued that a smaller tagset should improve tagging accuracy , since it puts less of a burden on the tagger to make fine distinctions .	S-38
BKG	In information-theoretic terms , the number of decisions required is smaller , and hence the tagger need contribute less information to make the decisions .	S-39
BKG	A smaller tagset may also mean that more words have only one possible tag and so can be handled trivially .	S-40
BKG	Conversely , more detail in the tagset may help the tagger when the properties of two adjacent words give support to the choice of tag for both of them ; that is , the transitions between tags contribute the information the tagger needs .	S-41
BKG	For example , if determiners and nouns are marked for number , then the tagger can effectively model agreement in simple noun phrases , by having a higher probability for a singular determiner followed by a singular noun that it does for a singular determiner followed by a plural noun .	S-42
BKG	Theory on its own does not help much in deciding which point of view should dominate .	S-43
OWN	Two experiments were conducted on three corpora : 300 k words of Swedish text from the ECI Multilingual CD-ROM , and 100 k words each of English and French from a corpus of International Telecommunications Union text .	S-44
OWN	In the first experiment the whole of each corpus was used to train the model , and a small sample from the same text was used as test data .	S-45
OWN	For the second experiment , 95 % of the corpus was used in training and the remainder in testing .	S-46
OWN	The importance of the second test is that it includes unknown words , which are difficult to tag .	S-47
OWN	The tagsets were progressively modified , by textually substituting simplified tags for the original ones and e e-running the training and test procedures using the modified corpora .	S-48
OWN	The changes to the tagset are listed below .	S-49
OWN	In the results that follow , we will identify tagset that include a given distinction with an uppercase letter and ones that do not with a lowercase letter ; for example G for a tagset that marks gender , and g for one that does not .	S-50
OWN	Swedish	S-51
OWN	The changes made were entirely based on inflections .	S-52
OWN	G Gender : masculine , neuter , common gender ( `` UTR '' in the tagset ) .	S-53
OWN	N Number : singular , plural .	S-54
OWN	D Definiteness : definite , indefinite .	S-55
OWN	C Case : nominative , genitive .	S-56
OWN	French	S-57
OWN	The changes other than V were based on inflections .	S-58
OWN	G Gender : masculine , feminine .	S-59
OWN	N Number : singular , plural .	S-60
OWN	P Person : identified as 1 st to 6th in the tagset .	S-61
OWN	V Verbs : treat avoir and etre as being the same as any other verb .	S-62
OWN	English	S-63
OWN	The changes here are more varied than for the other languages , and generally consisted of removing some of the finer subdivisions of the major classes .	S-64
OWN	The grouping of some of these changes is admittedly a little ad hoc , and was intended to give a good distribution of tagset sizes ; not all combinations were tried .	S-65
OWN	C Reduce specific conjunction classes to a common class , and simplify one adjective class .	S-66
OWN	A Simplify noun and adverb classes .	S-67
OWN	P Simplify pronoun classes .	S-68
OWN	N Number : all singular / plural distinctions removed .	S-69
OWN	V Use the same class for have , do and be as for other verbs .	S-70
OWN	The sizes of the resulting tagsets and the degree of ambiguity in the corpora which resulted appear below .	S-71
OWN	Accuracy figures quoted here are for ambiguous and unknown words only , and therefore factor out effects due to the varying degree of ambiguity as the tagset changes .	S-72
OWN	In fact , this is a rather approximate way of accounting for ambiguity , since it does not take the length of ambiguous sequences into account , and the accuracy is likely to deteriorate more on long sequences of ambiguous words than on short ones .	S-73
OWN	The tests were run using Good-Turing correction to the probability estimates ; that is , rather than estimating the probability of the transition from a tag i to a tag j as the count of transition from i to j in the training corpus divided by the total frequency of tag i , one was added to the count of all transitions , and the total tag frequencies adjusted correspondingly .	S-74
OWN	The purpose in using this correction is to correct for corpora which might not provide enough training data .	S-75
OWN	On the largest tagsets , the correction was found to give a very slight reduction in the accuracy for Swedish , and to improve the French and English accuracies by about 1.5 % , suggesting that it is indeed needed .	S-76
OWN	The first experiment , with no unknown words , gave accuracies on ambiguous words of 91 - 93 % for Swedish , 94 - 97 % for French and 85 - 90 % for English .	S-77
OWN	The results for English are surprisingly low ( for example , on the Penn treebank , the tagger gives an accuracy of 95 - 96 % ) , and may be due to long sequences of ambiguous words .	S-78
OWN	The results appear in table.	S-79
OWN	The figures include the degree of ambiguity , that is , the number of words in the corpus for which more than one tag was hypothesised .	S-80
OWN	The accuracy is plotted against the size of the tagset in figures-, where the numbers on the points correspond to the index of tagsets listed .	S-81
OWN	Summarising the patterns :	S-82
OWN	Swedish	S-83
OWN	Larger tagset generally gives higher accuracy .	S-84
OWN	The results are quite widely spread .	S-85
OWN	French	S-86
OWN	Clustered , with an accuracy on all tagsets which do not mark gender of around 96 % - 96.5 % ; when gender is marked 94 % - 94.5 % .	S-87
OWN	English	S-88
OWN	Larger tagset tends to give larger accuracy , though with less of a spread than for Swedish .	S-89
OWN	The sizes of the tagsets ranged from approximately 80 - 200 tags for Swedish , 35 - 90 for French , and 70 - 160 for English .	S-90
OWN	As discussed above , it is not clear what would happen with larger tagsets , but some experiments based on the Susanne corpus and using tagsets ranging from 236 to 425 tags suggest that the trend to higher accuracy continues with even bigger tagsets .	S-91
OWN	In the second experiment , the test corpora included `` unknown '' words , which had not been seen during training , and for which the tagger hypothesises all open-class tags .	S-92
OWN	Two results are interesting to look at here : the accuracy on the unknown words , and the accuracy on words which were ambiguous but were found in the training corpus .	S-93
OWN	The results , in outline , are :	S-94
OWN	Swedish	S-95
OWN	Similar results on known words to first experiment .	S-96
OWN	For unknown words , smaller tagsets give higher accuracy .	S-97
OWN	French	S-98
OWN	For ambiguous words , the pattern and accuracy were similar to first experiment .	S-99
OWN	For unknown words , the pattern of accuracies was again similar , with tagsets that do not include gender giving accuracies of 51 % - 52 % , and those which do giving 45 % - 46 % .	S-100
OWN	English	S-101
OWN	Ambiguous words gave similar results to the first test .	S-102
OWN	Unknown words show a weak tendency to give higher accuracy on smaller tagsets .	S-103
OWN	Typical accuracies on ambiguous words were 90 - 92 % , 93 - 97 % and 83 - 88 % for Swedish , French and English respectively , with the corresponding accuracies on unknown words being 25 - 50 % , 45 - 52 % and 44 - 58 % .	S-104
OWN	Tablelists the results , giving the tagset size , the degree of ambiguity and the accuracies on known ambiguous and unknown words .	S-105
OWN	The ambiguous word accuracy is plotted in figures-.	S-106
OWN	What seems to come out from these results is that there is not a consistent relationship between the size of the tagsets and the tagging accuracy .	S-107
OWN	The most common pattern was for a larger tagset to give higher accuracy , but there were notable exceptions in French ( where gender marking was the key factor ) , in Swedish unknown words ( which show the reverse trend ) and in English unknown words ( which show no very clear trend at all ) .	S-108
OWN	This seems to fit quite well with the difficulties that were suggested above in reasoning about the effect of tagset size .	S-109
OWN	The main conclusion of this paper is therefore that the knowledge engineering component of setting up a tagger should concentrate on optimising the tagset for external criteria , and that the internal criterion of tagset size does not show sufficient generality to be taken into account without prior knowledge of properties of the language .	S-110
OWN	Perhaps this is not too surprising , but it is useful to have an experimental confirmation that the linguistics matters rather than the engineering .	S-111
OWN	One final observation about the experiments : the accuracy on unknown words was very low in all of the tests , and was particularly bad in Swedish .	S-112
OWN	The tagger used in the experiments took a very simple-minded approach to unknown words .	S-113
OWN	An alternative that is often used is to limit the possible tags using a simple morphological analysis or some other examination of the surface form of the word .	S-114
OWN	For example , in a variant of the English tagger which was not used in these experiments , a module which reduces the range of possible tags based on testing for only seven surface characteristics such as capitalisation and word endings improved the unknown word accuracy by 15 - 20 .	S-115
OWN	The results above show that if it were not for unknown words , there might be some argument for favouring larger tagsets , since they have some tendency to give a higher accuracy .	S-116
OWN	A tentative experiment on the contribution of using morphological or surface analysis in French and Swedish was therefore carried out .	S-117
OWN	Firstly , in both languages , the unknown words from the second experiment were looked up in the lexicon trained from the full corpus to see what tags they might have .	S-118
OWN	For Swedish , 96 % of the unknown words came from inflected classes , and had a single tag ; for French the figure was about 60 % .	S-119
OWN	In both cases , very few of the unknown words ( less than 1 % ) had more than one tag .	S-120
OWN	This provides some hope that an inflectional analysis might should help considerably with unknown words .	S-121
OWN	For confirmation , the list of French unknown words was given to a French grammarian , who predicted that it would be possible to make a good guess at the correct tag from the morphology for around 70 % of the words , and could narrow down the possible tags to two or three for about a further 25 % .	S-122
OWN	However , further research is needed to determine how realistic these estimates turn out to be .	S-123
AIM	We have shown how a simple experiment in changing the tagset shows that the relationship between tagset size and accuracy is a weak one and is not consistent against languages .	S-124
CTR	This seems to go against the `` folklore '' of the tagging community , where smaller tagsets are often held to be better for obtaining good accuracy .	S-125
OWN	I have suggested that what is important is to choose the tagset required for the application , rather than to optimise it for the tagger .	S-126
OWN	A follow-up to this work might be to apply similar tests in other languages to provide a further confirmation of the results , and to see if language families which similar characteristics can be identified .	S-127
OWN	A further conclusion might be that when a corpus is being tagged by hand , a large tagset should be used , since it can always be reduced to a smaller one if the application demands it .	S-128
OWN	Perhaps the major factor we have to set against this is the danger of introducing more human errors into the manual tagging process , by increasing the cognitive load on the human annotators .	S-129
