<?xml version='1.0' encoding='ISO-8859-1'?>
<!DOCTYPE PAPER SYSTEM "paper-structure.dtd">
<PAPER>
<METADATA>
<FILENO>9503023</FILENO>
<APPEARED><CONFERENCE>EACL</CONFERENCE><YEAR>1995</YEAR></APPEARED>
<CLASSIFICATION> Lg.Nn </CLASSIFICATION>
</METADATA>
<TITLE> A fast partial parse of natural language sentences using a connectionist method </TITLE>
<AUTHORLIST>
<AUTHOR>Caroline Lyon</AUTHOR>
<AUTHOR>Bob Dickerson</AUTHOR>
</AUTHORLIST>
<ABSTRACT>
<A-S ID='A-0' DOCUMENTC='S-0' AZ='OWN'> The pattern matching capabilities of neural networks can be used to locate syntactic constituents of natural language . </A-S>
<A-S ID='A-1' AZ='AIM' DOCUMENTC='S-19;S-20;S-23'> This paper describes a fully automated hybrid system , using neural nets operating within a grammatic framework . </A-S>
<A-S ID='A-2' AZ='OWN'> It addresses the representation of language for connectionist processing , and describes methods of constraining the problem size . </A-S>
<A-S ID='A-3' AZ='OWN'> The function of the network is briefly explained , and results are given . </A-S>
</ABSTRACT>
<BODY>
<DIV DEPTH='1'>
<HEADER ID='H-0'> Introduction </HEADER>
<P>
<S ID='S-0' ABSTRACTC='A-0' AZ='AIM'> The pattern matching capabilities of neural networks can be used to detect syntactic constituents of natural language . </S>
<S ID='S-1' AZ='CTR'> This approach bears comparison with probabilistic systems , but has the advantage that negative as well as positive information can be modelled . </S>
<S ID='S-2' AZ='OWN'> Also , most computation is done in advance , when the nets are trained , so the run time computational load is low . </S>
<S ID='S-3' AZ='AIM'> In this work neural networks are used as part of a fully automated system that finds a partial parse of declarative sentences . </S>
<S ID='S-4' AZ='OWN'> The connectionist processors operate within a grammatic framework , and are supported by pre-processors that filter the data and reduce the problem to a computationally tractable size . </S>
<S ID='S-5' AZ='OWN'> A prototype can be accessed via the Internet , on which users can try their own text ( details from the authors ) . </S>
<S ID='S-6' AZ='OWN'> It will take a sentence , locate the subject and then find the head of the subject . </S>
<S ID='S-7' AZ='OWN'> Typically 10 sentences take about 2 seconds , 50 sentences about 4 seconds , to process on a Sparc10 workstation . </S>
<S ID='S-8' AZ='OWN'> Using the prototype on technical manuals the subject and its head can be detected in over 90 % of cases ( See Section <CREF/> ) . </S>
</P>
<P>
<S ID='S-9' AZ='OWN'> The well known complexity of parsing is addressed by decomposing the problem , and then locating one syntactic constituent at a time . </S>
<S ID='S-10' AZ='OWN'> The sentence is first decomposed into the broad syntactic categories  </S>
</P>
<P>
<S ID='S-11' AZ='OWN'> pre-subject - subject - predicate </S>
</P>
<P>
<S ID='S-12' AZ='OWN'> by locating the subject </S>
<S ID='S-13' AZ='OWN'> Then these constituents can be processed further . </S>
<S ID='S-14' AZ='OWN'> The underlying principle employed at each step is to take a sentence , or part of a sentence , and generate strings with the boundary markers of the syntactic constituent in question placed in all possible positions . </S>
<S ID='S-15' AZ='OWN'> Then a neural net selects the string with the correct placement . </S>
</P>
<P>
<S ID='S-16' AZ='OWN'> This paper gives an overview of how natural language is converted to a representation that the neural nets can handle , and how the problem is reduced to a manageable size . </S>
<S ID='S-17' AZ='OWN'> It then outlines the neural net selection process . </S>
<S ID='S-18' AZ='BAS'> A comprehensive account is given in <REF TYPE='A' SELF="YES">Lyon 1994</REF> ; descriptions of the neural net process are also in <REF SELF="YES" TYPE='A'>Lyon 1993</REF> and <REF SELF="YES" TYPE='A'>Lyon and Frank 1992</REF> . </S>
<S ID='S-19' ABSTRACTC='A-1' AZ='OWN'> This is a hybrid system . </S>
<S ID='S-20' ABSTRACTC='A-1' AZ='OWN'> The core process is data driven , as the parameters of the neural networks are derived from training text . </S>
<S ID='S-21' AZ='OWN'> The neural net is trained in supervised mode on examples that have been manually marked `` correct '' and `` incorrect '' . </S>
<S ID='S-22' AZ='OWN'> It will then be able to classify unseen examples . </S>
<S ID='S-23' ABSTRACTC='A-1' AZ='OWN'> However , the initial processing stages , in which the problem size is constrained , operate within a skeletal grammatic framework . </S>
<S ID='S-24' AZ='OWN'> Computational tractability is further addressed by reducing data through the application of prohibitive rules as local constraints . </S>
<S ID='S-25' AZ='OWN'> The pruning process is remarkably effective . </S>
</P>
<IMAGE ID='I-0'/>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-1'> The corpus of sentences from technical manuals </HEADER>
<P>
<S ID='S-26' AZ='OWN'> This work has principally been developed on text of technical manuals from Perkins Engines Ltd. , which have been translated by a semi-automatic process <REF TYPE='P'>Pym 1993</REF> . </S>
<S ID='S-27' AZ='OWN'> Now , a partial parse can support such a process . </S>
<S ID='S-28' AZ='OWN'> For instance , frequently occurring modal verbs such as `` must '' are not distinguished by number in English , but they are in many other languages . </S>
<S ID='S-29' AZ='OWN'> It is necessary to locate the subject , then identify the head and determine its number in order to translate the main verb correctly in sentences like <CREF/> below . </S>
</P>
<EXAMPLE ID='E-0'>
<EX-S> If a cooler is fitted to the gearbox , [ the pipe [ connections ] of the cooler ] must be regularly checked for corrosion . </EX-S>
</EXAMPLE>
<P>
<S ID='S-30' AZ='OWN'> This parser has been trained to find the syntactic subject head that agrees in number with the main verb . </S>
<S ID='S-31' AZ='OWN'> The manuals are written using the PACE ( Perkins Approved Clear English ) guidelines , with the aim of producing clear , unambiguous texts . </S>
<S ID='S-32' AZ='OWN'> All declarative sentences have been extracted for processing : about half were imperatives . </S>
<S ID='S-33' AZ='OWN'> This level of classification can be done automatically in future . </S>
<S ID='S-34' AZ='OWN'> Table <CREF/> and Figure <CREF/> show some of the characteristics of the corpus . </S>
</P>
<IMAGE ID='I-1'/>
<P>
<S ID='S-35' AZ='OWN'> Punctuation marks are counted as words , formulae as 1 word . </S>
</P>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-2'> Language Representation (I) </HEADER>
<P>
<S ID='S-36' AZ='OWN'> In order to reconcile computational feasibility to empirical realism an appropriate form of language representation is critical . </S>
<S ID='S-37' AZ='OWN'> The first step in constraining the problem size is to partition an unlimited vocabulary into a restricted number of part-of-speech tags . </S>
<S ID='S-38' AZ='OWN'> Different stages of processing place different requirements on the classification system , so customised tagsets have been developed . </S>
<S ID='S-39' AZ='OWN'> For the first processing stage we need to place the subject markers , and , as a further task , disambiguate tags . </S>
<S ID='S-40' AZ='OWN'> It was not found necessary to use number information at this stage . </S>
<S ID='S-41' AZ='OWN'> For example , consider the sentence : </S>
</P>
<EXAMPLE ID='E-1'>
<EX-S> Still waters run deep . </EX-S>
</EXAMPLE>
<P>
<S ID='S-42' AZ='OWN'> The word `` waters '' could be a 3rd person , singular , present verb or a plural noun . </S>
<S ID='S-43' AZ='OWN'> However , in order to disambiguate the tag and place the subject markers it is only necessary to know that it is a noun or else a verb . </S>
<S ID='S-44' AZ='OWN'> The sentence parsed at the first level returns : </S>
</P>
<EXAMPLE ID='E-2'>
<EX-S> [ Still waters ] run deep . </EX-S>
</EXAMPLE>
<P>
<S ID='S-45' AZ='OWN'> The tagset used at this stage , mode 1 , has 21 classes , not distinguished for number . </S>
<S ID='S-46' AZ='OWN'> However , the head of the subject is then found and number agreement with the verb can be assessed . </S>
<S ID='S-47' AZ='OWN'> At this stage the tagset , mode 2 , includes number information and has 28 classes . </S>
<S ID='S-48' AZ='OWN'> Devising optimal tagsets for given tasks is a field in which further work is planned . </S>
<S ID='S-49' AZ='OWN'> We need larger tagsets to capture more linguistic information , but smaller ones to constrain the computational load . </S>
<S ID='S-50' AZ='OWN'> Information theoretic tools can be used to find the entropy of different tag sequence languages , and support decisions on representation . </S>
</P>
<P>
<S ID='S-51' AZ='OWN'> A functional approach is taken to tagging : words are allocated to classes depending on their syntactic role . </S>
<S ID='S-52' AZ='OWN'> For instance , superlative adjectives can act as nouns , so they are initially given the 2 tags : noun or adjective . </S>
<S ID='S-53' AZ='OWN'> This approach can be extended by taking adjacent words which act jointly as single lexical items as a unit . </S>
<S ID='S-54' AZ='OWN'> Thus the pair `` most  &#60;  adjective  >  '' is taken as a single superlative adjective . </S>
</P>
<P>
<S ID='S-55' AZ='OWN'> Text is automatically tagged using the first modules of the CLAWS program ( 1985 version ) , in which words are allocated one or more tags from 134 classes <REF TYPE='P'>Garside 1987</REF> . </S>
<S ID='S-56' AZ='OWN'> These 134 tags are then mapped onto the small customised tagsets . </S>
<S ID='S-57' AZ='OWN'> Tag disambiguation is part of the parsing task , handled by the neural net and its pre-processor . </S>
<S ID='S-58' AZ='OWN'> This version of CLAWS has a dictionary of about 6,300 words only . </S>
<S ID='S-59' AZ='OWN'> Other words are tagged using suffix information , or else defaults are invoked . </S>
<S ID='S-60' AZ='OWN'> The correct tag is almost always included in the set allocated , but more tags than necessary are often proposed . </S>
<S ID='S-61' AZ='OWN'> A larger dictionary in later versions will address this problem . </S>
</P>
<DIV DEPTH='2'>
<HEADER ID='H-3'> Representing syntactic boundary markers </HEADER>
<P>
<S ID='S-62' AZ='BKG'> In the same way that tags are allocated to words , or to punctuation marks , they can represent the boundaries of syntactic constituents , such as noun phrases and verb phrases . </S>
<S ID='S-63' AZ='OTH'> Boundary markers can be considered invisible tags , or hypertags , which have probabilistic relationships with adjacent tags in the same way that words do . </S>
<S ID='S-64' AZ='OTH'> <REF TYPE='A'>Atwell 1987</REF> and <REF TYPE='A'>Church 1989</REF> have used this approach . </S>
<S ID='S-65' AZ='CTR'> If embedded syntactic constituents are sought in a single pass , this can lead to computation
al overload <REF TYPE='P'>Pocock and Atwell 1994</REF> . </S>
<S ID='S-66' AZ='OWN'> Our approach uses a similar concept , but differs in that embedded syntactic constituents are detected one at a time in separate steps . </S>
<S ID='S-67' AZ='OWN'> There are only 2 hypertags - the opening and closing brackets marking the possible location ( s ) of the syntactic constituent in question . </S>
<S ID='S-68' AZ='OWN'> Using this representation a hierarchical language structure is converted to a string of tags represented by a linear vector . </S>
</P>
<IMAGE ID='I-2'/>
</DIV>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-4'> Constraining the generation of candidate strings </HEADER>
<P>
<S ID='S-69' AZ='OWN'> This system generates sets of tag strings for each sentence , with the hypertags placed in all possible positions . </S>
<S ID='S-70' AZ='OWN'> Thus , for the subject detection task : </S>
</P>
<EXAMPLE ID='E-3'>
<EX-S> Then the performance of the pump must be monitored . </EX-S>
</EXAMPLE>
<P>
<S ID='S-71' AZ='OWN'> will generate strings of tags including : </S>
</P>
<EXAMPLE ID='E-4'>
<EX-S> [ Then ] the performance of the pump must be monitored . </EX-S>
<EX-S> performance of the pump must be monitored . </EX-S>
<EX-S> Then [ the performance of the ] pump must be monitored . </EX-S>
<EX-S> Then [ the performance of the pump ] must be monitored . </EX-S>
</EXAMPLE>
<P>
<S ID='S-72' AZ='OWN'> Hypertags are always inserted in pairs , so that closure is enforced . </S>
<S ID='S-73' AZ='OWN'> There were arbitrary limits of a maximum of 10 words in the pre-subject and 10 words within the subject for the initial work described here . </S>
<S ID='S-74' AZ='OWN'> These are now extended to 15 words in the pre-subject , 12 in the subject - see Section <CREF/> . </S>
<S ID='S-75' AZ='OWN'> There must be at least one word beyond the end of the subject and before the end-of-sentence mark . </S>
<S ID='S-76' AZ='OWN'> Therefore , using the initial restrictions , in a sentence of 22 words or more ( counting punctuation marks as words ) there could be 100 alternative placements . </S>
<S ID='S-77' AZ='OWN'> However , some words will have more than one possible tag . </S>
<S ID='S-78' AZ='OWN'> For instance , in sentence <CREF/> above 5 words have 2 alternative tags , which will generate <EQN/> possible strings before the hypertags are inserted . </S>
<S ID='S-79' AZ='OWN'> Since there are 22 words ( including punctuation ) the total number of strings would be <EQN/> </S>
<S ID='S-80' AZ='OWN'> It is not feasible to detect one string out of this number : if the classifier marked all strings incorrect the percentage wrongly classified would only be <EQN/> , yet it would be quite useless . </S>
<S ID='S-81' AZ='OWN'> In order to find the correct string most of the outside candidates must be dropped ,  </S>
</P>
<DIV DEPTH='2'>
<HEADER ID='H-5'> The skeletal grammatic framework </HEADER>
<P>
<S ID='S-82' AZ='BAS'> A minimal grammar , set out in <REF TYPE='A' SELF="YES">Lyon 1994</REF> in EBNF form , is composed of 9 rules . </S>
<S ID='S-83' AZ='OTH'> For instance , the subject must contain a noun-type word . </S>
<S ID='S-84' AZ='OTH'> Applying this particular rule to sentence <CREF/> above would eliminate candidate strings <CREF/> and <CREF/> . </S>
<S ID='S-85' AZ='OTH'> We also have the 2 arbitrary limits on length of pre-subject and subject . </S>
<S ID='S-86' AZ='OWN'> There is a small set of 4 extensions to the grammar , or semi-local constraints . </S>
<S ID='S-87' AZ='OWN'> For instance , if a relative pronoun occurs , then a verb must follow in that constituent . </S>
<S ID='S-88' AZ='OWN'> On the technical manuals the constraints of the grammatic framework put up to 6 % of declarative sentences outside our system , most commonly because the pre-subject is too long . </S>
<S ID='S-89' AZ='OWN'> A small number are excluded because the system cannot handle a co-ordinated head . </S>
<S ID='S-90' AZ='OWN'> With the length of pre-subject extended to 15 words , and subject to 12 words , an average of 2 % are excluded ( 7 out of 351 ) . </S>
</P>
</DIV>
<DIV DEPTH='2'>
<HEADER ID='H-6'> Prohibition Tables </HEADER>
<P>
<S ID='S-91' AZ='OWN'> The grammatic framework alone does not reduce the number of candidate strings sufficiently for the subject detection stage . </S>
<S ID='S-92' AZ='BAS'> This problem is addressed further by a method suggested by <REF TYPE='A'>Barton et al. 1987</REF> that local constraints can rein in the generation of an intractable number of possibilities . </S>
<S ID='S-93' AZ='OWN'> In our system the local constraints are prohibited tag pairs and triples . </S>
<S ID='S-94' AZ='OWN'> These are adjacent tags which are not allowed , such as `` determiner - verb or '' `` start of subject - verb '' . </S>
<S ID='S-95' AZ='OWN'> If during the generation of a candidate string a prohibited tuple is encountered , then the process is aborted . </S>
<S ID='S-96' AZ='OWN'> There are about 100 prohibited pairs and 120 triples . </S>
<S ID='S-97' AZ='OWN'> By using these methods the number of candidate strings is drastically reduced . </S>
<S ID='S-98' AZ='OWN'> For the technical manuals an average of 4 strings , seldom more than 15 strings , are left . </S>
<S ID='S-99' AZ='OWN'> Around 25 % of sentences are left with a single string . </S>
<S ID='S-100' AZ='OWN'> These filters or `` rules '' differ fundamentally from generative rules that produce allowable strings in a language . </S>
<S ID='S-101' AZ='OWN'> In those cases only productions that are explicitly admitted are allowed . </S>
<S ID='S-102' AZ='OWN'> Here , in contrast , anything that is not expressly prohibited is allowed . </S>
<S ID='S-103' AZ='OWN'> At this stage the data is ready to present to the neural net . </S>
<S ID='S-104' AZ='OWN'> Figure <CREF/> gives an overview of the whole process . </S>
</P>
</DIV>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-7'> Language Representation (II) </HEADER>
<P>
<S ID='S-105' AZ='OWN'> Different network architectures have been investigated , but they all share the same input and output representation . </S>
<S ID='S-106' AZ='OWN'> The output from the net is a vector whose 2 elements , or nodes , represent `` correct '' and `` incorrect '' , `` yes '' and `` no '' - see Figure <CREF/> . </S>
<S ID='S-107' AZ='OWN'> The input to the net is derived from the candidate strings , the sequences of tags and hypertags . </S>
<S ID='S-108' AZ='OWN'> These must be converted to binary vectors . </S>
<S ID='S-109' AZ='OWN'> Each element of the vector will represent a feature that is flagged 0 or 1 , absent or present . </S>
</P>
<P>
<S ID='S-110' AZ='OWN'> Though the form in which the vector is written may give an illusion of representing order , no sequential order is maintained . </S>
<S ID='S-111' AZ='OWN'> A method of representing a sequence must be chosen . </S>
<S ID='S-112' AZ='OWN'> The sequential order of the input is captured here , partially , by taking adjacent tags , pairs and triples , as the feature elements . </S>
<S ID='S-113' AZ='OWN'> The individual tags are converted to a bipos and tripos representation . </S>
<S ID='S-114' AZ='OWN'> Using this method each tag is in 3 tripos and 2 bipos elements . </S>
<S ID='S-115' AZ='OWN'> This highly redundant code will aid the processing of sparse data typical of natural language . </S>
</P>
<P>
<S ID='S-116' AZ='OWN'> For most of the work described here the sentence was dynamically truncated 2 words beyond the hypertag marking the close of the subject . </S>
<S ID='S-117' AZ='OWN'> This process has now been improved by going further along the sentence . </S>
</P>
<IMAGE ID='I-3'/>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-8'> The function of the net </HEADER>
<P>
<S ID='S-118' AZ='BAS'> The net that gave best results was a simple single layer net ( Figure <CREF/> ) , derived from the Hodyne net of <REF TYPE='A'>Wyard and Nightingale 1990</REF> . </S>
<S ID='S-119' AZ='OWN'> This is conventionally a `` single layer '' net , since there is one layer of processing nodes . </S>
<S ID='S-120' AZ='OWN'> Multi-layer networks , which can process linearly inseparable data , were also investigated , but are not necessary for this particular processing task . </S>
<S ID='S-121' AZ='OWN'> The linear separability of data is related to its order , and this system uses higher order pairs and triples as input . </S>
<S ID='S-122' AZ='OWN'> The question of appropriate network architecture is examined in <REF TYPE='A'>Pao 1989</REF> , <REF TYPE='A'>Widrow and Lehr 1992</REF> and <REF  TYPE='A' SELF="YES">Lyon 1994</REF> . </S>
</P>
<DIV DEPTH='2'>
<HEADER ID='H-9'> The training process </HEADER>
<P>
<S ID='S-123' AZ='OWN'> The net is presented with training strings whose desired classification has been manually marked . </S>
<S ID='S-124' AZ='OWN'> The weights on the connections between input and output nodes are adjusted until a required level of performance is reached . </S>
<S ID='S-125' AZ='OWN'> Then the weights are fixed and the trained net is ready to classify unseen sentences . </S>
<S ID='S-126' AZ='OWN'> The prototype accessible via the Internet has been trained on sentences from the technical manuals , slightly augmented . </S>
</P>
<P>
<S ID='S-127' AZ='OWN'> Initially the weighted links are disabled . </S>
<S ID='S-128' AZ='OWN'> When a string is presented to the network in training mode , it activates a set of input nodes . </S>
<S ID='S-129' AZ='OWN'> If an input node is not already linked to the output node representing the desired response , it will be connected and the weight on the connection will be initialised to 1.0 . </S>
<S ID='S-130' AZ='OWN'> Most input nodes are connected to both outputs , since most tuples occur in both grammatical and ungrammatical strings . </S>
<S ID='S-131' AZ='OWN'> However , some will only be connected to one output - see Figure <CREF/> . </S>
</P>
<P>
<S ID='S-132' AZ='OWN'> The input layer potentially has a node for each possible tuple . </S>
<S ID='S-133' AZ='OWN'> With 28 tags , 2 hypertags and a start symbol the upper bound on the number of input nodes is <EQN/> . </S>
<S ID='S-134' AZ='OWN'> In practice the maximum activated is currently about 1000 . </S>
<S ID='S-135' AZ='OWN'> In testing mode , if a previously unseen tuple appears it makes zero contribution to the result . </S>
<S ID='S-136' AZ='OWN'> The activations at the input layer are fed forward through the weighted connections to the output nodes , where they are summed . </S>
<S ID='S-137' AZ='OWN'> The highest output marks the winning node . </S>
<S ID='S-138' AZ='OWN'> If the desired node wins , then no action is taken . </S>
<S ID='S-139' AZ='OWN'> If the desired node does not win , then the weight on connections to the desired node are incremented , while the weights on connections to the unwanted node are decremented . </S>
</P>
<P>
<S ID='S-140' AZ='OWN'> This algorithm differs from some commonly used methods . </S>
<S ID='S-141' AZ='OWN'> In feed forward networks trained in supervised mode to perform a classification task different penalty measures can be used to trigger a weight update . </S>
<S ID='S-142' AZ='OWN'> Back propagation and some single layer training methods typically minimise a metric based on the least squared error ( LSE ) between desired and actual activation of the output nodes . </S>
<S ID='S-143' AZ='OWN'> The reason why a differentiable error measure of this sort is necessary for multi-layer nets is well documented <REF TYPE='P'>Rumelhart and McClelland 1986</REF> . </S>
<S ID='S-144' AZ='OWN'> However , for single layer nets we can choose to update weights directly : the error at an output node can trigger weight updates on the connections that feed it . </S>
<S ID='S-145' AZ='OWN'> Solutions with LSE are not necessarily the same as minimising the number of misclassifications , and for certain types of data this second method of direct training may be appropriate . </S>
<S ID='S-146' AZ='OWN'> Now , in the natural language domain it is desirable to get information from infrequent as well as common events . </S>
<S ID='S-147' AZ='OWN'> Rare events , rather than being noise , can make a useful contribution to a classification task . </S>
<S ID='S-148' AZ='OWN'> We need a method that captures information from infrequent events , and adopt a direct measure of misclassification . </S>
<S ID='S-149' AZ='OWN'> This may be better suited to data with a `` Zipfian '' distribution <REF TYPE='P'>Shannon 1951</REF> . </S>
</P>
<P>
<S ID='S-150' AZ='OWN'> The update factor is chosen to meet several requirements . </S>
<S ID='S-151' AZ='OWN'> It should always be positive , and asymptotic to maximum and minimum bounds . </S>
<S ID='S-152' AZ='OWN'> The factor should be greatest in the central region , least as it moves away in either direction . </S>
<S ID='S-153' AZ='OWN'> We are currently still using the original Hodyne function because it works well in practice . </S>
<S ID='S-154' AZ='OWN'> The update factor is given in the following formula . </S>
<S ID='S-155' AZ='OWN'> If <EQN/> for strengthening weights and <EQN/> for weakening them , then  </S>
</P>
<IMAGE ID='I-4'/>
<P>
<S ID='S-156' AZ='OWN'> Recall that weights are initialised to 1.0 . </S>
<S ID='S-157' AZ='OWN'> After training we find that the weight range is bounded by  </S>
</P>
<IMAGE ID='I-5'/>
<P>
<S ID='S-158' AZ='OWN'> Total time for training is measured in seconds . </S>
<S ID='S-159' AZ='OWN'> The number of iterative cycles that are necessary depends on the threshold chosen for the trained net to cross , and on details of the vector representation . </S>
<S ID='S-160' AZ='OWN'> The demonstration prototype takes about 15 seconds . </S>
<S ID='S-161' AZ='OWN'> With the most recent improved representation about 1000 strings can be trained in 1 second , to 97 % . </S>
<S ID='S-162' AZ='OWN'> The results from using these nets are given in Table <CREF/> . </S>
<S ID='S-163' AZ='OWN'> It was found that triples alone gave as good results as pairs and triples together . </S>
<S ID='S-164' AZ='OWN'> And though the nets easily train to 99 % correct , the lower threshold gives slightly better generalisation and thus gives better results on the test data . </S>
</P>
</DIV>
<DIV DEPTH='2'>
<HEADER ID='H-10'> The testing process </HEADER>
<P>
<S ID='S-165' AZ='OWN'> When the trained net is run on unseen data the weights on the links are fixed . </S>
<S ID='S-166' AZ='OWN'> Any link that is still disabled is activated and initialised to 0 , so that tuples which have not occurred in the training corpus make no contribution to the classification task . </S>
<S ID='S-167' AZ='OWN'> Sentences are put through the pre-processer one at a time and the candidate strings which are generated are then presented to the network . </S>
<S ID='S-168' AZ='OWN'> The output is now interpreted differently . </S>
<S ID='S-169' AZ='OWN'> The difference between the `` yes '' and `` no '' activation levels is recorded for each string , and this score is considered a measure of grammaticality , <EQN/> . </S>
<S ID='S-170' AZ='OWN'> The string with the highest <EQN/> score is taken as the correct one . </S>
</P>
<P>
<S ID='S-171' AZ='OWN'> For the results given below , the networks were trained on part of the corpus and tested on another part of the corpus . </S>
<S ID='S-172' AZ='OWN'> For the prototype in which users can process their own text , the net was trained on the whole corpus , slightly augmented . </S>
</P>
<IMAGE ID='I-6'/>
<IMAGE ID='I-7'/>
</DIV>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-11'> Results </HEADER>
<P>
<S ID='S-173' AZ='OWN'> There are several measures of correctness that can be taken when results are evaluated . </S>
<S ID='S-174' AZ='OWN'> The most lenient is whether or not the subject and head markers are placed correctly - the type of measure used in the IBM / Lancaster work <REF TYPE='P'>Black et al. 1993</REF> . </S>
<S ID='S-175' AZ='OWN'> Since we are working towards a hierarchical language structure , we may want the words within constituents correctly tagged , ready for the next stage of processing . </S>
<S ID='S-176' AZ='OWN'> `` correct - A '' also requires that the words within the subject are correctly tagged . </S>
<S ID='S-177' AZ='OWN'> The results in Tables <CREF/> and <CREF/> give an indication of performance levels . </S>
</P>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-12'> Using negative information </HEADER>
<P>
<S ID='S-178' AZ='OWN'> When parses are postulated for a sentence negative as well as positive examples are likely to occur . </S>
<S ID='S-179' AZ='OWN'> Now , in natural language negative correlations are an important source of information : the occurrence of some words or groups of words inhibit others from following . </S>
<S ID='S-180' AZ='OWN'> We wish to exploit these constraints . </S>
<S ID='S-181' AZ='OTH'> <REF TYPE='A'>Brill et al. 1990</REF> recognised this , and introduced the idea of distituents . </S>
<S ID='S-182' AZ='OTH'> These are elements of a sentence that should be separated , as opposed to elements of constituents that cling together . </S>
<S ID='S-183' AZ='OTH'> <REFAUTHOR>Brill</REFAUTHOR> addresses the problem of finding a valid metric for distituency by using a generalized mutual information statistic . </S>
<S ID='S-184' AZ='OTH'> Distituency is marked by a mutual information minima . </S>
<S ID='S-185' AZ='OTH'> His method is supported by a small 4 rule grammar . </S>
</P>
<P>
<S ID='S-186' AZ='CTR'> However , this approach does not fully capture the sense in which inhibitory factors play a negative and not just a neutral role . </S>
<S ID='S-187' AZ='OWN'> We want to distinguish between items that are unlikely to occur ever , and those that have just not happened to turn up in the training data . </S>
<S ID='S-188' AZ='OWN'> For example , in sentence [ CREF] above strings <CREF/> , <CREF/> and [ CREF] can never be correct . </S>
<S ID='S-189' AZ='OWN'> These should be distinguished from possibly correct parses that are not in the training data . </S>
<S ID='S-190' AZ='OTH'> In order that `` improbabilities '' can be modelled by inhibitory connections <REF TYPE='P'>Niles and Silverman 1990</REF> show how a Hidden Markov Model can be implemented by a neural network . </S>
</P>
<P>
<S ID='S-191' AZ='OTH'> The theoretical ground for incorporating negative examples in a language learning process originates with the work of <REF TYPE='A'>Gold 1967</REF> , developed by <REF TYPE='A'>Angluin 1980</REF> . </S>
<S ID='S-192' AZ='OTH'> He examined the process of learning the grammar of a formal language from examples . </S>
<S ID='S-193' AZ='OTH'> He showed that , for languages at least as high in the Chomsky hierarchy as CFGs , inference from positive data alone is strictly less powerful than inference from both positive and negative data together . </S>
<S ID='S-194' AZ='OTH'> To illustrate this informally consider a case of inference from a number of examples : as they are presented to the inference machine , possible grammars are postulated . </S>
<S ID='S-195' AZ='CTR'> However , with positive data alone a problem of over generalization arises : the postulated grammar may be a superset of the real grammar , and sentences that are outside the real grammar could be accepted . </S>
<S ID='S-196' AZ='OWN'> If both positive and negative data is used , counter examples will reduce the postulated grammar so that it is nearer the real grammar . </S>
<S ID='S-197' AZ='OWN'> <REFAUTHOR>Gold</REFAUTHOR> developed his theory for formal languages : it is argued that similar considerations apply here . </S>
<S ID='S-198' AZ='OTH'> A grammar may be inferred from positive examples alone for certain subsets of regular languages <REF TYPE='P'>Garcia and Vidal 1990</REF> , or an inference process may degenerate into a look up procedure if every possible positive example is stored . </S>
<S ID='S-199' AZ='OTH'> In these cases negative information is not required , but they are not plausible models for unbounded natural language . </S>
<S ID='S-200' AZ='OWN'> In our method the required parse is found by inferring the grammar from both positive and negative information , which is effectively modelled by the neural net . </S>
<S ID='S-201' AZ='OWN'> Future work will investigate the effect of training the networks on the positive examples alone . </S>
<S ID='S-202' AZ='OWN'> With our current size corpus there is not enough data . </S>
</P>
<DIV DEPTH='2'>
<HEADER ID='H-13'> Relationship between the neural net and prohibition table </HEADER>
<P>
<S ID='S-203' AZ='OWN'> The relationship between the neural net and the rules in the prohibition table should be seen in the following way . </S>
<S ID='S-204' AZ='OWN'> Any single rule prohibiting a tuple of adjacent tags could be omitted and the neural network would handle it by linking the node representing that tuple to `` no '' only . </S>
<S ID='S-205' AZ='OWN'> However , for some processing steps we need to reduce the number of candidate tag strings presented to the neural network to manageable proportions ( see Section <CREF/> ) . </S>
<S ID='S-206' AZ='OWN'> The data must be pre-processed by filtering through the prohibition rule constraints . </S>
<S ID='S-207' AZ='OWN'> If the number of candidate strings is within desirable bounds , such as for the head detection task , no rules are used . </S>
<S ID='S-208' AZ='OWN'> Our system is data driven as far as possible : the rules are invoked if they are needed to make the problem computationally tractable . </S>
</P>
</DIV>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-14'> Conclusion </HEADER>
<P>
<S ID='S-209' AZ='AIM'> Our working prototype indicates that the methods described here are worth developing , and that connectionist methods can be used to generalise from the training corpus to unseen text . </S>
<S ID='S-210' AZ='OWN'> Since data can be represented as higher order tuples , single layer networks can be used . </S>
<S ID='S-211' AZ='CTR'> The traditional problems of training times do not arise . </S>
<S ID='S-212' AZ='OTH'> We have also used multi-layer nets on this data : they have no advantages , and perform slightly less well <REF SELF="YES" TYPE='P'>Lyon 1994</REF> . </S>
</P>
<P>
<S ID='S-213' AZ='OWN'> The supporting role of the grammatic framework and the prohibition filters should not be underestimated . </S>
<S ID='S-214' AZ='OWN'> Whenever the scope of the system is extended it has been found necessary to enhance these elements . </S>
</P>
<P>
<S ID='S-215' AZ='OWN'> The most laborious part of this work is preparing the training data . </S>
<S ID='S-216' AZ='OWN'> Each time the representation is modified a new set of strings is generated that need marking up . </S>
<S ID='S-217' AZ='OWN'> An autodidactic check is now included which speeds up this task . </S>
<S ID='S-218' AZ='OWN'> We run marked up training data through an early version of the network trained on the same data , so the results should be almost all correct . </S>
<S ID='S-219' AZ='OWN'> If an `` incorrect '' parse occurs we can then check whether that sentence was properly marked up . </S>
</P>
<P>
<S ID='S-220' AZ='OWN'> Some of the features of the system described here could be used in a stochastic process . </S>
<S ID='S-221' AZ='OWN'> However , connectionist methods have low computational loads at runtime . </S>
<S ID='S-222' AZ='OWN'> Moreover , they can utilise more of the implicit information in the training data by modelling negative relationships . </S>
<S ID='S-223' AZ='OWN'> This is a powerful concept that can be exploited in the effort to squeeze out every available piece of useful information for natural language processing . </S>
</P>
<P>
<S ID='S-224' AZ='OWN'> Future work is planned to extend this very limited partial parser , and decompose sentences further into their hierarchical constituent parts . </S>
<S ID='S-225' AZ='OWN'> In order to do this a number of subsidiary tasks will be addressed . </S>
<S ID='S-226' AZ='OWN'> The system is being improved by identifying groups of words that act as single lexical items . </S>
<S ID='S-227' AZ='OWN'> The decomposition of the problem can be investigated further : for instance , should the tag disambiguation task precede the placement of the subject boundary markers in a separate step ? </S>
<S ID='S-228' AZ='OWN'> More detailed investigation of language representation issues will be undertaken . </S>
<S ID='S-229' AZ='OWN'> And the critical issues of investigating the most appropriate network architectures will be carried on . </S>
</P>
</DIV>
</BODY>
<REFERENCELIST>
<REFERENCE>
D. <SURNAME>Angluin</SURNAME>.
<DATE>1980</DATE>.
Inductive inference of formal languages from positive data.
Information and Control, 45.
</REFERENCE>
<REFERENCE>
E. <SURNAME>Atwell</SURNAME>.
<DATE>1987</DATE>.
Constituent-likelihood grammar.
In R Garside, G Leech, and G Sampson, editors, The Computational
  Analysis of English: a corpus-based approach. Longman.
</REFERENCE>
<REFERENCE>
G. E. <SURNAME>Barton</SURNAME>, R. C. <SURNAME>Berwick</SURNAME>, and E. S. <SURNAME>Ristad</SURNAME>.
<DATE>1987</DATE>.
Computational Complexity and Natural Language.
MIT Press.
</REFERENCE>
<REFERENCE>
E. <SURNAME>Black</SURNAME>, R. <SURNAME>Garside</SURNAME>, and G. <SURNAME>Leech</SURNAME>.
<DATE>1993</DATE>.
Statistically driven computer grammars of English: the
  IBM/Lancaster approach.
Rodopi.
</REFERENCE>
<REFERENCE>
E. <SURNAME>Brill</SURNAME>, D. <SURNAME>Magerman</SURNAME>, M. <SURNAME>Marcus</SURNAME> and B. <SURNAME>Santorini</SURNAME>.
<DATE>1990</DATE>.
Deducing linguistic structure from the statistics of large corpora.
In DARPA Speech and Natural Language Workshop.
</REFERENCE>
<REFERENCE>
K. W. <SURNAME>Church</SURNAME>, Bell <SURNAME>Laboratories</SURNAME>.
<DATE>1989</DATE>.
A stochastic parts program and noun phrase parser for unrestricted
  text.
In IEEE conference record  of ICASSP.
</REFERENCE>
<REFERENCE>
P. <SURNAME>Garcia</SURNAME> and E. <SURNAME>Vidal</SURNAME>.
<DATE>1990</DATE>.
Inference of k-testable languages in the strict sense and application
  to syntactic pattern recognition.
IEEE Trans. on Pattern Analysis and Machine Intelligence, 12.
</REFERENCE>
<REFERENCE>
R. <SURNAME>Garside</SURNAME>.
<DATE>1987</DATE>.
The CLAWS word-tagging system.
In R Garside, G Leech, and G Sampson, editors, The Computational
  Analysis of English: a corpus based approach. Longman.
</REFERENCE>
<REFERENCE>
E. M. <SURNAME>Gold</SURNAME>.
<DATE>1967</DATE>.
Language identification in the limit.
Information and Control, 10.
</REFERENCE>
<REFERENCE>
C. <SURNAME>Lyon</SURNAME>.
<DATE>1994</DATE>.
The representation of natural language to enable neural networks
  to detect syntactic features.
PhD thesis.
</REFERENCE>
<REFERENCE>
C. <SURNAME>Lyon</SURNAME>.
<DATE>1993</DATE>.
Using neural networks to infer grammatical structures in natural
  language.
In Proc. of IEE Colloquium on Grammatical Inference.
</REFERENCE>
<REFERENCE>
C. <SURNAME>Lyon</SURNAME> and R. <SURNAME>Frank</SURNAME>.
<DATE>1992</DATE>.
Detecting structures in natural language using a neural net with
   rules.
In Proc. of International Conference on Artificial Neural
   Networks (ICANN).
</REFERENCE>
<REFERENCE>
L. <SURNAME>Niles</SURNAME> and H. <SURNAME>Silverman</SURNAME>.
<DATE>1990</DATE>.
Combining Hidden Markov Models and Neural Network Classifiers.
In IEEE conference record of ICASSP.
</REFERENCE>
<REFERENCE>
Yohhan <SURNAME>Pao</SURNAME>.
<DATE>1989</DATE>.
Adaptive Pattern Recognition and Neural Networks.
Addison Wesley.
</REFERENCE>
<REFERENCE>
R. <SURNAME>Pocock</SURNAME> and E. <SURNAME>Atwell</SURNAME>.
<DATE>1994</DATE>.
Treebank trained probabilistic parsing of lattices.
School of Computer Studies, Leeds University.
In The Speech-Oriented Probabilistic Parser Project:
  Final Report to MoD.
</REFERENCE>
<REFERENCE>
P. <SURNAME>Pym</SURNAME>.
<DATE>1993</DATE>.
Perkins Engines and Publications.
In Proceedings of Technology and Language in Europe <DATE>2000</DATE>.
  DGXIII-E of the European Commission.
</REFERENCE>
<REFERENCE>
D. <SURNAME>Rumelhart</SURNAME> and J. <SURNAME>McClelland</SURNAME>.
<DATE>1986</DATE>.
Parallel Distributed Processing
MIT.
</REFERENCE>
<REFERENCE>
C. E. <SURNAME>Shannon</SURNAME>.
<DATE>1951</DATE>.
Prediction and Entropy of Printed English.
In Bell System Technical Journal.
</REFERENCE>
<REFERENCE>
B. <SURNAME>Widrow</SURNAME> and M. <SURNAME>Lehr</SURNAME>.
<DATE>1992</DATE>.
30 years of adaptive neural networks.
In Neural networks: theoretical foundations and analysis
      edited by C Lau.
IEEE press.
</REFERENCE>
<REFERENCE>
P. <SURNAME>Wyard</SURNAME> and C. <SURNAME>Nightingale</SURNAME>.
<DATE>1990</DATE>.
A  Single Layer Higher Order Neural Net and its Application to
  Context Free Grammar Recognition
In Connection Science, 4.
</REFERENCE>
</REFERENCELIST>
</PAPER>
