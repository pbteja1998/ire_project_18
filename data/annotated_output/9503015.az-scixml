AIM	The paper describes a parser for Categorial Grammar which provides fully word by word incremental interpretation .	A-0
OWN	The parser does not require fragments of sentences to form constituents , and thereby avoids problems of spurious ambiguity .	A-1
OWN	The paper includes a brief discussion of the relationship between basic Categorial Grammar and other formalisms such as HPSG , Dependency Grammar and the Lambek Calculus .	A-2
OWN	It also includes a discussion of some of the issues which arise when parsing lexicalised grammars , and the possibilities for using statistical techniques for tuning to particular languages .	A-3
BKG	There is a large body of psycholinguistic evidence which suggests that meaning can be extracted before the end of a sentence , and before the end of phrasal constituents,.	S-0
BKG	There is also recent evidence suggesting that , during speech processing , partial interpretations can be built extremely rapidly , even before words are completed.	S-1
BKG	There are also potential computational applications for incremental interpretation , including early parse filtering using statistics based on logical form plausibility , and interpretation of fragments of dialogues ( a survey is provided by, henceforth referred to as) .	S-2
BKG	In the current computational and psycholinguistic literature there are two main approaches to the incremental construction of logical forms .	S-3
OTH	One approach is to use a grammar with ` non-standard ' constituency , so that an initial fragment of a sentence , such as John likes , can be treated as a constituent , and hence be assigned a type and a semantics .	S-4
OTH	This approach is exemplified by Combinatory Categorial Grammar , CCG, which takes a basic CG with just application , and adds various new ways of combining elements together .	S-5
OTH	Incremental interpretation can then be achieved using a standard bottom-up shift reduce parser , working from left to right along the sentence .	S-6
OTH	The alternative approach , exemplified by the work ofon top-down parsing, andon left-corner parsingis to associate a semantics directly with the partial structures formed during a top-down or left-corner parse .	S-7
OTH	For example , a syntax tree missing a noun phrase , such as the following	S-8
OTH	can be given a semantics as a function from entities to truth values i.e.x. likes ( john , x ) , without having to say that John likes is a constituent .	S-9
CTR	Neither approach is without problems .	S-10
CTR	If a grammar is augmented with operations which are powerful enough to make most initial fragments constituents , then there may be unwanted interactions with the rest of the grammar ( examples of this in the case of CCG and the Lambek Calculus are given in Section) .	S-11
CTR	The addition of extra operations also means that , for any given reading of a sentence there will generally be many different possible derivations ( so-called ` spurious ' ambiguity ) , making simple parsing strategies such as shift-reduce highly inefficient .	S-12
CTR	The limitations of the parsing approaches become evident when we consider grammars with left recursion .	S-13
CTR	In such cases a simple top-down parser will be incomplete , and a left corner parser will resort to buffering the input ( so won't be fully word-by-word ) .	S-14
OTH	illustrate the problem by considering the fragment Mary thinks John .	S-15
OTH	This has a small number of possible semantic representations ( the exact number depending upon the grammar ) e.g.	S-16
OTH	The second representation is appropriate if the sentence finishes with a sentential modifier .	S-17
OTH	The third allows there to be a verb phrase modifier .	S-18
BKG	If the semantic representation is to be read off syntactic structure , then the parser must provide a single syntax tree ( possibly with empty nodes ) .	S-19
BKG	However , there are actually any number of such syntax trees corresponding to , for example , the first semantic representation , since the np and the s can be arbitrarily far apart .	S-20
BKG	The following tree is suitable for the sentence Mary thinks John shaves but not for e.g. Mary thinks John coming here was a mistake .	S-21
OTH	suggest various possibilities for packing the partial syntax trees , including using Tree Adjoining Grammaror Description Theory.	S-22
OTH	One further possibility is to choose a single syntax tree , and to use destructive tree operations later in the parse .	S-23
BAS	The approach which we will adopt here is based on,.	S-24
OWN	Partial syntax trees can be regarded as performing two main roles .	S-25
OWN	The first is to provide syntactic information which guides how the rest of the sentence can be integrated into the tree .	S-26
OWN	The second is to provide a basis for a semantic representation .	S-27
OWN	The first role can be captured using syntactic types , where each type corresponds to a potentially infinite number of partial syntax trees .	S-28
OWN	The second role can be captured by the parser constructing semantic representations directly .	S-29
OWN	The general processing model therefore consists of transitions of the form :	S-30
OWN	This provides a state-transition or dynamic model of processing , with each state being a pair of a syntactic type and a semantic value .	S-31
CTR	The main difference between our approach and that of,is that it is based on a more expressive grammar formalism , Applicative Categorial Grammar , as opposed to Lexicalised Dependency Grammar .	S-32
OTH	Applicative Categorial Grammars allow categories to have arguments which are themselves functions ( e.g. very can be treated as a function of a function , and given the type (n/n)/(n/n) when used as an adjectival modifier ) .	S-33
OTH	The ability to deal with functions of functions has advantages in enabling more elegant linguistic descriptions , and in providing one kind of robust parsing : the parser never fails until the last word , since there could always be a final word which is a function over all the constituents formed so far .	S-34
CTR	However , there is a corresponding problem of far greater non-determinism , with even unambiguous words allowing many possible transitions .	S-35
OWN	It therefore becomes crucial to either perform some kind of ambiguity packing , or language tuning .	S-36
TXT	This will be discussed in the final section of the paper .	S-37
OTH	Applicative Categorial Grammar is the most basic form of Categorial Grammar , with just a single combination rule corresponding to function application .	S-38
OTH	It was first applied to linguistic description byandin the 1950s .	S-39
OTH	Although it is still used for linguistic description, it has been somewhat overshadowed in recent years by HPSG, and by Lambek Categorial Grammars.	S-40
OTH	It is therefore worth giving some brief indications of how it fits in with these developments .	S-41
OTH	The first directed Applicative CG was proposed by.	S-42
OTH	Functional types included a list of arguments to the left , and a list of arguments to the right .	S-43
OTH	Translating's notation into a feature based notation similar to that in HPSG, we obtain the following category for a ditransitive verb such as put :	S-44
OTH	The list of arguments to the left are gathered under the feature , l , and those to the right , an np and a pp in that order , under the feature r .	S-45
OTH	employed a single application rule , which corresponds to the following :	S-46
OTH	The result was a system which comes very close to the formalised dependency grammars ofand.	S-47
OTH	The only real difference is thatallowed arguments to themselves be functions .	S-48
OTH	For example , an adverb such as slowly could be given the type	S-49
CTR	An unfortunate aspect of's first system was that the application rule only ever resulted in a primitive type .	S-50
CTR	Hence , arguments with functional types had to correspond to single lexical items : there was no way to form the type nps for a non-lexical verb phrase such as likes Mary .	S-51
OTH	Rather than adapting the Application Rule to allow functions to be applied to one argument at a time ,'s second system ( often called AB Categorial Grammar , or/CG ,) adopted a ` Curried ' notation , and this has been adopted by most CGs since .	S-52
OTH	To represent a function which requires an np on the left , and an np and a pp to the right , there is a choice of the following three types using Curried notation :	S-53
OTH	Most CGs either choose the third of these ( to give a vp structure ) , or include a rule of Associativity which means that the types are interchangeable ( in the Lambek Calculus , Associativity is a consequence of the calculus , rather than being specified separately ) .	S-54
OTH	The main impetus to change Applicative CG came from the work of.	S-55
OTH	noted that the use of function composition allows CGs to deal with unbounded dependency constructions .	S-56
OTH	Function composition enables a function to be applied to its argument , even if that argument is incomplete e.g.	S-57
OTH	This allows peripheral extraction , where the ` gap ' is at the start or the end of e.g. a relative clause .	S-58
OTH	Variants of the composition rule were proposed in order to deal with non-peripheral extraction , but this led to unwanted effects elsewhere in the grammar.	S-59
OTH	Subsequent treatments of non-peripheral extraction based on the Lambek Calculus ( where standard composition is built in : it is a rule which can be proven from the calculus ) have either introduced an alternative to the forward and backward slashes i.e. / andfor normal args ,for wh-args, or have introduced so called modal operators on the wh-argument.	S-60
OTH	Both techniques can be thought of as marking the wh-arguments as requiring special treatment , and therefore do not lead to unwanted effects elsewhere in the grammar .	S-61
CTR	However , there are problems with having just composition , the most basic of the non-applicative operations .	S-62
OTH	In CGs which contain functions of functions ( such as very , or slowly ) , the addition of composition adds both new analyses of sentences , and new strings to the language .	S-63
OTH	This is due to the fact that composition can be used to form a function , which can then be used as an argument to a function of a function .	S-64
OTH	For example , if the two types , n / n and n / n are composed to give the type n / n , then this can be modified by an adjectival modifier of type ( n / n ) / ( n / n ) .	S-65
CTR	Thus , the noun very old dilapidated car can get the unacceptable bracketing , [ [ very [ old dilapidated ] ] car ] .	S-66
OTH	Associative CGs with Composition , or the Lambek Calculus also allow strings such as boy with the to be given the type n / n predicting very boy with the car to be an acceptable noun .	S-67
OTH	Although individual examples might be possible to rule out using appropriate features , it is difficult to see how to do this in general whilst retaining a calculus suitable for incremental interpretation .	S-68
OWN	If wh-arguments need to be treated specially anyway ( to deal with non-peripheral extraction ) , and if composition as a general rule is problematic , this suggests we should perhaps return to grammars which use just Application as a general operation , but have a special treatment for wh-arguments .	S-69
OWN	Using the non-Curried notation of, it is more natural to use a separate wh-list than to mark wh-arguments individually .	S-70
OWN	For example , the category appropriate for relative clauses with a noun phrase gap would be :	S-71
OWN	It is then possible to specify operations which act as purely applicative operations with respect to the left and right arguments lists , but more like composition with respect to the wh-list .	S-72
OTH	This is very similar to the way in which wh-movement is dealt with in GPSGand HPSG , where wh-arguments are treated using slash mechanisms or feature inheritance principles which correspond closely to function composition .	S-73
CTR	Given that our arguments have produced a categorial grammar which looks very similar to HPSG , why not use HPSG rather than Applicative CG ?	S-74
CTR	The main reason is that Applicative CG is a much simpler formalism , which can be given a very simple syntax semantics interface , with function application in syntax mapping to function application in semantics.	S-75
OWN	This in turn makes it relatively easy to provide proofs of soundness and completeness for an incremental parsing algorithm .	S-76
OWN	Ultimately , some of the techniques developed here should be able to be extended to more complex formalisms such as HPSG .	S-77
BAS	In this section we define a grammar similar to's first grammar .	S-78
CTR	However , unlike, we allow one argument to be absorbed at a time .	S-79
OWN	The resulting grammar is equivalent to AB Categorial Grammar plus associativity .	S-80
OWN	The categories of the grammar are defined as follows :	S-81
OWN	If X is a syntactic type ( e.g. s , np ) , thenis a category .	S-82
OWN	If X is a syntactic type , and L and R are lists of categories , thenis a category .	S-83
OWN	Application to the right is defined by the rule :	S-84
OWN	Application to the left is defined by the rule :	S-85
OWN	The basic grammar provides some spurious derivations , since sentences such as John likes Mary can be bracketed as either ( ( John likes ) Mary ) or ( John ( likes Mary ) ) .	S-86
OWN	However , we will see that these spurious derivations do not translate into spurious ambiguity in the parser , which maps from strings of words directly to semantic representations .	S-87
OWN	Most parsers which work left to right along an input string can be described in terms of state transitions i.e. by rules which say how the current parsing state ( e.g. a stack of categories , or a chart ) can be transformed by the next word into a new state .	S-88
OWN	Here this will be made particularly explicit , with the parser described in terms of just two rules which take a state , a new word and create a new state .	S-89
OWN	There are two unusual features .	S-90
OWN	Firstly , there is nothing equivalent to a stack mechanism : at all times the state is characterised by a single syntactic type , and a single semantic value , not by some stack of semantic values or syntax trees which are waiting to be connected together .	S-91
OWN	Secondly , all transitions between states occur on the input of a new word : there are no ` empty ' transitions ( such as the reduce step of a shift-reduce parser ) .	S-92
OWN	The two rules , which are given in Figure, are difficult to understand in their most general form .	S-93
OWN	Here we will work upto the rules gradually , by considering which kinds of rules we might need in particular instances .	S-94
OWN	Consider the following pairing of sentence fragments with their simplest possible CG type :	S-95
OWN	Now consider taking each type as a description of the state that the parser is in after absorbing the fragment .	S-96
OWN	We obtain a sequence of transitions as follows :	S-97
OWN	If an embedded sentence such as John likes Sue is a mapping from an s / s to an s , this suggests that it might be possible to treat all sentences as mapping from some category expecting an s to that category i.e. from X / s to X .	S-98
OWN	Similarly , all noun phrases might be treated as mappings from an X / np to an X .	S-99
OWN	Now consider individual transitions .	S-100
OWN	The simplest of these is where the type of argument expected by the state is matched by the next word i.e.	S-101
OWN	This can be generalised to the following rule , which is similar to Function Application in standard CG	S-102
OWN	A similar transition occurs for likes .	S-103
OWN	Here an nps was expected , but likes only provides part of this : it requires an np to the right to form an nps .	S-104
OWN	Thus after likes is absorbed the state category will need to expect an np .	S-105
OWN	The rule required is similar to Function Composition in CG i.e.	S-106
OWN	Considering this informally in terms of tree structures , what is happening is the replacement of an empty node in a partial tree by a second partial tree i.e.	S-107
OWN	The two rules specified so far need to be further generalised to allow for the case where a lexical item has more than one argument ( e.g. if we replace likes by a di-transitive such as gives or a tri-transitive such as bets ) .	S-108
OWN	This is relatively trivial using a non-curried notation similar to that used for AACG .	S-109
OWN	What we obtain is the single rule of State-Application , which corresponds to application when the list of arguments , R, is empty , to function composition when Ris of length one , and to n-ary composition when Ris of length n .	S-110
OWN	The only change needed from AACG notation is the inclusion of an extra feature list , the h list , which stores information about which arguments are waiting for a head ( the reasons for this will be explained later ) .	S-111
OWN	The lexicon is identical to that for a standard AACG , except for having h-lists which are always set to empty .	S-112
OWN	Now consider the first transition .	S-113
OWN	Here a sentence was expected , but what was encountered was a noun phrase , John .	S-114
OWN	The appropriate rule in CG notation would be :	S-115
OWN	This rule states that if looking for a Y and get a Z then look for a Y which is missing a Z .	S-116
OWN	In tree structure terms we have :	S-117
OWN	The rule of State-Prediction is obtained by further generalising to allow the lexical item to have missing arguments , and for the expected argument to have missing arguments .	S-118
OWN	State-Application and State-Prediction together provide the basis of a sound and complete parser .	S-119
OWN	Parsing of sentences is achieved by starting in a state expecting a sentence , and applying the rules non-deterministically as each word is input .	S-120
OWN	A successful parse is achieved if the final state expects no more arguments .	S-121
OWN	As an example , reconsider the string John likes Sue .	S-122
OWN	The sequence of transitions corresponding to John likes Sue being a sentence , is given in Figure.	S-123
OWN	The transition on encountering John is deterministic : State-Application cannot apply , and State-Prediction can only be instantiated one way .	S-124
OWN	The result is a new state expecting an argument which , given an np could give an s i.e. an nps .	S-125
OWN	The transition on input of likes is non-deterministic .	S-126
OWN	State-Application can apply , as in Figure.	S-127
OWN	However , State-Prediction can also apply , and can be instantiated in four ways ( these correspond to different ways of cutting up the left and right subcategorisation lists of the lexical entry , likes , i.e. asnpornp) .	S-128
OWN	One possibility corresponds to the prediction of an ss modifier , a second to the prediction of an ( nps )( nps ) modifier ( i.e. a verb phrase modifier ) , a third to there being a function which takes the subject and the verb as separate arguments , and the fourth corresponds to there being a function which requires an s / np argument .	S-129
OWN	The second of these is perhaps the most interesting , and is given in Figure.	S-130
OWN	It is the choice of this particular transition at this point which allows verb phrase modification , and hence , assuming the next word is Sue , an implicit bracketing of the string fragment as ( John ( likes Sue ) ) .	S-131
OWN	Note that if State-Application is chosen , or the first of the State-Prediction possibilities , the fragment John likes Sue retains a flat structure .	S-132
OWN	If there is to be no modification of the verb phrase , no verb phrase structure is introduced .	S-133
OWN	This relates to there being no spurious ambiguity : each choice of transition has semantic consequences ; each choice affects whether a particular part of the semantics is to be modified or not .	S-134
OWN	Finally , it is worth noting why it is necessary to use h-lists .	S-135
OWN	These are needed to distinguish between cases of real functional arguments ( of functions of functions ) , and functions formed by State-Prediction .	S-136
OWN	Consider the following trees , where the nps node is empty .	S-137
OWN	Both trees have the same syntactic type , however in the first case we want to allow for there to be an ss modifier of the lower s , but not in the second .	S-138
OWN	The headed list distinguishes between the two cases , with only the first having an np on its headed list , allowing prediction of an s modifier .	S-139
OWN	When we consider full sentence processing , as opposed to incremental processing , the use of lexicalised grammars has a major advantage over the use of more standard rule based grammars .	S-140
OWN	In processing a sentence using a lexicalised formalism we do not have to look at the grammar as a whole , but only at the grammatical information indexed by each of the words .	S-141
OWN	Thus increases in the size of a grammar don't necessarily effect efficiency of processing , provided the increase in size is due to the addition of new words , rather than increased lexical ambiguity .	S-142
OWN	Once the full set of possible lexical entries for a sentence is collected , they can , if required , then be converted back into a set of phrase structure rules ( which should correspond to a small subset of the rule based formalism equivalent to the whole lexicalised grammar ) , before being parsing with a standard algorithm such as's .	S-143
OWN	In incremental parsing we cannot predict which words will appear in the sentence , so cannot use the same technique .	S-144
OWN	However , if we are to base a parser on the rules given above , it would seem that we gain further .	S-145
OWN	Instead of grammatical information being localised to the sentence as a whole , it is localised to a particular word in its particular context : there is no need to consider a pp as a start of a sentence if it occurs at the end , even if there is a verb with an entry which allows for a subject pp .	S-146
OWN	However there is a major problem .	S-147
OWN	As we noted in the last paragraph , it is the nature of parsing incrementally that we don't know what words are to come next .	S-148
OWN	But here the parser doesn't even use the information that the words are to come from a lexicon for a particular language .	S-149
OWN	For example , given an input of 3 nps , the parser will happily create a state expecting 3 nps to the left .	S-150
OWN	This might be a likely state for say a head final language , but an unlikely state for a language such as English .	S-151
OWN	Note that incremental interpretation will be of no use here , since the semantic representation should be no more or less plausible in the different languages .	S-152
OWN	In practical terms , a naive interactive parallel Prolog implementation on a current workstation fails to be interactive in a real sense after about 8 words .	S-153
OWN	What seems to be needed is some kind of language tuning .	S-154
OWN	This could be in the nature of fixed restrictions to the rules e.g. for English we might rule out uses of prediction when a noun phrase is encountered , and two already exist on the left list .	S-155
OWN	A more appealing alternative is to base the tuning on statistical methods .	S-156
OWN	This could be achieved by running the parser over corpora to provide probabilities of particular transitions given particular words .	S-157
OWN	These transitions would capture the likelihood of a word having a particular part of speech , and the probability of a particular transition being performed with that part of speech .	S-158
OWN	There has already been some early work done on providing statistically based parsing using transitions between recursively structured syntactic categories.	S-159
OWN	Unlike a simple Markov process , there are a potentially infinite number of states , so there is inevitably a problem of sparse data .	S-160
OWN	It is therefore necessary to make various generalisations over the states , for example by ignoring the Rlists .	S-161
OWN	The full processing model can then be either serial , exploring the most highly ranked transitions first ( but allowing backtracking if the semantic plausibility of the current interpretation drops too low ) , or ranked parallel , exploring just the n paths ranked highest according to the transition probabilities and semantic plausibility .	S-162
AIM	The paper has presented a method for providing interpretations word by word for basic Categorial Grammar .	S-163
OWN	The final section contrasted parsing with lexicalised and rule based grammars , and argued that statistical language tuning is particularly suitable for incremental , lexicalised parsing strategies .	S-164
