AIM	We describe and experimentally evaluate a method for automatically clustering words according to their distribution in particular syntactic contexts .	A-0
OWN	Deterministic annealing is used to find lowest distortion sets of clusters .	A-1
OWN	As the annealing parameter increases , existing clusters become unstable and subdivide , yielding a hierarchical `` soft '' clustering of the data .	A-2
OWN	Clusters are used as the basis for class models of word coocurrence , and the models evaluated with respect to held-out test data .	A-3
BKG	Methods for automatically classifying words according to their contexts of use have both scientific and practical interest .	S-0
BKG	The scientific questions arise in connection to distributional views of linguistic ( particularly lexical ) structure and also in relation to the question of lexical acquisition both from psychological and computational learning perspectives .	S-1
BKG	From the practical point of view , word classification addresses questions of data sparseness and generalization in statistical language models , particularly models for deciding among alternative analyses proposed by a grammar .	S-2
BKG	It is well known that a simple tabulation of frequencies of certain words participating in certain configurations , for example of frequencies of pairs of a transitive main verb and the head noun of its direct object , cannot be reliably used for comparing the likelihoods of different alternative configurations .	S-3
BKG	The problem is that for large enough corpora the number of possible joint events is much larger than the number of event occurrences in the corpus , so many events are seen rarely or never , making their frequency counts unreliable estimates of their probabilities .	S-4
OTH	proposed dealing with the sparseness problem by estimating the likelihood of unseen events from that of `` similar '' events that have been seen .	S-5
OTH	For instance , one may estimate the likelihood of a particular direct object for a verb from the likelihoods of that direct object for similar verbs .	S-6
OTH	This requires a reasonable definition of verb similarity and a similarity estimation method .	S-7
OTH	In's proposal , words are similar if we have strong statistical evidence that they tend to participate in the same events .	S-8
CTR	His notion of similarity seems to agree with our intuitions in many cases , but it is not clear how it can be used directly to construct word classes and corresponding models of association .	S-9
AIM	Our research addresses some of the same questions and uses similar raw data , but we investigate how to factor word association tendencies into associations of words to certain hidden senses classes and associations between the classes themselves .	S-10
AIM	While it may be worthwhile to base such a model on preexisting sense classes, in the work described here we look at how to derive the classes directly from distributional data .	S-11
OWN	More specifically , we model senses as probabilistic concepts or clusters c with corresponding cluster membership probabilitiesfor each word w .	S-12
CTR	Most other class-based modeling techniques for natural language rely instead on `` hard '' Boolean classes.	S-13
CTR	Class construction is then combinatorially very demanding and depends on frequency counts for joint events involving particular words , a potentially unreliable source of information as we noted above .	S-14
OWN	Our approach avoids both problems .	S-15
OWN	In what follows , we will consider two major word classes ,and, for the verbs and nouns in our experiments , and a single relation between them , in our experiments relation between a transitive main verb and the head noun of its direct object .	S-16
OWN	Our raw knowledge about the relation consists of the frequenciesof occurrence of particular pairs (v,n) in the required configuration in a training corpus .	S-17
OWN	Some form of text analysis is required to collect such a collection of pairs .	S-18
BAS	The corpus used in our first experiment was derived from newswire text automatically parsed by's parser Fidditch.	S-19
BAS	More recently , we have constructed similar tables with the help of a statistical part-of-speech taggerand of tools for regular expression pattern matching on tagged corpora.	S-20
OWN	We have not yet compared the accuracy and coverage of the two methods , or what systematic biases they might introduce , although we took care to filter out certain systematic errors , for instance the misparsing of the subject of a complement clause as the direct object of a main verb for report verbs like `` say '' .	S-21
AIM	We will consider here only the problem of classifying nouns according to their distribution as direct objects of verbs ; the converse problem is formally similar .	S-22
OWN	More generally , the theoretical basis for our method supports the use of clustering to build models for any n-ary relation in terms of associations between elements in each coordinate and appropriate hidden units ( cluster centroids ) and associations between those hidden units .	S-23
OWN	For the noun classification problem , the empirical distribution of a noun n is then given by the conditional density.	S-24
OWN	The problem we study is how to use theto classify the.	S-25
OWN	Our classification method will construct a setof clusters and cluster membership probabilities.	S-26
OWN	Each cluster c is associated to a cluster centroid, which is discrete density overobtained by averaging appropriately the.	S-27
OWN	To cluster nouns n according to their conditional verb distributions, we need a measure of similarity between distributions .	S-28
BAS	We use for this purpose the relative entropy or Kullback-Leibler ( KL ) distance between two distributions .	S-29
OWN	This is a natural choice for a variety of reasons , which we will just sketch here .	S-30
OWN	First of all ,is zero just in case p = q , and it increases as the probability decreases that p is the relative frequency distribution of a random sample drawn according to p .	S-31
OWN	More formally , the probability mass given by q to the set of all samples of length n with relative frequency distribution p is bounded by.	S-32
OWN	Therefore , if we are trying to distinguish among hypotheseswhen p is the relative frequency distribution of observations ,gives the relative weight of evidence in favor of.	S-33
OWN	Furthermore , a similar relation holds betweenfor two empirical distributions p and p ' and the probability that p and p ' are drawn from the same distribution q .	S-34
OWN	We can thus use the relative entropy between the context distributions for two words to measure how likely they are to be instances of the same cluster centroid .	S-35
OWN	From an information theoretic perspectivemeasures how inefficient on average it would be to use a code based on q to encode a variable distributed according to p .	S-36
OWN	With respect to our problem ,thus gives us the loss of information in using cluster centroidinstead of the actual distribution for wordwhen modeling the distributional properties of n .	S-37
OWN	Finally , relative entropy is a natural measure of similarity between distributions for clustering because its minimization leads to cluster centroids that are a simple weighted average of member distributions .	S-38
OWN	One technical difficulty is thatis not defined when p'(x) = 0 but.	S-39
OWN	We could sidestep this problem ( as we did initially ) by smoothing zero frequencies appropriately.	S-40
CTR	However , this is not very satisfactory because one of the goals of our work is precisely to avoid the problems of data sparseness by grouping words into classes .	S-41
CTR	It turns out that the problem is avoided by our clustering technique , since it does not need to compute the KL distance between individual word distributions , but only between a word distribution and average distributions , the current cluster centroids , which are guaranteed to be nonzero whenever the word distributions are .	S-42
CTR	This is a useful advantage of our method compared with agglomerative clustering techniques that need to compare individual objects being considered for grouping .	S-43
AIM	In general , we are interested on how to organize a set of linguistic objects such as words according to the contexts in which they occur , for instance grammatical constructions or n-grams .	S-44
AIM	We will show elsewhere that the theoretical analysis outlined here applies to that more general problem , but for now we will only address the more specific problem in which the objects are nouns and the contexts are verbs that take the nouns as direct objects .	S-45
AIM	Our problem can be seen as that of learning a joint distribution of pairs from a large sample of pairs .	S-46
OWN	The pair coordinates come from two large setsand, with no preexisting topological or metric structure , and the training data is a sequence S of N independently drawn pairs .	S-47
OWN	From a learning perspective , this problem falls somewhere in between unsupervised and supervised learning .	S-48
OWN	As in unsupervised learning , the goal is to learn the underlying distribution of the data .	S-49
OWN	But in contrast to most unsupervised learning settings , the objects involved have no internal structure or attributes allowing them to be compared with each other .	S-50
OWN	Instead , the only information about the objects is the statistics of their joint appearance .	S-51
OWN	These statistics can thus be seem as a weak form of object labelling analogous to supervision .	S-52
OWN	While clusters based on distributional similarity are interesting on their own , they can also be profitably seen as a means of summarizing a joint distribution .	S-53
OWN	In particular , we would like to find a set of clusterssuch that each conditional distributioncan be approximately decomposed as	S-54
OWN	whereis the membership probability of n in c andis v 's conditional probability given by the centroid distribution for cluster c .	S-55
OWN	The above decomposition can be written in a more symmetric form as	S-56
OWN	assuming thatandcoincide .	S-57
OWN	We will takeas our basic clustering model .	S-58
OWN	To determine this decomposition we need to solve the two connected problems of finding find suitable forms for the cluster membership and centroid distributions, and of maximizing the goodness of fit between the model distributionand the observed data .	S-59
OWN	Goodness of fit is determined by the model 's likelihood of the observations .	S-60
OWN	The maximum likelihood ( ML ) estimation principle is thus the natural tool to determine the centroid distributions.	S-61
OWN	As for the membership probabilities , they must be determined solely by the relevant measure of object-to-cluster similarity , which in the present work is the relative entropy between object and cluster centroid distributions .	S-62
OWN	Since no other information is available , the membership is determined by maximizing the configuration entropy subject for a fixed average distortion .	S-63
OWN	With the maximum entropy ( ME ) membership distribution , ML estimation is equivalent to the minimization of the average distortion of the data .	S-64
BAS	The combined entropy maximization entropy and distortion minimization is carried out by a two-stage iterative process similar to the EM method.	S-65
OWN	The first stage of an iteration is a maximum likelihood , or minimum distortion , estimation of the cluster centroids given fixed membership probabilities .	S-66
OWN	In the second iteration stage , the entropy of the membership distribution is maximized with a fixed average distortion .	S-67
OWN	This joint optimization searches for a saddle point in the distortion-entropy parameters , which is equivalent to minimizing a linear combination of the two known as free energy in statistical mechanics .	S-68
OWN	This analogy with statistical mechanics is not coincidental , and provide us with a better understanding of the clustering procedure .	S-69
OWN	For the maximum likelihood argument , we start by estimating the likelihood of the sequence S of N independent observations of pairs.	S-70
OWN	Using, the sequence 's model log likelihood is	S-71
OWN	Fixing the number of clusters ( model size ), we want to maximizewith respect to the distributionsand.	S-72
OWN	The variation ofwith respect to these distributions is	S-73
OWN	withandkept normalized .	S-74
OWN	Using Bayes 's formula , we have	S-75
OWN	or	S-76
OWN	for any c , which we substitute intoto obtain	S-77
OWN	since.	S-78
OWN	This expression is particularly useful when the cluster distributionsandare of exponential form , precisely what will be provided by the ME step described below .	S-79
OWN	At this point we need to specify the clustering model in more detail .	S-80
OWN	In the derivation so far we have treatedandsymmetrically , corresponding to clusters not of verbs or nouns but of verb-noun associations .	S-81
OWN	In principle such a symmetric model may be more accurate , but in this paper we will concentrate on asymmetric models in which cluster memberships are associated to just one of the components of the joint distribution and the cluster centroids are specified only by the other component .	S-82
OWN	In particular , the model we use in our experiments has noun clusters with cluster memberships determined byand centroid distributions determined by.	S-83
OWN	The asymmetric model simplifies the estimation significantly by dealing with a single component , but it has the disadvantage that the joint distribution ,has two different and not necessarily consistent expressions in terms of asymmetric models for the two coordinates .	S-84
OWN	While variations ofandin equationare not independent , we can treat them separately .	S-85
OWN	First , for fixed average distortion between the cluster centroid distributionsand the data, we find the cluster membership probabilities , which are the Bayes 's inverses of the, that maximize the entropy of the cluster distributions .	S-86
OWN	With the membership distributions thus obtained , we then look for thethat maximize the log likelihood l ( S ) .	S-87
OWN	It turns out that this will also be the values ofthat minimize the average distortion between the asymmetric cluster model and the data .	S-88
OWN	Given any similarity measurebetween nouns and cluster centroids , the average cluster distortion is	S-89
OWN	If we maximize the cluster membership entropy	S-90
OWN	subject to normalization ofand fixed, we obtain the following standard exponential forms for the class and membership distributions	S-91
OWN	where the normalization sums ( partition functions ) areand.	S-92
OWN	Notice thatdoes not need to be symmetric for this derivation , as the two distributions are simply related by Bayes 's rule .	S-93
OWN	Returning to the log-likelihood variation, we can now useforand the assumption for the asymmetric model that the cluster membership stays fixed as we adjust the centroids , to obtain	S-94
OWN	where the variation of [EQn] is now included in the variation of.	S-95
OWN	For a large enough sample , we may replace the sum over observations inby the average over.	S-96
OWN	which , applying Bayes 's rule , becomes	S-97
OWN	At the log-likelihood maximum , the variationmust vanish .	S-98
OWN	We will see below that the use of relative entropy for similarity measure makesvanish at the maximum as well , so the log likelihood can be maximized by minimizing the average distortion with respect to the class centroids while class membership is kept fixed	S-99
OWN	or , sufficiently , if each of the inner sums vanish	S-100
OWN	We first show that the minimization of the relative entropy yields the natural expression for cluster centroids	S-101
OWN	To minimize the average distortion, we observe that the variation of the KL distance between noun and centroid distributions with respect to the centroid distribution, with each centroid distribution normalized by the Lagrange multiplier, is given by	S-102
OWN	Substituting this expression into, we obtain	S-103
OWN	Since theare now independent , we obtain immediately the desired centroid expression, which is the desired weighted average of noun distributions .	S-104
OWN	We can now see that the variationvanishes for centroid distributions given by, since it follows fromthat	S-105
OWN	The combined minimum distortion and maximum entropy optimization is equivalent to the minimization of a single function , the free energy	S-106
OWN	whereis the average distortionand H is the cluster membership entropy.	S-107
OWN	The free energy determines both the distortion and the membership entropy through	S-108
OWN	with temperature.	S-109
OWN	The most important property of the free energy is that its minimum determines the balance between the `` disordering '' maximum entropy and `` ordering '' distortion minimization in which the system is most likely to be found .	S-110
OWN	In fact the probability to find the system at a given configuration is exponential in F	S-111
OWN	so a system is most likely to be found in its minimal free energy configuration .	S-112
BAS	The analogy with statistical mechanics suggests a deterministic annealing procedure for clustering, in which the number of clusters is determined through a sequence of phase transitions by continuously increasing the parameterfollowing an annealing schedule .	S-113
OTH	The higher, the more local is the influence of each noun on the definition of centroids .	S-114
OTH	The dissimilarity plays here the role of distortion .	S-115
OTH	When the scale parameteris close to zero , the dissimilarities are almost irrelevant , all words contribute about equally to each centroid , and so the lowest average distortion solution involves just one cluster which is the average of all word densities .	S-116
OTH	Asis slowly increased , a point ( phase transition ) is eventually reached which the natural solution involves two distinct centroids .	S-117
OTH	We say then that the original cluster has split into the two new clusters .	S-118
OTH	In general , if we take any cluster c and a twin c ' of c such that the centroidis a small random pertubation of, below the criticalat which c splits the membership and centroid reestimation procedure given by equationsandwill makeandconverge , that is , c and c ' are really the same cluster .	S-119
OTH	But withabove the critical value for c , the two centroids will diverge , giving rise to two daughters of c .	S-120
OWN	Our clustering procedure is thus as follows .	S-121
OWN	We start with very lowand a single cluster whose centroid is the average of all noun distributions .	S-122
OWN	For any given, we have a current set of leaf clusters corresponding to the current free energy ( local ) minimum .	S-123
OWN	To refine such a solution , we search for the lowestwhich is the critical value for some current leaf cluster splits .	S-124
OWN	Ideally , there is just one split at that critical value , but for practical performance and numerical accuracy reasons we may have several splits at the new critical point .	S-125
OWN	The splitting procedure can then be repeated to achieve the desired number of clusters or model cross-entropy .	S-126
OWN	All our experiments involve the asymmetric model described in the previous section .	S-127
OWN	As explained there , our clustering procedure yields for each value ofa setof clusters minimizing the free energy F , and the asymmetric model forestimates the conditional verb distribution for a noun n by	S-128
OWN	wherealso depends on.	S-129
OWN	As a first experiment , we used our method to classify the 64 nouns appearing most frequently as heads of direct objects of the verb `` fire '' in one year ( 1988 ) of Associated Press newswire .	S-130
OWN	In this corpus , the chosen nouns appear as direct object heads of a total of 2147 distinct verbs , so each noun is represented by a density over the 2147 verbs .	S-131
OWN	Figureshows the five words most similar to the each cluster centroid for the four clusters resulting from the first two cluster splits .	S-132
OWN	It can be seen that first split separates the objects corresponding to the weaponry sense of `` fire '' ( cluster 1 ) from the ones corresponding to the personnel action ( cluster 2 ) .	S-133
OWN	The second split then further refines the weaponry sense into a projectile sense ( cluster 3 ) and a gun sense ( cluster 4 ) .	S-134
OWN	That split is somewhat less sharp , possibly because not enough distinguishing contexts occur in the corpus .	S-135
OWN	Figureshows the four closest nouns to the centroid of each of a set of hierarchical clusters derived from verb-object pairs involving the 1000 most frequent nouns in the June 1991 electronic version of Grolier 's Encyclopedia ( 10 million words ) .	S-136
OWN	The preceding qualitative discussion provides some indication of what aspects of distributional relationships may be discovered by clustering .	S-137
OWN	However , we also need to evaluate clustering more rigorously as a basis for models of distributional relationships .	S-138
OWN	So , far , we have looked at two kinds of measurements of model quality :	S-139
OWN	relative entropy between held-out data and the asymmetric model , and	S-140
OWN	performance on the task of deciding which of two verbs is more likely to take a given noun as direct object when the data relating one of the verbs to the noun has been witheld from the training data .	S-141
OWN	The evaluation described below was performed on the largest data set we have worked with so far , extracted from 44 million words of 1988 Associated Press newswire with the pattern matching techniques mentioned earlier .	S-142
OWN	This collection process yielded 1112041 verb-object pairs .	S-143
OWN	We selected then the subset involving the 1000 most frequent nouns in the corpus for clustering , and randomly divided it into a training set of 756721 pairs and a test set of 81240 pairs .	S-144
OWN	Figureplots the average relative entropy of several data sets to asymmetric clustered models of different sizes , given by	S-145
OWN	whereis the relative frequency distribution of verbs taking n as direct object in the test set .	S-146
OWN	For each critical value of, we show the relative entropy with respect to the asymmetric model based onof the training set ( set train ) , of randomly selected held-out test set ( set test ) , and of held-out data for a further 1000 nouns that were not clustered ( set new ) .	S-147
OWN	Unsurprisingly , the training set relative entropy decreases monotonically .	S-148
OWN	The test set relative entropy decreases to a minimum at 206 clusters , and then starts increasing , suggesting that larger models are overtrained .	S-149
OWN	The new noun test set is intended to test whether clusters based on the 1000 most frequent nouns are useful classifiers for the selectional properties of nouns in general .	S-150
OWN	As the figure shows , the cluster model provides over one bit of information about the selectional properties of the new nouns , but the overtraining effect is even sharper than for the held-out data involving the 1000 clustered nouns .	S-151
OWN	We also evaluated asymmetric cluster models on a verb decision task closer to possible applications to disambiguation in language analysis .	S-152
OWN	The task consists judging which of two verbs v and v ' is more likely to take a given noun n as object , when all occurrences of ( v , n ) in the training set were deliberately deleted .	S-153
OWN	Thus this test evaluates how well the models reconstruct missing data in the verb distribution for n from the cluster centroids close to n .	S-154
BAS	The data for this test was built from the training data for the previous one in the following way , based on a suggestion by.	S-155
OWN	A small number ( 104 ) of ( v , n ) pairs with a fairly frequent verb ( between 500 and 5000 occurrences ) was randomly picked , and all occurrences of each pair in the training set were deleted .	S-156
OWN	The resulting training set was used to build a sequence of cluster models as before .	S-157
OWN	Each model was used to decide which of two verbs v and v ' are more likely to appear with a noun n where the ( v , n ) data was deleted from the training set , and the decisions compared with the corresponding ones derived from the original event frequencies in the initial data set .	S-158
OWN	More specifically , for each deleted pair ( v , n ) and each verb v ' that occurred with n in the initial data either at least twice as frequently or at most half as frequently as v , we compared the sign ofwith that offor the initial data set .	S-159
OWN	The error rate for each model is simply the proportion of sign disagreements in the selected ( v , n , v ' ) triples .	S-160
OWN	Figureshows the error rates for each model for all the selected ( v , n , v ' ) ( all ) and for just those exceptional triples in which the log frequency ratio of ( n , v ) and ( n , v ' ) differs from the log marginal frequency ratio of v and v ' .	S-161
OWN	In other words , the exceptional cases are those in which predictions based just on the marginal frequencies , which the initial one-cluster model represents , would be consistently wrong .	S-162
OWN	Here too we see some overtraining for the largest models considered , although not for the exceptional verbs .	S-163
AIM	We have demonstrated that a general divisive clustering procedure for probability distributions can be used to group words according to their participation in particular grammatical relations with other words .	S-164
OWN	The resulting clusters are intuitively informative , and can be used to construct class-based word coocurrence models with substantial predictive power .	S-165
OWN	While the clusters derived by the proposed method seem in many cases semantically significant , this intuition needs to be grounded in a more rigorous assessment .	S-166
OWN	In addition to predictive power evaluations of the kind we have already carried out , it might be worth comparing automatically-derived clusters with human judgements in a suitable experimental setting .	S-167
OWN	Moving further in the direction of class-based language models , we plan to consider additional distributional relations ( for instance , adjective-noun ) and apply the results of clustering to the grouping of lexical associations in lexicalized grammar frameworks such as stochastic lexicalized tree-adjoining grammars.	S-168
OWN	We would like to thank Don Hindle for making available the 1988 Associated Press verb-object data set , the Fidditch parser and a verb-object structure filter , Mats Rooth for selecting the objects of `` fire '' data set and many discussions , David Yarowsky for help with his stemming and concordancing tools , and Ido Dagan for suggesting ways of testing cluster models .	S-169
