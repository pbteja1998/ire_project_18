OWN	The pattern matching capabilities of neural networks can be used to locate syntactic constituents of natural language .	A-0
AIM	This paper describes a fully automated hybrid system , using neural nets operating within a grammatic framework .	A-1
OWN	It addresses the representation of language for connectionist processing , and describes methods of constraining the problem size .	A-2
OWN	The function of the network is briefly explained , and results are given .	A-3
AIM	The pattern matching capabilities of neural networks can be used to detect syntactic constituents of natural language .	S-0
CTR	This approach bears comparison with probabilistic systems , but has the advantage that negative as well as positive information can be modelled .	S-1
OWN	Also , most computation is done in advance , when the nets are trained , so the run time computational load is low .	S-2
AIM	In this work neural networks are used as part of a fully automated system that finds a partial parse of declarative sentences .	S-3
OWN	The connectionist processors operate within a grammatic framework , and are supported by pre-processors that filter the data and reduce the problem to a computationally tractable size .	S-4
OWN	A prototype can be accessed via the Internet , on which users can try their own text ( details from the authors ) .	S-5
OWN	It will take a sentence , locate the subject and then find the head of the subject .	S-6
OWN	Typically 10 sentences take about 2 seconds , 50 sentences about 4 seconds , to process on a Sparc10 workstation .	S-7
OWN	Using the prototype on technical manuals the subject and its head can be detected in over 90 % of cases ( See Section) .	S-8
OWN	The well known complexity of parsing is addressed by decomposing the problem , and then locating one syntactic constituent at a time .	S-9
OWN	The sentence is first decomposed into the broad syntactic categories	S-10
OWN	pre-subject - subject - predicate	S-11
OWN	by locating the subject	S-12
OWN	Then these constituents can be processed further .	S-13
OWN	The underlying principle employed at each step is to take a sentence , or part of a sentence , and generate strings with the boundary markers of the syntactic constituent in question placed in all possible positions .	S-14
OWN	Then a neural net selects the string with the correct placement .	S-15
OWN	This paper gives an overview of how natural language is converted to a representation that the neural nets can handle , and how the problem is reduced to a manageable size .	S-16
OWN	It then outlines the neural net selection process .	S-17
BAS	A comprehensive account is given in; descriptions of the neural net process are also inand.	S-18
OWN	This is a hybrid system .	S-19
OWN	The core process is data driven , as the parameters of the neural networks are derived from training text .	S-20
OWN	The neural net is trained in supervised mode on examples that have been manually marked `` correct '' and `` incorrect '' .	S-21
OWN	It will then be able to classify unseen examples .	S-22
OWN	However , the initial processing stages , in which the problem size is constrained , operate within a skeletal grammatic framework .	S-23
OWN	Computational tractability is further addressed by reducing data through the application of prohibitive rules as local constraints .	S-24
OWN	The pruning process is remarkably effective .	S-25
OWN	This work has principally been developed on text of technical manuals from Perkins Engines Ltd. , which have been translated by a semi-automatic process.	S-26
OWN	Now , a partial parse can support such a process .	S-27
OWN	For instance , frequently occurring modal verbs such as `` must '' are not distinguished by number in English , but they are in many other languages .	S-28
OWN	It is necessary to locate the subject , then identify the head and determine its number in order to translate the main verb correctly in sentences likebelow .	S-29
OWN	This parser has been trained to find the syntactic subject head that agrees in number with the main verb .	S-30
OWN	The manuals are written using the PACE ( Perkins Approved Clear English ) guidelines , with the aim of producing clear , unambiguous texts .	S-31
OWN	All declarative sentences have been extracted for processing : about half were imperatives .	S-32
OWN	This level of classification can be done automatically in future .	S-33
OWN	Tableand Figureshow some of the characteristics of the corpus .	S-34
OWN	Punctuation marks are counted as words , formulae as 1 word .	S-35
OWN	In order to reconcile computational feasibility to empirical realism an appropriate form of language representation is critical .	S-36
OWN	The first step in constraining the problem size is to partition an unlimited vocabulary into a restricted number of part-of-speech tags .	S-37
OWN	Different stages of processing place different requirements on the classification system , so customised tagsets have been developed .	S-38
OWN	For the first processing stage we need to place the subject markers , and , as a further task , disambiguate tags .	S-39
OWN	It was not found necessary to use number information at this stage .	S-40
OWN	For example , consider the sentence :	S-41
OWN	The word `` waters '' could be a 3rd person , singular , present verb or a plural noun .	S-42
OWN	However , in order to disambiguate the tag and place the subject markers it is only necessary to know that it is a noun or else a verb .	S-43
OWN	The sentence parsed at the first level returns :	S-44
OWN	The tagset used at this stage , mode 1 , has 21 classes , not distinguished for number .	S-45
OWN	However , the head of the subject is then found and number agreement with the verb can be assessed .	S-46
OWN	At this stage the tagset , mode 2 , includes number information and has 28 classes .	S-47
OWN	Devising optimal tagsets for given tasks is a field in which further work is planned .	S-48
OWN	We need larger tagsets to capture more linguistic information , but smaller ones to constrain the computational load .	S-49
OWN	Information theoretic tools can be used to find the entropy of different tag sequence languages , and support decisions on representation .	S-50
OWN	A functional approach is taken to tagging : words are allocated to classes depending on their syntactic role .	S-51
OWN	For instance , superlative adjectives can act as nouns , so they are initially given the 2 tags : noun or adjective .	S-52
OWN	This approach can be extended by taking adjacent words which act jointly as single lexical items as a unit .	S-53
OWN	Thus the pair `` most  <  adjective  >  '' is taken as a single superlative adjective .	S-54
OWN	Text is automatically tagged using the first modules of the CLAWS program ( 1985 version ) , in which words are allocated one or more tags from 134 classes.	S-55
OWN	These 134 tags are then mapped onto the small customised tagsets .	S-56
OWN	Tag disambiguation is part of the parsing task , handled by the neural net and its pre-processor .	S-57
OWN	This version of CLAWS has a dictionary of about 6,300 words only .	S-58
OWN	Other words are tagged using suffix information , or else defaults are invoked .	S-59
OWN	The correct tag is almost always included in the set allocated , but more tags than necessary are often proposed .	S-60
OWN	A larger dictionary in later versions will address this problem .	S-61
BKG	In the same way that tags are allocated to words , or to punctuation marks , they can represent the boundaries of syntactic constituents , such as noun phrases and verb phrases .	S-62
OTH	Boundary markers can be considered invisible tags , or hypertags , which have probabilistic relationships with adjacent tags in the same way that words do .	S-63
OTH	andhave used this approach .	S-64
CTR	If embedded syntactic constituents are sought in a single pass , this can lead to computation al overload.	S-65
OWN	Our approach uses a similar concept , but differs in that embedded syntactic constituents are detected one at a time in separate steps .	S-66
OWN	There are only 2 hypertags - the opening and closing brackets marking the possible location ( s ) of the syntactic constituent in question .	S-67
OWN	Using this representation a hierarchical language structure is converted to a string of tags represented by a linear vector .	S-68
OWN	This system generates sets of tag strings for each sentence , with the hypertags placed in all possible positions .	S-69
OWN	Thus , for the subject detection task :	S-70
OWN	will generate strings of tags including :	S-71
OWN	Hypertags are always inserted in pairs , so that closure is enforced .	S-72
OWN	There were arbitrary limits of a maximum of 10 words in the pre-subject and 10 words within the subject for the initial work described here .	S-73
OWN	These are now extended to 15 words in the pre-subject , 12 in the subject - see Section.	S-74
OWN	There must be at least one word beyond the end of the subject and before the end-of-sentence mark .	S-75
OWN	Therefore , using the initial restrictions , in a sentence of 22 words or more ( counting punctuation marks as words ) there could be 100 alternative placements .	S-76
OWN	However , some words will have more than one possible tag .	S-77
OWN	For instance , in sentenceabove 5 words have 2 alternative tags , which will generatepossible strings before the hypertags are inserted .	S-78
OWN	Since there are 22 words ( including punctuation ) the total number of strings would be	S-79
OWN	It is not feasible to detect one string out of this number : if the classifier marked all strings incorrect the percentage wrongly classified would only be, yet it would be quite useless .	S-80
OWN	In order to find the correct string most of the outside candidates must be dropped ,	S-81
BAS	A minimal grammar , set out inin EBNF form , is composed of 9 rules .	S-82
OTH	For instance , the subject must contain a noun-type word .	S-83
OTH	Applying this particular rule to sentenceabove would eliminate candidate stringsand.	S-84
OTH	We also have the 2 arbitrary limits on length of pre-subject and subject .	S-85
OWN	There is a small set of 4 extensions to the grammar , or semi-local constraints .	S-86
OWN	For instance , if a relative pronoun occurs , then a verb must follow in that constituent .	S-87
OWN	On the technical manuals the constraints of the grammatic framework put up to 6 % of declarative sentences outside our system , most commonly because the pre-subject is too long .	S-88
OWN	A small number are excluded because the system cannot handle a co-ordinated head .	S-89
OWN	With the length of pre-subject extended to 15 words , and subject to 12 words , an average of 2 % are excluded ( 7 out of 351 ) .	S-90
OWN	The grammatic framework alone does not reduce the number of candidate strings sufficiently for the subject detection stage .	S-91
BAS	This problem is addressed further by a method suggested bythat local constraints can rein in the generation of an intractable number of possibilities .	S-92
OWN	In our system the local constraints are prohibited tag pairs and triples .	S-93
OWN	These are adjacent tags which are not allowed , such as `` determiner - verb or '' `` start of subject - verb '' .	S-94
OWN	If during the generation of a candidate string a prohibited tuple is encountered , then the process is aborted .	S-95
OWN	There are about 100 prohibited pairs and 120 triples .	S-96
OWN	By using these methods the number of candidate strings is drastically reduced .	S-97
OWN	For the technical manuals an average of 4 strings , seldom more than 15 strings , are left .	S-98
OWN	Around 25 % of sentences are left with a single string .	S-99
OWN	These filters or `` rules '' differ fundamentally from generative rules that produce allowable strings in a language .	S-100
OWN	In those cases only productions that are explicitly admitted are allowed .	S-101
OWN	Here , in contrast , anything that is not expressly prohibited is allowed .	S-102
OWN	At this stage the data is ready to present to the neural net .	S-103
OWN	Figuregives an overview of the whole process .	S-104
OWN	Different network architectures have been investigated , but they all share the same input and output representation .	S-105
OWN	The output from the net is a vector whose 2 elements , or nodes , represent `` correct '' and `` incorrect '' , `` yes '' and `` no '' - see Figure.	S-106
OWN	The input to the net is derived from the candidate strings , the sequences of tags and hypertags .	S-107
OWN	These must be converted to binary vectors .	S-108
OWN	Each element of the vector will represent a feature that is flagged 0 or 1 , absent or present .	S-109
OWN	Though the form in which the vector is written may give an illusion of representing order , no sequential order is maintained .	S-110
OWN	A method of representing a sequence must be chosen .	S-111
OWN	The sequential order of the input is captured here , partially , by taking adjacent tags , pairs and triples , as the feature elements .	S-112
OWN	The individual tags are converted to a bipos and tripos representation .	S-113
OWN	Using this method each tag is in 3 tripos and 2 bipos elements .	S-114
OWN	This highly redundant code will aid the processing of sparse data typical of natural language .	S-115
OWN	For most of the work described here the sentence was dynamically truncated 2 words beyond the hypertag marking the close of the subject .	S-116
OWN	This process has now been improved by going further along the sentence .	S-117
BAS	The net that gave best results was a simple single layer net ( Figure) , derived from the Hodyne net of.	S-118
OWN	This is conventionally a `` single layer '' net , since there is one layer of processing nodes .	S-119
OWN	Multi-layer networks , which can process linearly inseparable data , were also investigated , but are not necessary for this particular processing task .	S-120
OWN	The linear separability of data is related to its order , and this system uses higher order pairs and triples as input .	S-121
OWN	The question of appropriate network architecture is examined in,and.	S-122
OWN	The net is presented with training strings whose desired classification has been manually marked .	S-123
OWN	The weights on the connections between input and output nodes are adjusted until a required level of performance is reached .	S-124
OWN	Then the weights are fixed and the trained net is ready to classify unseen sentences .	S-125
OWN	The prototype accessible via the Internet has been trained on sentences from the technical manuals , slightly augmented .	S-126
OWN	Initially the weighted links are disabled .	S-127
OWN	When a string is presented to the network in training mode , it activates a set of input nodes .	S-128
OWN	If an input node is not already linked to the output node representing the desired response , it will be connected and the weight on the connection will be initialised to 1.0 .	S-129
OWN	Most input nodes are connected to both outputs , since most tuples occur in both grammatical and ungrammatical strings .	S-130
OWN	However , some will only be connected to one output - see Figure.	S-131
OWN	The input layer potentially has a node for each possible tuple .	S-132
OWN	With 28 tags , 2 hypertags and a start symbol the upper bound on the number of input nodes is.	S-133
OWN	In practice the maximum activated is currently about 1000 .	S-134
OWN	In testing mode , if a previously unseen tuple appears it makes zero contribution to the result .	S-135
OWN	The activations at the input layer are fed forward through the weighted connections to the output nodes , where they are summed .	S-136
OWN	The highest output marks the winning node .	S-137
OWN	If the desired node wins , then no action is taken .	S-138
OWN	If the desired node does not win , then the weight on connections to the desired node are incremented , while the weights on connections to the unwanted node are decremented .	S-139
OWN	This algorithm differs from some commonly used methods .	S-140
OWN	In feed forward networks trained in supervised mode to perform a classification task different penalty measures can be used to trigger a weight update .	S-141
OWN	Back propagation and some single layer training methods typically minimise a metric based on the least squared error ( LSE ) between desired and actual activation of the output nodes .	S-142
OWN	The reason why a differentiable error measure of this sort is necessary for multi-layer nets is well documented.	S-143
OWN	However , for single layer nets we can choose to update weights directly : the error at an output node can trigger weight updates on the connections that feed it .	S-144
OWN	Solutions with LSE are not necessarily the same as minimising the number of misclassifications , and for certain types of data this second method of direct training may be appropriate .	S-145
OWN	Now , in the natural language domain it is desirable to get information from infrequent as well as common events .	S-146
OWN	Rare events , rather than being noise , can make a useful contribution to a classification task .	S-147
OWN	We need a method that captures information from infrequent events , and adopt a direct measure of misclassification .	S-148
OWN	This may be better suited to data with a `` Zipfian '' distribution.	S-149
OWN	The update factor is chosen to meet several requirements .	S-150
OWN	It should always be positive , and asymptotic to maximum and minimum bounds .	S-151
OWN	The factor should be greatest in the central region , least as it moves away in either direction .	S-152
OWN	We are currently still using the original Hodyne function because it works well in practice .	S-153
OWN	The update factor is given in the following formula .	S-154
OWN	Iffor strengthening weights andfor weakening them , then	S-155
OWN	Recall that weights are initialised to 1.0 .	S-156
OWN	After training we find that the weight range is bounded by	S-157
OWN	Total time for training is measured in seconds .	S-158
OWN	The number of iterative cycles that are necessary depends on the threshold chosen for the trained net to cross , and on details of the vector representation .	S-159
OWN	The demonstration prototype takes about 15 seconds .	S-160
OWN	With the most recent improved representation about 1000 strings can be trained in 1 second , to 97 % .	S-161
OWN	The results from using these nets are given in Table.	S-162
OWN	It was found that triples alone gave as good results as pairs and triples together .	S-163
OWN	And though the nets easily train to 99 % correct , the lower threshold gives slightly better generalisation and thus gives better results on the test data .	S-164
OWN	When the trained net is run on unseen data the weights on the links are fixed .	S-165
OWN	Any link that is still disabled is activated and initialised to 0 , so that tuples which have not occurred in the training corpus make no contribution to the classification task .	S-166
OWN	Sentences are put through the pre-processer one at a time and the candidate strings which are generated are then presented to the network .	S-167
OWN	The output is now interpreted differently .	S-168
OWN	The difference between the `` yes '' and `` no '' activation levels is recorded for each string , and this score is considered a measure of grammaticality ,.	S-169
OWN	The string with the highestscore is taken as the correct one .	S-170
OWN	For the results given below , the networks were trained on part of the corpus and tested on another part of the corpus .	S-171
OWN	For the prototype in which users can process their own text , the net was trained on the whole corpus , slightly augmented .	S-172
OWN	There are several measures of correctness that can be taken when results are evaluated .	S-173
OWN	The most lenient is whether or not the subject and head markers are placed correctly - the type of measure used in the IBM / Lancaster work.	S-174
OWN	Since we are working towards a hierarchical language structure , we may want the words within constituents correctly tagged , ready for the next stage of processing .	S-175
OWN	`` correct - A '' also requires that the words within the subject are correctly tagged .	S-176
OWN	The results in Tablesandgive an indication of performance levels .	S-177
OWN	When parses are postulated for a sentence negative as well as positive examples are likely to occur .	S-178
OWN	Now , in natural language negative correlations are an important source of information : the occurrence of some words or groups of words inhibit others from following .	S-179
OWN	We wish to exploit these constraints .	S-180
OTH	recognised this , and introduced the idea of distituents .	S-181
OTH	These are elements of a sentence that should be separated , as opposed to elements of constituents that cling together .	S-182
OTH	addresses the problem of finding a valid metric for distituency by using a generalized mutual information statistic .	S-183
OTH	Distituency is marked by a mutual information minima .	S-184
OTH	His method is supported by a small 4 rule grammar .	S-185
CTR	However , this approach does not fully capture the sense in which inhibitory factors play a negative and not just a neutral role .	S-186
OWN	We want to distinguish between items that are unlikely to occur ever , and those that have just not happened to turn up in the training data .	S-187
OWN	For example , in sentence [ CREF] above strings,and [ CREF] can never be correct .	S-188
OWN	These should be distinguished from possibly correct parses that are not in the training data .	S-189
OTH	In order that `` improbabilities '' can be modelled by inhibitory connectionsshow how a Hidden Markov Model can be implemented by a neural network .	S-190
OTH	The theoretical ground for incorporating negative examples in a language learning process originates with the work of, developed by.	S-191
OTH	He examined the process of learning the grammar of a formal language from examples .	S-192
OTH	He showed that , for languages at least as high in the Chomsky hierarchy as CFGs , inference from positive data alone is strictly less powerful than inference from both positive and negative data together .	S-193
OTH	To illustrate this informally consider a case of inference from a number of examples : as they are presented to the inference machine , possible grammars are postulated .	S-194
CTR	However , with positive data alone a problem of over generalization arises : the postulated grammar may be a superset of the real grammar , and sentences that are outside the real grammar could be accepted .	S-195
OWN	If both positive and negative data is used , counter examples will reduce the postulated grammar so that it is nearer the real grammar .	S-196
OWN	developed his theory for formal languages : it is argued that similar considerations apply here .	S-197
OTH	A grammar may be inferred from positive examples alone for certain subsets of regular languages, or an inference process may degenerate into a look up procedure if every possible positive example is stored .	S-198
OTH	In these cases negative information is not required , but they are not plausible models for unbounded natural language .	S-199
OWN	In our method the required parse is found by inferring the grammar from both positive and negative information , which is effectively modelled by the neural net .	S-200
OWN	Future work will investigate the effect of training the networks on the positive examples alone .	S-201
OWN	With our current size corpus there is not enough data .	S-202
OWN	The relationship between the neural net and the rules in the prohibition table should be seen in the following way .	S-203
OWN	Any single rule prohibiting a tuple of adjacent tags could be omitted and the neural network would handle it by linking the node representing that tuple to `` no '' only .	S-204
OWN	However , for some processing steps we need to reduce the number of candidate tag strings presented to the neural network to manageable proportions ( see Section) .	S-205
OWN	The data must be pre-processed by filtering through the prohibition rule constraints .	S-206
OWN	If the number of candidate strings is within desirable bounds , such as for the head detection task , no rules are used .	S-207
OWN	Our system is data driven as far as possible : the rules are invoked if they are needed to make the problem computationally tractable .	S-208
AIM	Our working prototype indicates that the methods described here are worth developing , and that connectionist methods can be used to generalise from the training corpus to unseen text .	S-209
OWN	Since data can be represented as higher order tuples , single layer networks can be used .	S-210
CTR	The traditional problems of training times do not arise .	S-211
OTH	We have also used multi-layer nets on this data : they have no advantages , and perform slightly less well.	S-212
OWN	The supporting role of the grammatic framework and the prohibition filters should not be underestimated .	S-213
OWN	Whenever the scope of the system is extended it has been found necessary to enhance these elements .	S-214
OWN	The most laborious part of this work is preparing the training data .	S-215
OWN	Each time the representation is modified a new set of strings is generated that need marking up .	S-216
OWN	An autodidactic check is now included which speeds up this task .	S-217
OWN	We run marked up training data through an early version of the network trained on the same data , so the results should be almost all correct .	S-218
OWN	If an `` incorrect '' parse occurs we can then check whether that sentence was properly marked up .	S-219
OWN	Some of the features of the system described here could be used in a stochastic process .	S-220
OWN	However , connectionist methods have low computational loads at runtime .	S-221
OWN	Moreover , they can utilise more of the implicit information in the training data by modelling negative relationships .	S-222
OWN	This is a powerful concept that can be exploited in the effort to squeeze out every available piece of useful information for natural language processing .	S-223
OWN	Future work is planned to extend this very limited partial parser , and decompose sentences further into their hierarchical constituent parts .	S-224
OWN	In order to do this a number of subsidiary tasks will be addressed .	S-225
OWN	The system is being improved by identifying groups of words that act as single lexical items .	S-226
OWN	The decomposition of the problem can be investigated further : for instance , should the tag disambiguation task precede the placement of the subject boundary markers in a separate step ?	S-227
OWN	More detailed investigation of language representation issues will be undertaken .	S-228
OWN	And the critical issues of investigating the most appropriate network architectures will be carried on .	S-229
