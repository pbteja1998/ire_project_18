<?xml version='1.0' encoding='ISO-8859-1'?>
<!DOCTYPE PAPER SYSTEM "paper-structure.dtd">
<PAPER>
<METADATA>
<FILENO>9408004</FILENO>
<APPEARED><CONFERENCE TYPE='Workshop'>ACL</CONFERENCE><YEAR>1994</YEAR></APPEARED>
<CLASSIFICATION> Lg.Pr.St Lg.Pr.Gr.Ps </CLASSIFICATION>
</METADATA>
<TITLE> Parsing with Principles and Probabilities </TITLE>
<AUTHORLIST>
<AUTHOR>Andrew Fordham</AUTHOR>
<AUTHOR>Matthew Crocker</AUTHOR>
</AUTHORLIST>
<ABSTRACT>
<A-S ID='A-0' AZ='OWN'> This paper is an attempt to bring together two approaches to language analysis . </A-S>
<A-S ID='A-1' AZ='AIM'> The possible use of probabilistic information in principle-based grammars and parsers is considered , including discussion on some theoretical and computational problems that arise . </A-S>
<A-S ID='A-2' AZ='OWN'> Finally a partial implementation of these ideas is presented , along with some preliminary results from testing on a small set of sentences . </A-S>
</ABSTRACT>
<BODY>
<DIV DEPTH='1'>
<HEADER ID='H-0'> Introduction </HEADER>
<P>
<S ID='S-0' AZ='BKG'> Both principle-based parsing and probabilistic methods for the analysis of natural language have become popular in the last decade . </S>
<S ID='S-1' AZ='BKG'> While the former borrows from advanced linguistic specifications of syntax , the latter has been more concerned with extracting distributional regularities from language to aid the implementation of NLP systems and the analysis of corpora . </S>
</P>
<P>
<S ID='S-2' AZ='BKG'> These symbolic and statistical approaches are beginning to draw together as it becomes clear that one cannot exist entirely without the other : the knowledge of language posited over the years by theoretical linguists has been useful in constraining and guiding statistical approaches , and the corpora now available to linguists have resurrected the desire to account for real language data in a more principled way than had previously been attempted . </S>
</P>
<P>
<S ID='S-3' AZ='AIM'> This paper falls directly between these approaches , using statistical information derived from corpora analysis to weight syntactic analyses produced by a ` principles and parameters ' parser . </S>
<S ID='S-4' AZ='TXT'> The use of probabilistic information in principle-based grammars and parsers is considered , including discussion on some theoretical and computational problems that arise . </S>
<S ID='S-5' AZ='TXT'> Finally a partial implementation of these ideas is presented , along with some preliminary results from testing on a small set of sentences . </S>
</P>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-1'> Government-Binding Theory </HEADER>
<P>
<S ID='S-6' AZ='OTH'> The principles and parameters paradigm in linguistics is most fully realised in the Government-Binding Theory ( GB ) of <REF TYPE='A'>Chomsky 1981</REF> , <REF TYPE='A'>Chomsky 1986</REF> and others . </S>
<S ID='S-7' AZ='OTH'> The grammar is divided into modules which filter out ungrammatical structures at the various levels of representation ; these levels are related by general transformations . </S>
<S ID='S-8' AZ='OTH'> A sketch of the organisation of GB ( the ` T-model ' ) is shown in figure <CREF/> . </S>
</P>
<IMAGE ID='I-0'/>
<P>
<S ID='S-9' AZ='CTR'> Little work has been done on the complexity of algorithms used to parse with a principle-based grammar , since such grammars do not exist as accepted mathematically well-defined constructs . </S>
<S ID='S-10' AZ='CTR'> It has been estimated that in general , principle-based parsing can only be accomplished in exponential time , i.e. <EQN/> <REF  TYPE='P'>Berwick and Weinberg 1984</REF>, <REF  TYPE='P'>Weinberg 1988</REF> . </S>
</P>
<P>
<S ID='S-11' AZ='OTH'> A feature of principle-based grammars is their potential to assign some meaningful representation to a string which is strictly ungrammatical . </S>
<S ID='S-12' AZ='OTH'> It is an inherent feature of phrase structure grammars that they classify the strings of words from a language into two ( infinite ) sets , one containing the grammatical strings and the other the ungrammatical strings . </S>
<S ID='S-13' AZ='CTR'> Although attempts have been made to modify PS grammars / parsers to cope with extragrammatical input <REF TYPE='P'>Carbonell and Hayes 1983</REF> , <REF TYPE='P'>Douglas and Dale 1992</REF> , <REF TYPE='P'>Jensen et al. 1983</REF> , <REF TYPE='P'>Mellish 1989</REF> , this is a feature which has to be ` added on ' and tends to affect the statement of the grammar . </S>
</P>
<P>
<S ID='S-14' AZ='OTH'> Due to the lack of an accepted formalism for the specification of principle-based grammars , <REF TYPE='A' SELF="YES">Crocker and Lewin 1992</REF> define the declarative ` Proper Branch ' formalism , which can be used with a number of different parsing methods . </S>
</P>
<P>
<S ID='S-15' AZ='OTH'> A proper branch is a set of three nodes -- a mother and two daughters -- which are constructed by the parser , using a simple mechanism such as a shift-reduce interpreter , and then ` licensed ' by the principles of grammar . </S>
<S ID='S-16' AZ='OTH'> A complete phrase marker of the input string can then be constructed by following the manner in which the mother node from one proper branch is used as a daughter node in a dominating proper branch . </S>
</P>
<P>
<S ID='S-17' AZ='OTH'> Each proper branch is a binary branching structure , and so all grammatical constraints will need to be encoded locally . </S>
<S ID='S-18' AZ='OTH'> <REF TYPE='A' SELF="YES">Crocker 1992</REF> develops `` a ` representational ' reformulation of the transformational model which decomposes syntactic analysis into several representation types -- including phrase structure , chains , and coindexation -- allowing one to maintain the strictly local characterisation of principles with respect to their relevant representation types , '' <REF TYPE='P' SELF="YES">Crocker and Lewin 1992</REF> , p. 511 . </S>
</P>
<P>
<S ID='S-19' AZ='OTH'> By using the proper branch method of axiomatising the grammar , the structure building section of the parser is only constrained in that it must produce proper branches ; it is therefore possible to experiment with different interpreters ( i.e. structure proposing engines ) while keeping the grammar constant . </S>
</P>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-2'> The Grammar and Parser </HEADER>
<P>
<S ID='S-20' AZ='BAS'> A small principle-based parser was built , following the proper branch formalism developed in <REF SELF="YES" TYPE='A'>Crocker and Lewin 1992</REF> . </S>
<S ID='S-21' AZ='OWN'> Although the grammar is very limited , the use of probabilities in ranking the parser 's output can be seen as a first step towards implementing a principle-based parser using a more fully specified collection of grammar modules . </S>
</P>
<P>
<S ID='S-22' AZ='OWN'> The grammar is loosely based on three modules taken from Government-Binding Theory -- X-bar theory , Theta Theory and Case Theory . </S>
<S ID='S-23' AZ='OWN'> Although these embody the spirit of the constraints found in <REF TYPE='A'>Chomsky 1981</REF> they are not intended to be entirely faithful to this specification of syntactic theory . </S>
<S ID='S-24' AZ='OWN'> There is also only a single level of representation ( which is explicitly constructed for output purposes but not consulted by the parser ) . </S>
<S ID='S-25' AZ='OWN'> This representation is interpreted as S-structure . </S>
</P>
<P>
<S ID='S-26' AZ='TXT'> Explanations of the knowledge contained within each grammar principle is given in the following sections . </S>
</P>
<DIV DEPTH='2'>
<HEADER ID='H-3'> X Theory </HEADER>
<P>
<S ID='S-27' AZ='OTH'> X-bar Theory uses a set of schemata to license local subtrees . </S>
<S ID='S-28' AZ='BAS'> We use a parametrised version of the X-bar schemata , similar to that of <REF TYPE='A'>Muysken 1983</REF> , but employing features which relate to the state of the head word 's theta grid to give five schemata ( figure <CREF/> ) . </S>
</P>
<IMAGE ID='I-1'/>
<P>
<S ID='S-29' AZ='OWN'> A node includes the following features ( among others ) : </S>
</P>
<P>
<S ID='S-30' AZ='OWN'> Category : the standard category names are employed . </S>
</P>
<P>
<S ID='S-31' AZ='OWN'> Specifier ( SPEC ) : this feature specifies whether the word at the head of the phrase being built requires a specifier . </S>
</P>
<P>
<S ID='S-32' AZ='OWN'> Complement ( COMP ) : the complement feature is redundant in that the information used to derive it 's value is already present in a word 's theta grid , and will therefore be checked for well-formedness by the theta criterion . </S>
<S ID='S-33' AZ='OWN'> Since this information is not referenced until later , the COMP feature is used to limit the number of superfluous proper-branches generated by the parser . </S>
</P>
<P>
<S ID='S-34' AZ='OWN'> The head ( i.e. lexical item ) of a node is carried on each projection of that node along with its theta grid . </S>
</P>
<P>
<S ID='S-35' AZ='OWN'> The probabilities for occurrences of the X-bar schema were obtained from sentences from the preliminary Penn Treebank corpus of the Wall Street Journal , chosen because of their length and the head of their verb phrase ( i.e. the main verbs were all from the set for which theta role data was obtained ) ; the examples were manually parsed by the authors . </S>
</P>
<P>
<S ID='S-36' AZ='OWN'> The probabilities were calculated using the following equation , where <EQN/> is a specific schema , <EQN/> is the set of X-bar schemata and A and B and C are variables over category , SPEC and COMP feature bundles : </S>
<IMAGE ID='I-2'/>
</P>
<P>
<S ID='S-37' AZ='OWN'> This is different to manner in which probabilities are collected for stochastic context-free grammars , where the identity of the mother node is taken into account , as in the equation below : </S>
<IMAGE ID='I-3'/>
</P>
<P>
<S ID='S-38' AZ='OWN'> This would result in misleading probabilities for the X-bar schemata since the use of schemata <CREF/> , <CREF/> , and <CREF/> would immediately bring down the probability of a parse compared to a parse of the same string which happened to use only <CREF/> and <CREF/> . </S>
</P>
<P>
<S ID='S-39' AZ='OWN'> The overall ( X-bar ) likelihood of a parse can then be computed by multiplying together all the probabilities obtained from each application of the schemata , in a manner analogous to that used to obtain the probability of a phrase marker generated by an SCFG. Using the schemata in this way suggests that the building of structure is category independent , i.e. it is just as likely that a verb will have a ( filled ) specifier position as it is for a noun . </S>
<S ID='S-40' AZ='CTR'> The work on stochastic context-free grammars suggests a different set of results , in that the specific categories involved in expansions are all important . </S>
<S ID='S-41' AZ='OTH'> While SCFGs will tend to deny that all categories expand in certain ways with the same probabilities , they make this claim while using a homogeneous grammar formalism . </S>
<S ID='S-42' AZ='OWN'> When a more modular theory is employed , the source of the supposedly category specific information is not as obvious . </S>
<S ID='S-43' AZ='OWN'> The use of lexical probabilities on specifier and complement co-occurrence with specific heads ( i.e. lexical items ) could exihibit properties that appear to be category specific , but are in fact caused by common properties which are shared by lexical items of the same category . </S>
<S ID='S-44' AZ='CTR'> Since it can be argued that the probabilistic information on lexical items will be needed independently , there is no need to use category specific information in assigning probabilities to syntactic configurations . </S>
</P>
</DIV>
<DIV DEPTH='2'>
<HEADER ID='H-4'> Theta Theory </HEADER>
<P>
<S ID='S-45' AZ='OTH'> Theta theory is concerned with the assignment of an argument structure to a sentence . </S>
<S ID='S-46' AZ='OTH'> A verb has a number of the thematic ( or ` theta ' ) roles which must be assigned to its arguments , e.g. a transitive verb has one theta role to ` discharge ' which must be assigned to an NP . </S>
</P>
<P>
<S ID='S-47' AZ='OWN'> If a binary branching formalism is employed , or indeed any formalism where the arguments of an item and the item itself are not necessarily all sisters , the problem of when to access the probability of a theta application is presented . </S>
<S ID='S-48' AZ='OWN'> The easiest method of obtaining and applying theta probabilities will be with reference to whole theta grids . </S>
<S ID='S-49' AZ='OWN'> Each theta grid for a word will be assigned a probability which is not dependent on any particular items in the grid , but rather on the occurrence of the theta grid as a whole . </S>
</P>
<P>
<S ID='S-50' AZ='OWN'> A preliminary version of the Penn Treebank bracketed corpus was analysed to extract information on the sisters of particular verbs . </S>
<S ID='S-51' AZ='OWN'> Although the Penn Treebank data is unreliable since it does not always distinguish complements from adjuncts , it was the only suitable parsed corpus to which the authors had access . </S>
<S ID='S-52' AZ='OWN'> Although the distinction between complements and adjuncts is a theoretically interesting one , the process of determining which constructions fill which functional roles in the analysis of real text often creates a number of problems ( see <REF TYPE='A'>Hindle and Rooth 1993</REF> for discussion on this issue regarding output of the Fidditch parser <REF TYPE='P'>Hindle 1993</REF> ) . </S>
</P>
<P>
<S ID='S-53' AZ='OWN'> The probabilities for each of the verbs ' theta grids were calculated using the equation below , where <EQN/> is the probability of the theta grid <EQN/> occurring with the verb v , <EQN/> is an occurrence of the items in <EQN/> being licensed by v , and S ranges over all theta grids for v : </S>
<IMAGE ID='I-4'/>
</P>
</DIV>
<DIV DEPTH='2'>
<HEADER ID='H-5'> Case Theory </HEADER>
<P>
<S ID='S-54' AZ='OTH'> In its simplest form , Case theory invokes the Case filter to ensure that all noun phrases in a parse are assigned ( abstract ) case . </S>
<S ID='S-55' AZ='OTH'> Case theory differs from both X-bar and Theta theory in that it is category specific : only NPs require , or indeed can be assigned , abstract case . </S>
<S ID='S-56' AZ='OWN'> If we are to implement a probabilistic version of a modular grammar theory incorporating a Case component , a relevant question is : are there multiple ways of assigning Case to noun phrases in a sentence ? </S>
<S ID='S-57' AZ='OWN'> i.e. can ambiguity arise due to the presence of two candidate Case assigners . </S>
</P>
<P>
<S ID='S-58' AZ='OTH'> Case theory suggests that the answer to this is negative , since Case assignment is linked to theta theory via visibility , and it is not possible for an NP to receive more than one theta role . </S>
<S ID='S-59' AZ='OTH'> As a result , the use of Case probabilities in a parser would be at best unimportant , since some form of ambiguity is needed in the module , i.e. it is possible to satisfy the Case filter in more than one way , for probabilities associated with the module to be of any use . </S>
<S ID='S-60' AZ='OWN'> While having a provision for using probabilities deduced from Case information , the implemented parser does not in fact use Case in its parse ranking operations . </S>
</P>
</DIV>
<DIV DEPTH='2'>
<HEADER ID='H-6'> Local Calculation </HEADER>
<P>
<S ID='S-61' AZ='OWN'> The use of a heterogeneous grammar formalism and multiple probabilities invokes the problem of their combination . </S>
<S ID='S-62' AZ='OWN'> There are at least two ways in which each mother 's probabilities can be calculated ; firstly , the probability information of the same type can be used : the daughters ' X-bar probabilities alone could be used in calculating the mother 's X-bar probability . </S>
<S ID='S-63' AZ='OWN'> Alternatively , a combination of some or all of the daughters ' probability features could be employed , thus making , e.g. , the X-bar probability of the mother dependent upon all the stochastic information from the daughters , including theta and Case probabilities , etc . </S>
</P>
<P>
<S ID='S-64' AZ='OWN'> The need for a method of combining the daughter probabilities into a useful figure for the calculation of the mother probabilities is likely to involve trial and error , since theory thus far has had nothing to say on the subject . </S>
<S ID='S-65' AZ='OWN'> The former method , using only the relevant daughter probabilities , therefore seems to be the most fruitful path to follow at the outset , since it does not require a way of integrating probabilities from different modules while the parse is in progress , nor is it as computationally expensive . </S>
</P>
</DIV>
<DIV DEPTH='2'>
<HEADER ID='H-7'> Global Calculation </HEADER>
<P>
<S ID='S-66' AZ='OWN'> The manner in which the global probability is calculated will be partly dependent upon the information contained in the local probability calculations . </S>
</P>
<P>
<S ID='S-67' AZ='OWN'> If the probabilities for partial analyses have been calculated using only probabilities of the same types from the subanalyses -- e.g. X-bar , Theta -- the probabilities at the top level will have been calculated using informationally distinct figures . </S>
<S ID='S-68' AZ='OWN'> This has the advantage of making ` pure ' probabilities available , in that the X-bar probability will reflect the likelihood of the structure alone , and will be ` uncontaminated ' by any other information . </S>
<S ID='S-69' AZ='OWN'> It should then be possible to experiment with different methods of combining these probabilities , other than the obvious ` multiplying them together ' techniques , which could result in one type of probabililty emerging as the most important . </S>
</P>
<P>
<S ID='S-70' AZ='OWN'> On the other hand , if probabilities calculated during the parse take all the different types of probabilities into account at each calculation -- i.e. the X-bar , theta , etc . </S>
<S ID='S-71' AZ='OWN'> probabilities on daughters are all taken into account when calculating the mother 's X-bar probability -- the probabilities at the top level will not be pure , and a lot of the information contained in them will be redundant since they will share a large subset of the probabilities used in their separate calculations . </S>
<S ID='S-72' AZ='OWN'> It will not therefore be easy to gain theoretical insight using these statistics , and their most profitable method of combination is likely to be more haphazard affair than when more pure probabilities are used . </S>
</P>
<P>
<S ID='S-73' AZ='OWN'> The parser used in testing employed the first method and therefore produced separate module probabilities for each node . </S>
<S ID='S-74' AZ='OWN'> For the lack of a better , theoretically motivated method for combining these figures , the product of the probabilities was taken as the global probability for each parse . </S>
</P>
</DIV>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-8'> Testing the Parser </HEADER>
<P>
<S ID='S-75' AZ='OWN'> The parser was tested using sixteen sentences containing verbs for which data had been collected from the Penn Treebank corpus . </S>
<S ID='S-76' AZ='OWN'> The sentences were created by the authors to exhibit at least a degree of ambiguity when it came to attaching a post-verbal phrase as an adjunct or a complement . </S>
<S ID='S-77' AZ='OWN'> In order to force the choice of the ` best ' parse on to the verb , the probabilities of theta grids for nouns , prepositions , etc . </S>
<S ID='S-78' AZ='OWN'> was kept constant . </S>
</P>
<P>
<S ID='S-79' AZ='OWN'> Of these 16 highest ranked parses , 7 are the expected parse , with the other 9 exhibiting some form of mis-attachment . </S>
<S ID='S-80' AZ='OWN'> The fact that each string received multiple parses ( the mean number of analyses being 9.135 , and the median , 6 ) suggests that the probabilistic information did favourably guide the selection of a single analysis . </S>
</P>
<P>
<S ID='S-81' AZ='OWN'> It is not really possible to say from these results how successful the whole approach of probabilistic principle-based parsing would be if it were fully implemented . </S>
<S ID='S-82' AZ='OWN'> The inconclusive nature of the results obtained was due to a number of limiting factors of the implementation including the simplicity of the grammar and the lack of available data . </S>
</P>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-9'> Discussion </HEADER>
<DIV DEPTH='2'>
<HEADER ID='H-10'> Limitations of the Grammar </HEADER>
<P>
<S ID='S-83' AZ='BAS'> The grammar employed is a partial characterisation of <REFAUTHOR>Chomsky</REFAUTHOR> 's Government-Binding theory <REF TYPE='P'>Chomsky 1981</REF> , <REF TYPE='P'>Chomsky 1986</REF> and only takes account of very local constraints ( i.e. X-bar , Theta and Case ) ; a way of encoding all constraints in the proper branch formalism <REF TYPE='P' SELF="YES">Crocker 1992</REF> will be needed before a grammar of sufficient coverage to be useful in corpora analysis can be formulated . </S>
<S ID='S-84' AZ='OWN'> The problem with using results obtained from the implementation given here is that the grammar is sufficiently underspecified and so leaves too great a task for the probabilistic information . </S>
</P>
<P>
<S ID='S-85' AZ='OWN'> This approach could be viewed as putting the cart before the horse ; the usefulness of stochastic information in parsers presumes that a certain level of accuracy can be achieved by the grammar alone . </S>
<S ID='S-86' AZ='OWN'> While GB is an elegant theory of cognitive syntax , it has yet to be shown that such a modular characteristion can be successfully employed in corpus analysis . </S>
</P>
</DIV>
<DIV DEPTH='2'>
<HEADER ID='H-11'> Statistical Data and their Source </HEADER>
<P>
<S ID='S-87' AZ='OWN'> The use of the preliminary Penn Treebank corpus for the extraction of probabilities used in the implementation above was a choice forced by lack of suitable materials . </S>
<S ID='S-88' AZ='OWN'> There are still very few parsed corpora available , and none that contain information which is specified to the level required by , e.g. , a GB grammar . </S>
<S ID='S-89' AZ='OWN'> While this is not an absolute limitation , in that it is theoretically possible to extract this information manually or semi-automatically from a corpus , time constraints entailed the rejection of this approach . </S>
</P>
<P>
<S ID='S-90' AZ='OWN'> It would be ultimately desirable if the use of probabilities in principle-based parsing could be used to mirror the way that a syntactic theory such as Government-Binding handles constructions -- various modules of the grammar conspire to rule out illegal structures or derivations . </S>
<S ID='S-91' AZ='OWN'> It would be an elegant result if a construction such as the passive were to use probabilities for chains , Case assignment etc . </S>
<S ID='S-92' AZ='OWN'> to select a parse that reflected the lexical changes that had been undergone , e.g. the greater likelihood of an NP featuring in the verb 's theta grid . </S>
<S ID='S-93' AZ='OWN'> It is this property of a number of modules working hand in hand that needs to be carried over into the probabilistic domain . </S>
</P>
<P>
<S ID='S-94' AZ='OWN'> The objections that linguists once held against statistical methods are disappearing slowly , partly due to results in corpora analysis that show the inadequacy of linguistic theory when applied to naturally occurring data . </S>
<S ID='S-95' AZ='OWN'> It is also the case that the rise of the connectionist phoenix has brought the idea of weighted ( though not strictly probabilistic ) functions of cognition back to the fore , freeing the hands of linguists who believe that while an explanatorily adequate theory of grammar is an elegant construct , its human implementation , and its usage in computational linguists may not be straight forward . </S>
<S ID='S-96' AZ='OWN'> This paper has hopefully shown that an integration of statistical methods and current linguistic theory is a goal worth pursuing . </S>
</P>
</DIV>
</DIV>
</BODY>
<REFERENCELIST>
<REFERENCE>
R. C. <SURNAME>Berwick</SURNAME> and A. S. <SURNAME>Weinberg</SURNAME>.
The Grammatical Basis of Linguistic Performance: Language Use
  and Acquistion.
MIT Press, Cambridge, MA., <DATE>1984</DATE>.
</REFERENCE>
<REFERENCE>
J. W. <SURNAME>Bresnan</SURNAME>.
A realistic transformational grammar.
In M. Halle, J. Bresnan, and G. Miller, editors, Linguistic
  Theory and Psychological Reality. MIT Press, Cambridge, MA., <DATE>1978</DATE>.
</REFERENCE>
<REFERENCE>
J. G. <SURNAME>Carbonell</SURNAME> and P. J. <SURNAME>Hayes</SURNAME>.
Recovery strategies for parsing extragrammatical language.
American Journal of Computational Linguistics,
  9(3-4):123-146, July-December <DATE>1983</DATE>.
</REFERENCE>
<REFERENCE>
N. <SURNAME>Chomsky</SURNAME>.
Lectures on Government and Binding.
Studies in Generative Grammar No. 9. Foris, Dordrecht, <DATE>1981</DATE>.
</REFERENCE>
<REFERENCE>
N. <SURNAME>Chomsky</SURNAME>.
Knowledge of Language: Its Nature, Origin, and Use.
Convergence. Praeger, New York, <DATE>1986</DATE>.
</REFERENCE>
<REFERENCE>
M. W. <SURNAME>Crocker</SURNAME>.
A Logical Model of Competence and Performance in the Human
  Sentence Processor.
PhD thesis, Dept. of Artificial Intelligence, University of
  Edinburgh, <DATE>1992</DATE>.
</REFERENCE>
<REFERENCE>
M. W. <SURNAME>Crocker</SURNAME> and I. <SURNAME>Lewin</SURNAME>.
Parsing as deduction: Rules versus principles.
In B. Neumann, editor, ECAI-92, Proceedings of the Tenth
  European Conference on Artificial Intelligence, pages 508-512, Vienna,
  Austria, <DATE>1992</DATE>.
</REFERENCE>
<REFERENCE>
S. <SURNAME>Douglas</SURNAME> and R. <SURNAME>Dale</SURNAME>.
Towards robust PATR.
In C. Boitet, editor, COLING-92, Proceedings of the Fourteenth
  International Conference on Computational Linguistics, pages 468-474,
  Nantes, France, <DATE>1992</DATE>.
</REFERENCE>
<REFERENCE>
D. <SURNAME>Hindle</SURNAME>.
A parser for text corpora.
In B. T. S. Atkins and A. Zampolli, editors, Computational
  Approaches to the Lexicon. <DATE>1993</DATE>.
</REFERENCE>
<REFERENCE>
D. <SURNAME>Hindle</SURNAME> and M. <SURNAME>Rooth</SURNAME>.
Structural ambiguity and lexical relations.
Computational Linguistics, 19(1), <DATE>1993</DATE>.
</REFERENCE>
<REFERENCE>
K. <SURNAME>Jensen</SURNAME>, G. E. <SURNAME>Heidborn</SURNAME>, L. A. <SURNAME>Miller</SURNAME>, and Y. <SURNAME>Ravin</SURNAME>.
Parse fitting and prose fixing: Getting a hold on ill-formedness.
American Journal of Computational Linguistics,
  9(3-4):147-160, July-December <DATE>1983</DATE>.
</REFERENCE>
<REFERENCE>
C. S. <SURNAME>Mellish</SURNAME>.
Some chart-based techniques for parsing ill-formed input.
In Proceedings of the 27th Annual Meeting of the Association for
  Computational Linguistics, pages 102-109, Vancouver, B.C., <DATE>1989</DATE>.
</REFERENCE>
<REFERENCE>
P. <SURNAME>Muysken</SURNAME>.
Parameterizing the notion of head.
Journal of Linguistic Research, 2:57-76, <DATE>1983</DATE>.
</REFERENCE>
<REFERENCE>
A. S. <SURNAME>Weinberg</SURNAME>.
Mathematical properties of grammars.
In F. J. Newmeyer, editor, Linguistics: the Cambridge Survey,
  Vol. 1, Linguistics Theory: Foundations, chapter 15, pages 415-429.
  Cambridge University Press, Cambridge, <DATE>1988</DATE>.
</REFERENCE>
</REFERENCELIST>
</PAPER>
