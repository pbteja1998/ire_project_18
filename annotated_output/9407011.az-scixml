OWN	We show that in modeling social interaction , particularly dialogue , the attitude of obligation can be a useful adjunct to the popularly considered attitudes of belief , goal , and intention and their mutual and shared counterparts .	A-0
AIM	In particular , we show how discourse obligations can be used to account in a natural manner for the connection between a question and its answer in dialogue and how obligations can be used along with other parts of the discourse context to extend the coverage of a dialogue system .	A-1
OTH	Most computational models of discourse are based primarily on an analysis of the intentions of the speakers,,.	S-0
OTH	An agent has certain goals , and communication results from a planning process to achieve these goals .	S-1
OTH	The speaker will form intentions based on the goals and then act on these intentions , producing utterances .	S-2
OTH	The hearer will then reconstruct a model of the speaker 's intentions upon hearing the utterance .	S-3
CTR	This approach has many strong points , but does not provide a very satisfactory account of the adherence to discourse conventions in dialogue .	S-4
BKG	For instance , consider one simple phenomena : a question is typically followed by an answer , or some explicit statement of an inability or refusal to answer .	S-5
BKG	The intentional story account of this goes as follows .	S-6
BKG	From the production of a question by Agent B , Agent A recognizes Agent B 's goal to find out the answer , and she adopts a goal to tell B the answer in order to be co-operative .	S-7
BKG	A then plans to achieve the goal , thereby generating the answer .	S-8
BKG	This provides an elegant account in the simple case , but requires a strong assumption of co-operativeness .	S-9
BKG	Agent A must adopt agent B 's goals as her own .	S-10
BKG	As a result , it does not explain why A says anything when she does not know the answer or when she is not predisposed to adopting B 's goals .	S-11
OTH	Several approaches have been suggested to account for this behavior .	S-12
OTH	introduced an intentional analysis at the discourse level in addition to the domain level , and assumed a set of conventional multi-agent actions at the discourse level .	S-13
OTH	Others have tried to account for this kind of behavior using social intentional constructs such as Joint intentionsor Shared Plans.	S-14
CTR	While these accounts do help explain some discourse phenomena more satisfactorily , they still require a strong degree of co-operativity to account for dialogue coherence , and do not provide easy explanations of why an agent might act in cases that do not support high-level mutual goals .	S-15
BKG	Consider a stranger approaching an agent and asking , `` Do you have the time ? '' It is unlikely that there is a joint intention or shared plan , as they have never met before .	S-16
BKG	From a purely strategic point of view , the agent may have no interest in whether the stranger 's goals are met .	S-17
BKG	Yet , typically agents will still respond in such situations .	S-18
BKG	As another example , consider a case in which the agent 's goals are such that it prefers that an interrogating agent not find out the requested information .	S-19
BKG	This might block the formation of an intention to inform , but what is it that inspires the agent to respond at all .	S-20
OWN	As these examples illustrate , an account of question answering must go beyond recognition of speaker intentions .	S-21
OWN	Questions do more than just provide evidence of a speaker 's goals , and something more than adoption of the goals of an interlocutor is involved in formulating a response to a question .	S-22
OTH	Some researchers , e.g. ,,, assume a library of discourse level actions , sometimes called dialogue games , which encode common communicative interactions .	S-23
OTH	To be co-operative , an agent must always be participating in one of these games .	S-24
OTH	So if a question is asked , only a fixed number of activities , namely those introduced by a question , are co-operative responses .	S-25
CTR	Games provide a better explanation of coherence , but still require the agents to recognize each other 's intentions to perform the dialogue game .	S-26
OTH	As a result , this work can be viewed as a special case of the intentional view .	S-27
OTH	An interesting model is described by, which separates out the conversational games from the task-related games in a way similar way to.	S-28
CTR	Because of this separation , they do not have to assume co-operation on the tasks each agent is performing , but still require recognition of intention and co-operation at the conversational level .	S-29
CTR	It is left unexplained what goals motivate conversational co-operation .	S-30
CTR	The problem with systems which impose co-operativity in the form of automatic goal adoption is that this makes it impossible to reason about cases in which one might want to violate these rules , especially in cases where the conversational co-operation might conflict with the agent 's personal goals .	S-31
CTR	We are developing an alternate approach that takes a step back from the strong plan-based approach .	S-32
OTH	By the strong plan-based account , we mean models where there is a set of personal goals which directly motivates all the behavior of the agent .	S-33
CTR	While many of the intuitions underlying these approaches seems close to right , we claim it is a mistake to attempt to analyze this behavior as arising entirely from the agent 's high-level goals .	S-34
OWN	We believe that people have a much more complex set of motivations for action .	S-35
OWN	In particular , much of one 's behavior arises from a sense of obligation to behave within limits set by the society that the agent is part of .	S-36
OWN	A model based on obligations differs from an intention-based approach in that obligations are independent of shared plans and intention recognition .	S-37
OWN	Rather , obligations are the result of rules by which an agent lives .	S-38
OWN	Social interactions are enabled by their being a sufficient compatibility between the rules affecting the interacting agents .	S-39
OWN	One responds to a question because this is a social behavior that is strongly encouraged as one grows up , and becomes instilled in the agent .	S-40
OWN	The model we propose is that an agent 's behavior is determined by a number of factors , including that agent 's current goals in the domain , and a set of obligations that are induced by a set of social conventions .	S-41
OWN	When planning , an agent considers both its goals and obligations in order to determine an action that addresses both to the extent possible .	S-42
OWN	When prior intentions and obligations conflict , an agent generally will delay pursuit of its intentions in order to satisfy the obligations , although the agent may behave otherwise at the cost of violating its obligations .	S-43
OWN	At any given time , an agent may have many obligations and many different goals , and planning involves a complex tradeoff between these different factors .	S-44
OWN	Returning to the example about questions , when an agent is asked a question , this creates an obligation to respond .	S-45
OWN	The agent does not have to adopt the goal of answering the question as one of her personal goals in order to explain the behavior .	S-46
OWN	Rather it is a constraint on the actions that the agent may plan to do .	S-47
OWN	In fact , the agent might have an explicit goal not to answer the question , yet still is obliged to offer a response ( e.g. , consider most politicians at press conferences ) .	S-48
OWN	The planning task then is to satisfy the obligation of responding to the question , without revealing the answer if at all possible .	S-49
OWN	In cases where the agent does not know the answer , the obligation to respond may be discharged by some explicit statement of her inability to give the answer .	S-50
OWN	Obligations represent what an agent should do , according to some set of norms .	S-51
OWN	The notion of obligation has been studied for many centuries , and its formal aspects are examined using Deontic Logic .	S-52
OWN	Our needs are fairly simple , and do not require an extensive survey of the complexities that arise in that literature .	S-53
OWN	Still , the intuitions underlying that work will help to clarify what an obligation is .	S-54
OWN	Generally , obligation is defined in terms of a modal operator often called permissible .	S-55
OWN	An action is obligatory if it is not permissible not to do it .	S-56
OWN	An action is forbidden if it is not permissible .	S-57
OWN	An informal semantics of the operator can be given by positing a set of rules of behavior R .	S-58
OWN	An action is obligatory if its occurrence logically follows from R , and forbidden if its non-occurrence logically follows from R .	S-59
OWN	An action that might occur or not-occur according to R is neither obligatory nor forbidden .	S-60
OWN	Just because an action is obligatory with respect to a set of rules R does not mean that the agent will perform the action .	S-61
CTR	So we do not adopt the model suggested byin which agents ' behavior cannot violate the defined social laws .	S-62
OWN	If an obligation is not satisfied , then this means that one of the rules must have been broken .	S-63
OWN	We assume that agents generally plan their actions to violate as few rules as possible , and so obligated actions will usually occur .	S-64
OWN	But when they directly conflict with the agent 's personal goals , the agent may choose to violate them .	S-65
OWN	Obligations are quite different from and can not be reduced to intentions and goals .	S-66
OWN	In particular , an agent may be obliged to do an action that is contrary to his goals ( for example , consider a child who has to apologize for hitting her younger brother ) .	S-67
OWN	Obligations also cannot be reduced to simple expectations , although obligations may act as a source of expectations .	S-68
OWN	Expectations can be used to guide the action interpretation and plan-recognition processes ( as proposed by) , but expectations do not in and of themselves provide a sufficient motivation for an agent to perform the expected action - in many cases there is nothing wrong with doing the unexpected or not performing an expected action .	S-69
OWN	The interpretation of an utterance will often be clear even without coherence with prior expectations .	S-70
OWN	We need to allow for the possibility that an agent has performed an action even when this violates expectations .	S-71
OWN	If an agent actually violates obligations as well then the agent can be held accountable .	S-72
OWN	Specific obligations arise from a variety of sources .	S-73
OWN	In a conversational setting , an accepted offer or a promise will incur an obligation .	S-74
OWN	Also , a command or request by the other party will bring about an obligation to perform the requested action .	S-75
OWN	If the obligation is to say something then we call this a discourse obligation .	S-76
OWN	Our model of obligation is very simple .	S-77
OWN	We use a set of rules that encode discourse conventions .	S-78
OWN	Whenever a new conversation act is determined to have been performed , then any future action that can be inferred from the conventional rules becomes an obligation .	S-79
OWN	We use a simple forward chaining technique to introduce obligations .	S-80
OWN	Some obligation rules based on the performance of conversation acts are summarized in Table.	S-81
OWN	When an agent performs a promise to perform an action , or performs an acceptance of a suggestion or request by another agent to perform an action , the agent obliges itself to achieve the action in question .	S-82
OWN	When another agent requests that some action be performed , the request itself brings an obligation to address the request : that is , either to accept it or to reject it ( and make the decision known to the requester ) - the requestee is not permitted to ignore the request .	S-83
OWN	A question establishes an obligation to answer the question .	S-84
OWN	If an utterance has not been understood , or is believed to be deficient in some way , this brings about an obligation to repair the utterance .	S-85
OWN	Obligations ( or at least beliefs that the agent has obligations ) will thus form an important part of the reasoning process of a deliberative agent , e.g. , the architecture proposed by.	S-86
OWN	In addition to considering beliefs about the world , which will govern the possibility of performing actions and likelyhood of success , and desires or goals which will govern the utility or desirability of actions , a social agent will also have to consider obligations , which govern the permissibility of actions .	S-87
OWN	There are a large number of strategies which may be used to incorporate obligations into the deliberative process , based on how much weight they are given compared to the agents goals .	S-88
OTH	present several strategies of moving from obligations to actions , including : automatically performing an obligated action , adopting all obligations as goals , or adopting an obligated action as a goal only when performing the action results in a state desired by the agent .	S-89
OTH	In the latter cases , these goals still might conflict with other goals of the agent , and so are not guaranteed to be performed .	S-90
OWN	In general , we will want to allow action based on obligations to supersede performance of intended actions .	S-91
OWN	For instance , consider an agent with an intention to do something as soon as possible .	S-92
OWN	If an obligation is imposed , it will still be possible to perform the intended action , but a well-behaved agent might need to delay performance until the obligation is dealt with .	S-93
OWN	For example , if the intention is to perform a series of inform acts , and then a listener requests repair of one , a well-behaved agent will repair that inform before proceeding to initiate the next intended one .	S-94
AIM	We have built a system that explicitly uses discourse obligations and communicative intentions to partake in natural dialogue .	S-95
BAS	This system plays the role of the dialogue manager in the TRAINS dialogue system , which acts as an intelligent planning assistant in a transportation domain .	S-96
OTH	While this is a domain where the assumption of co-operation is generally valid , the obligation model still provides for a much simpler analysis of the discourse behavior than a strongly plan-based account .	S-97
OTH	An example of a dialogue that the TRAINS system can engage in is shown in Figure.	S-98
TXT	Below we describe parts of the discourse model in more detail and then show how it is used to account for aspects of this dialogue .	S-99
OTH	The TRAINS Systemis a large integrated natural language conversation and plan reasoning system .	S-100
OTH	We concentrate here , however , on just one part of that system , the discourse actor which drives the actions of the dialogue manager module .	S-101
OTH	Figureillustrates the system from the viewpoint of the dialogue manager .	S-102
OTH	The dialogue manager is responsible for maintaining the flow of conversation and making sure that the conversational goals are met .	S-103
OTH	For this system , the main goals are that an executable plan which meets the user 's goals is constructed and agreed upon by both the system and the user and then that the plan is executed .	S-104
OTH	The dialogue manager must keep track of the current state of the dialogue , determine the effects of observed conversation acts , generate utterances back , and send commands to the domain plan reasoner and domain plan executor when appropriate .	S-105
OTH	Conversational action is represented using the theory of Conversation Actswhich augments traditional Core Speech Acts with levels of acts for turn-taking , grounding, and argumentation .	S-106
OTH	Each utterance will generally contain acts ( or partial acts ) at each of these levels .	S-107
OTH	As well as representing general obligations within the temporal logic used to represent general knowledge , the system also maintains two stacks ( one for each conversant ) of pending discourse obligations .	S-108
OTH	Each obligation on the stack is represented as an obligation type paired with a content .	S-109
OTH	The stack structure is appropriate because , in general , one must respond to the most recently imposed obligation first .	S-110
OTH	As explained in Section, the system will attend to obligations before considering other parts of the discourse context .	S-111
OTH	Most obligations will result in the formation of intentions to communicate something back to the user .	S-112
OTH	When the intentions are formed , the obligations are removed from the stack , although they have not yet actually been met .	S-113
OTH	If , for some reason , the system dropped the intention without satisfying it and the obligation were still current , the system would place them back on the stack .	S-114
OTH	The over-riding goal for the TRAINS domain is to construct and execute a plan that is shared between the two participants .	S-115
OTH	This leads to other goals such as accepting proposals that the other agent has suggested , performing domain plan synthesis , proposing to the other agent plans that the domain plan reasoner has constructed , or executing a completed plan .	S-116
OWN	In designing an agent to control the behavior of the dialogue manager , we choose a reactive approach in which the system will not deliberate and add new intentions until after it has performed the actions which are already intended .	S-117
OWN	As shown above , though , new obligations will need to be addressed before performing intended actions .	S-118
OWN	The agent 's deliberative behavior could thus be characterized in an abstract sense as :	S-119
OWN	When deciding what to do next , the agent first considers obligations and decides how to update the intentional structure ( add new goals or intentions ) based on these obligations .	S-120
OWN	Obligations might also lead directly to immediate action .	S-121
OWN	If there are no obligations , then the agent will consider its intentions and perform any actions which it can to satisfy these intentions .	S-122
OWN	If there are no performable intentions , then the system will deliberate on its overall goals and perhaps adopt some new intentions ( which can then be performed on the next iteration ) .	S-123
OWN	For the discourse actor , special consideration must be given to the extra constraints that participation in a conversation imposes .	S-124
OWN	This includes some weak general obligations ( such as acknowledging utterances by others and not interrupting ) as well as some extra goals coming from the domain setting to maintain a shared view of the world and the domain plans which are to be executed .	S-125
OWN	We prioritize the sources for the deliberations of the actor as follows :	S-126
OWN	Discourse Obligations from Table	S-127
OWN	Weak Obligation : Don't interrupt user 's turn	S-128
OWN	Intended Speech Acts	S-129
OWN	Weak Obl : Grounding ( coordinate mutual beliefs )	S-130
OWN	Discourse Goals : Domain Plan Negotiation	S-131
OWN	High-level Discourse Goals	S-132
OWN	The implemented actor serializes consideration of these sources into the algorithm in Figure.	S-133
OWN	The updating of the conversational state due to perceived conversation acts or actions of other modules of the system progresses asynchronously with the operation of the discourse actor .	S-134
OWN	Whenever the discourse actor is active , it will first decide on which task to attempt , according to the priorities given in Figure, and then work on that task .	S-135
OWN	After completing a particular task , it will then run through the loop again , searching for the next task , although by then the context may have changed due to , e.g. , the observance of a new utterance from the user .	S-136
OWN	The actor is always running and decides at each iteration whether to speak or not ( according to turn-taking conventions ) ; the system does not need to wait until a user utterance is observed to invoke the actor , and need not respond to user utterances in an utterance by utterance fashion .	S-137
OWN	Lines 2 - 3 of the algorithm in Figureindicate that the actor 's first priority is fulfilling obligations .	S-138
OWN	If there are any , then the actor will do what it thinks best to meet those obligations .	S-139
OWN	If there is an obligation to address a request , the actor will evaluate whether the request is reasonable , and if so , accept it , otherwise reject it , or , if it does not have sufficient information to decide , attempt to clarify the parameters .	S-140
OWN	In any case , part of meeting the obligation will be to form an intention to tell the user of the decision ( e.g. , the acceptance , rejection , or clarification ) .	S-141
OWN	When this intention is acted upon and the utterance produced , the obligation will be discharged .	S-142
OWN	Other obligation types are to repair an uninterpretable utterance or one in which the presuppositions are violated , or to answer a question .	S-143
OWN	In question answering , the actor will query its beliefs and will answer depending on the result , which might be that the system does not know the answer .	S-144
OWN	In most cases , the actor will merely form the intention to produce the appropriate utterance , waiting for a chance , according to turn-taking conventions , to actually generate the utterance .	S-145
OWN	In certain cases , though , such as a repair , the system will actually try to take control of the turn and produce an utterance immediately .	S-146
OWN	For motivations other than obligations , the system adopts a fairly `` relaxed '' conversational style ; it does not try to take the turn until given it by the user unless the user pauses long enough that the conversation starts to lag ( lines 14 - 17 ) .	S-147
OWN	When the system does not have the turn , the conversational state will still be updated , but the actor will not try to deliberate or act .	S-148
OWN	When the system does have the turn , the actor first ( after checking obligations ) examines its intended conversation acts .	S-149
OWN	If there are any , it calls the generator to produce an utterance ( lines 5 - 6 of the discourse actor algorithm ) .	S-150
OWN	Whatever utterances are produced are then reinterpreted ( as indicated in Figure) and the conversational state updated accordingly .	S-151
OWN	This might , of course , end up in releasing the turn .	S-152
OWN	It might not be convenient to generate all the intended acts in one utterance , in which case there will remain some intended acts left for future utterances to take care of ( unless the subsequent situation merits dropping those intentions ) .	S-153
OWN	Only intended speech acts that are part of the same argumentation acts as those which are uttered will be kept as intentions - others will revert back to whatever caused the intention to be formed , although subsequent deliberation might cause the intentions to be re-adopted .	S-154
OWN	If there are no intended conversation acts , the next thing the actor considers is the grounding situation ( lines 7 - 8 ) .	S-155
OWN	The actor will try to make it mutually believed ( or grounded ) whether particular speech acts have been performed .	S-156
OWN	This will involve acknowledging or repairing user utterances , as well as repairing and requesting acknowledgement of the system 's own utterances .	S-157
OWN	Generally , grounding is considered less urgent than acting based on communicative intentions , although some grounding acts will be performed on the basis of obligations which arise while interpreting prior utterances .	S-158
OWN	If all accessible utterances are grounded , the actor then considers the negotiation of domain beliefs and intentions ( lines 9 - 10 ) .	S-159
OWN	The actor will try to work towards a shared domain plan , adding intentions to perform the appropriate speech acts to work towards this goal .	S-160
OWN	This includes accepting , rejecting , or requesting retraction of user proposals , requesting acceptance of or retracting system proposals , and initiating new system proposals or counterproposals .	S-161
OWN	The actor will first look for User proposals which are not shared .	S-162
OWN	If any of these are found , it will add an intention to accept the proposal , unless the proposal is deficient in some way ( e.g. , it will not help towards the goal or the system has already come up with a better alternative ) .	S-163
OWN	In this latter case , the system will reject the user 's proposal and present or argue for its own proposal .	S-164
OWN	Next , the actor will look to see if any of its own proposals have not been accepted , requesting the user to accept them if they have been simply acknowledged , or retracting or reformulating them if they have already been rejected .	S-165
OWN	Finally , the actor will check its private plans for any parts of the plan which have not yet been proposed .	S-166
OWN	If it finds any here , it will adopt an intention to make a suggestion to the user .	S-167
OWN	If none of the more local conversational structure constraints described above require attention , then the actor will concern itself with its actual high-level goals .	S-168
OWN	For the TRAINS system , this will include making calls to the domain plan reasoner and domain executor , which will often return material to update the system 's private view of the plan and initiate its own new proposals .	S-169
OWN	It is also at this point that the actor will take control of the conversation , pursuing its own objectives rather than responding to those of the user .	S-170
OWN	Finally , if the system has no unmet goals that it can work towards achieving ( line 13 ) , it will hand the turn back to the user or try to end the conversation if it believes the user 's goals have been met as well .	S-171
OWN	The functioning of the actor can be illustrated by its behavior in the dialogue in Figure.	S-172
OWN	While the discussion here is informal and skips some details , the dialogue is actually processed in this manner by the implemented system .	S-173
BAS	More detail both on the dialogue manager and its operation on this example can be found in.	S-174
OWN	Utteranceis interpreted as performing two Core Speech Acts .	S-175
OWN	It is interpreted ( literally ) as the initiation of an inform about an obligation to perform a domain action ( shipping the oranges ) .	S-176
OWN	This utterance is also seen as ( the initiation of ) an ( indirect ) suggestion that this action be the goal of a shared domain plan to achieve the performance of the action .	S-177
OWN	In addition , this utterance releases the turn to the system .	S-178
OWN	Figureshows the relevant parts of the discourse state after interpretation of this utterance .	S-179
OWN	After interpreting utterance, the system first decides to acknowledge this utterance ( lines 7 - 8 in the actor algorithm ) - moving the suggestion from an unacknowledged to unaccepted - and then to accept the proposal ( lines 9 - 10 ) .	S-180
OWN	Finally , the system acts on the intentions produced by these deliberations ( lines 5 - 6 ) and produces the combined acknowledgement / acceptance of utterance.	S-181
OWN	This acceptance makes the goal shared and also satisfies the first of the discourse goals , that of getting the domain goal to work on .	S-182
OWN	Utterancesandare interpreted , but not responded to yet since the user keeps the turn ( in this case by following up with subsequent utterances before the system has a chance to act ) .	S-183
OWN	Utteranceinvokes a discourse obligation on the system to respond to the User 's assertion in 3 - 7 and also gives the turn to the system .	S-184
OWN	The resulting discourse context ( after the system decides to acknowledge ) is shown in Figure.	S-185
OWN	The system queries its domain knowledge base and decides that the user is correct here ( there are , indeed , oranges at Corning ) , and so decides to meet this obligation ( lines 2 - 3 ) by answering in the affirmative .	S-186
OWN	This results in forming an intention to inform , which is then realized ( along with the acknowledgement of the utterances ) by the production of utterance.	S-187
OWN	Similar considerations hold for the system responsesand.	S-188
OWN	The reasoning leading up to utteranceis similar to that leading to utterance.	S-189
OWN	Here the user is suggesting domain actions to help lead to the goal , and the system , when it gets the turn , acknowledges and accepts this suggestion .	S-190
OWN	Utterances,, andare interpreted as requests because of the imperative surface structure .	S-191
OWN	The discourse obligation to address the request is incurred only when the system decides to acknowledge the utterances and ground them .	S-192
OWN	After the decision to acknowledge , the obligations are incurred , and the system then addresses the requests , deciding to accept them all , and adding intentions to perform an accept speech act , which is then produced as 16 .	S-193
OWN	Utteranceis interpreted as a request for evaluation of the plan .	S-194
OWN	When the system decided to acknowledge , this creates a discourse obligation to address the request .	S-195
OWN	The system considers this ( invoking the domain plan reasoner to search the plan for problems or incomplete parts ) and decides that the plan will work , and so decides to perform the requested action - an evaluation speech act .	S-196
OWN	This is then generated as.	S-197
OWN	The discourse state after the decision to acknowledge is shown in Figure.	S-198
OWN	After the user 's assent , the system then checks its goals , and , having already come up with a suitable plan , executes this plan in the domain by sending the completed plan to the domain plan executor .	S-199
OWN	This example illustrates only a small fraction of the capabilities of the dialogue model .	S-200
OWN	In this dialogue , the system needed only to follow the initiative of the user .	S-201
OWN	However this architecture can handle varying degrees of initiative , while remaining responsive .	S-202
OWN	The default behavior is to allow the user to maintain the initiative through the plan construction phase of the dialogue .	S-203
OWN	If the user stops and asks for help , or even just gives up the initiative rather than continuing with further suggestions , the system will switch from plan recognition to plan elaboration and incrementally devise a plan to satisfy the goal ( although this plan would probably not be quite the same as the plan constructed in this dialogue ) .	S-204
OWN	We can illustrate the system behaving more on the basis of goals than obligations with a modification of the previous example .	S-205
OWN	Here , the user releases the turn back to the system after utterance, and the deliberation proceeds as follows : the system has no obligations , no communicative intentions , nothing is ungrounded , and there are no unaccepted proposals , so the system starts on its high-level goals .	S-206
OWN	Given its goal to form a shared plan , and the fact that the current plan ( consisting of the single abstract move-commodity action ) is not executable , the actor will call the domain plan reasoner to elaborate the plan .	S-207
OWN	This will return a list of augmentations to the plan which can be safely assumed ( including a move-engine event which generates the move-commodity , given the conditions that the oranges are in a boxcar which is attached to the engine ) , as well as some choice point where one of several possibilities could be added ( e.g. , a choice of the particular engine or boxcar to use ) .	S-208
OWN	Assuming that the user still has not taken the turn back , the system can now propose these new items to the user .	S-209
OWN	The choice could be resolved in any of several ways : the domain executor could be queried for a preference based on prior experience , or the system could put the matter up to the user in the form of an alternative question , or it could make an arbitrary choice and just suggest one to the user .	S-210
OWN	The user will now be expected to acknowledge and react to these proposals .	S-211
OWN	If the system does not get an acknowledgement , it will request acknowledgement the next time it considers the grounding situation .	S-212
OWN	If the proposal is not accepted or rejected , the system can request an acceptance .	S-213
OWN	If a proposal is rejected , the system can negotiate and offer a counterproposal or accept a counter proposal from the user .	S-214
OWN	Since the domain plan reasonerperforms both plan recognition and plan elaboration in an incremental fashion , proposals from system and user can be integrated naturally in a mixed-initiative fashion .	S-215
OWN	The termination condition will be a shared executable plan which achieves the goal , and each next action in the collaborative planning process will be based on local considerations .	S-216
OWN	We have argued that obligations play an important role in accounting for the interactions in dialog .	S-217
OWN	Obligations do not replace the plan-based model , but augment it .	S-218
OWN	The resulting model more readily accounts for discourse behavior in adversarial situations and other situations where it is implausible that the agents adopt each others goals .	S-219
OWN	The obligations encode learned social norms , and guide each agent 's behavior without the need for intention recognition or the use of shared plans at the discourse level .	S-220
OWN	While such complex intention recognition may be required in some complex interactions , it is not needed to handle the typical interactions of everyday discourse .	S-221
OWN	Furthermore , there is no requirement for mutually-agreed upon rules that create obligations .	S-222
OWN	Clearly , the more two agents agree on the rules , the smoother the interaction becomes , and some rules are clearly virtually universal .	S-223
OWN	But each agent has its own set of individual rules , and we do not need to appeal to shared knowledge to account for local discourse behavior .	S-224
CTR	We have also argued that an architecture that uses obligations provides a much simpler implementation than the strong plan-based approaches .	S-225
OWN	In particular , much of local discourse behavior can arise in a `` reactive manner '' without the need for complex planning .	S-226
OWN	The other side of the coin , however , is a new set of problems that arise in planning actions that satisfy the multiple constraints that arise from the agent 's personal goals and perceived obligations .	S-227
OWN	The model presented here allows naturally for a mixed-initiative conversation and varying levels of co-operativity .	S-228
OWN	Following the initiative of the other can be seen as an obligation driven process , while leading the conversation will be goal driven .	S-229
OWN	Representing both obligations and goals explicitly allows the system to naturally shift from one mode to the other .	S-230
OWN	In a strongly co-operative domain , such as TRAINS , the system can subordinate working on its own goals to locally working on concerns of the user , without necessarily having to have any shared discourse plan .	S-231
OWN	In less co-operative situations , the same architecture will allow a system to still adhere to the conversational conventions , but respond in different ways , perhaps rejecting proposals and refusing to answer questions .	S-232
