AIM	A German language model for the Xerox HMM tagger is presented .	A-0
CTR	This model 's performance is compared with two other German taggers with partial parameter re-estimation and full adaption of parameters from pre-tagged corpora .	A-1
OWN	The ambiguity types resolved by this model are analysed and compared to ambiguity types of English and French .	A-2
OWN	Finally , the model 's error types are described .	A-3
OWN	I argue that although the overall performance of these models for German is comparable to results for English and French , a more exact analysis demonstrates important differences in the types of disambiguation involved for German .	A-4
BKG	Since the late ' 80 s part-of-speech ( POS ) disambiguation using Hidden Markov Models ( HMM ) has been a widespread method for tagging texts .	S-0
OTH	Despite this fact , little work has been done so far toward employing this technology for the disambiguation of German texts,.	S-1
OTH	Earlier work of the authorandwithin the framework of a project on corpus based development of lexical knowledge bases ( ELWIS ) has produced LIKELY , a straightforward implementation of the Viterbi algorithm employing an HMM whose parameters were obtained from a pre-tagged text corpus .	S-2
OTH	Since then the original tag set was redefined , making the tagged corpus used to train the LIKELY tagger obsolete .	S-3
AIM	Within a current project on adapting bilingual dictionaries for online comprehension assistance ( COMPASS , LRE 62 - 080 ) , the need arose for a POS-disambiguator to facilitate a context sensitive dictionary look-up system .	S-4
BAS	As the COMPASS project makes ample use of Xerox technology for its core look-up engine and for POS disambiguation for languages other than German , the obvious thing to do was to develop a German language model for the Xerox tagger .	S-5
TXT	The following section describes the implementation of this new model .	S-6
TXT	In sectionthe results obtained using the new model are compared with the results from previous models .	S-7
TXT	An analysis of the types of disambiguation involved in these models is presented in section.	S-8
TXT	The model 's error types are analysed in section, and conclusions are drawn in section.	S-9
BAS	The version of the Xerox tagger used for the implementation described here is the DDS Tagger version 1.1.	S-10
OTH	This version differs from the current version ( 1.2 ) of the Xerox Tagger as described inin that the DDS Tagger accommodates lexicons and class guessers in the form of external finite-state transducers .	S-11
OWN	Implementing a new language model for this tagger involves supplying :	S-12
OWN	a definition of the tag set to be used by the HMM ,	S-13
OWN	a lexicon listing word forms with their equivalence classes , that is the list of POS tags that can be assigned to the word form ,	S-14
OWN	a class guesser that assigns equivalence classes to words not covered by the lexicon ,	S-15
OWN	a set of initial transition biases ,	S-16
OWN	a set of initial symbol biases ,	S-17
OWN	a sufficiently large text for training the HMM ,	S-18
OWN	a reference text with correctly assigned POS tags ,	S-19
OWN	a tokenizer that recognizes words in free text .	S-20
TXT	The following paragraphs describe these components in more detail .	S-21
OWN	The tag set used in the implementation is the smaller version of the two tag sets developed jointly by the Universities of Stuttgart and Tbingen , referred to as the ELWIS tag set,.	S-22
OWN	It consists of a total of 42 POS tags plus three tags for punctuation and a special tag for truncated words .	S-23
OWN	The tags used in the ELWIS tag set are given in table.	S-24
OWN	Detailed guidelines on the use of the individual tags are available in.	S-25
OWN	For each word form the lexicon gives the set of POS tags that can be assigned to that word .	S-26
OWN	This set may consist of one ( for unambiguous words ) or more tags and is referred to as the word 's equivalence class .	S-27
OWN	The lexicon used in this implementation was derived from Lingsoft 's GERTWOL morphological analyzer , which uses finite state transducers , by mapping the morpho-syntactic labels generated by GERTWOL onto the ELWIS tag set .	S-28
OWN	The lexicon is realized as a finite state transducer for the DDS Tagger .	S-29
OWN	Alternative mapping rules were developed to generate a lexicon with ELWIS tags from the German lexical database of the Centre for Lexical Information.	S-30
OWN	Class guessers for the Xerox tagger assign potential POS tags to unknown words according to a surface analysis of the word form .	S-31
OWN	In addition to the common practice of mapping POS tags according to the words ' suffixes , this implementation makes use of the case of the initial letter of a word -- which is highly significant for POS assignment in German .	S-32
OWN	The class guesser also takes care of POS-assignment for abbreviations , special symbol sequences and language external material in the text .	S-33
OWN	The class guesser , like the lexicon , is a finite state transducer .	S-34
OWN	The model is trained using a set of initial transition biases , including both positive and negative constraints on tag sequences .	S-35
OWN	Although the model can be trained without initial biases , the performance of the resulting model increases significantly if appropriate initial biases are used .	S-36
OWN	The biases in the model consisted for the most part in specifications of the most plausible successor tags for each tag in the tag set .	S-37
OWN	They were constructed manually and refined in a series of subsequent training and evaluation runs .	S-38
OWN	Initial symbol biases are an additional set of biases used to define preferences for tag assignment given a particular equivalence class .	S-39
OWN	Only a very few symbol biases were defined before evaluation of the first training runs , mainly to reflect biases towards equivalence classes used in the class guesser .	S-40
OWN	The majority of symbol biases were added to correct misguided biases chosen during the training processes .	S-41
OWN	The two texts used for training the HMM were selected from the German data contained on the's Multilingual CD-ROM: a 200,000 and 2,000,000 word sample from Summer 1992 issues of the German newspaper Frankfurter Rundschau .	S-42
OWN	The reference texts were also taken from the Frankfurter Rundschau , but do not overlap with the training texts .	S-43
OWN	The reference texts amount to a total of approximately 20,000 running words , which were manually tagged and checked .	S-44
OWN	The current version of the implementation uses a straightforward tokenizer accepting one line per token .	S-45
OWN	Training texts are pretokenized using an external tokenizer written in lex .	S-46
OWN	The best results of this implementation were obtained running 20 iterations of training over the 200,000 word training text , using a total of 50 transition and 17 symbol biases .	S-47
OWN	With this configuration of training parameters , the resulting HMM assignedincorrect tags when run on the reference texts and compared with the manually assigned tags .	S-48
OTH	The main advantage of the Xerox tagger when compared with earlier implementations of HMM taggers is that it can be trained using untagged text .	S-49
CTR	However , the performance of the resulting HMM is very poor if no initial biases are used to help the training process find suitable parameters .	S-50
OTH	For comparison , the evaluation procedure used to evaluate the implementation of the HMM tagger described in the preceding section was repeated without using any of the initial biases .	S-51
OTH	The result was a poor performance of the resulting HMM with an error rate of.	S-52
CTR	Choosing initial biases to help train a model is a subtle task in that it not only requires sound knowledge of the tag set used and the target language the model is aiming at , but it also requires a `` feel '' for how the initial biases may be modified during a given number of training iterations .	S-53
CTR	It is also sometimes frustrating that the linguistic knowledge used to create the initial biases gets `` optimized '' or `` trained '' away in subsequent iterations of training .	S-54
OTH	To overcome these disadvantages , hybrid technologies have been developed that combine free text training methods with parameter estimation from pre-tagged texts .	S-55
OTH	In such a setting , initial transition and symbol biases are replaced by frequencies of tag sequences and tag instantiation from a relatively small pre-tagged corpus .	S-56
OTH	The counted frequencies are taken as an approximation to the model 's probabilities and get smoothed in a small number of training iterations .	S-57
BAS	In order to see what could be gained for a German language model by such a hybrid technology , I used extensions to the Xerox tagger developed at the University of Stuttgart that facilitate initialization of an HMM with values obtained from a pre-tagged corpus.	S-58
OWN	Initial parameters were obtained by counting transition and symbol frequencies in a manually tagged 24,000 word corpus taken from the newspaper Stuttgarter Zeitung , that was kindly made available by the University of Stuttgart .	S-59
OWN	These initial parameters were adjusted in a single training iteration using Xerox 's Baum-Welch implementation for parameter re-estimation .	S-60
OWN	The texts used for training and as a reference for evaluation were the same as the ones used in the implementation described in section.	S-61
OWN	The resulting HMM had an error rate of 3.14 % .	S-62
OWN	The superior performance of this model confirms results presented by,, andfor English : empirically obtained initial values for transition and output probabilities with a small number of training iterations lead to significantly better results than intuitively generated biases do .	S-63
OTH	On the other end of the scale of parameter production for HMM POS disambiguators is the extraction of parameters exclusively on the basis of ( larger ) pre-tagged text corpora , with no Baum-Welch re-estimation involved .	S-64
OTH	Such an implementation is described in earlier work of the author,.	S-65
OTH	For this model error rates of( depending on the coverage of the underlying lexicon ) were reported .	S-66
CTR	These results , however , are not directly comparable to the implementations described in this paper , since the tag sets employed differ to some extent .	S-67
OWN	In the preceding sections the evaluation of the model was purely quantitative .	S-68
OWN	Performance was measured as the percent of mismatches between the output generated by the HMM and the tags assigned by manual tagging .	S-69
OWN	Although this error rate is an appropriate measure for the performance of an HMM given a particular reference text , it says little about the amount of disambiguation done by the tagger , and nothing about the ambiguity types that were involved in the disambiguation process .	S-70
OWN	The difficulty of disambiguation can be quantified by the ambiguity rate : the number of possible tag assignments divided by the number of words in a given text .	S-71
OWN	The test text used to evaluate the German model described in sectionhas an ambiguity rate of 1.51 , this is , the lexicon provides an average of 1.51 tags for each word in the text .	S-72
OWN	In an effort to compare the German model with what is reported for other languages , English and French tagged texts were analysed .	S-73
OWN	Both texts contained approximately 10,000 words and were tagged using an English resp .	S-74
OWN	French language model for the Xerox tagger .	S-75
OWN	The tag set used to annotate the English text is a slightly modified version of the Brown tag set , consisting of a total of 72 tags .	S-76
OWN	The ambiguity rate for this text is 1.41 .	S-77
OWN	For the French text the tag set described inwith 37 different tags is used .	S-78
OWN	Here the ambiguity rate is 1.52 .	S-79
OWN	In terms of ambiguity rates , the English , French , and German texts are thus quite comparable .	S-80
OWN	In order to compare the types of ambiguities that had to be resolved by the different language models for the HMM tagger , relative frequency tables of equivalence classes were computed for each of the three texts .	S-81
OWN	The top most frequent equivalence classes for the three languages are listed in tabletogether with their relative frequencies .	S-82
OWN	If the table is viewed in terms of the major word classes of noun , verb , adjective , adverb , and closed-class forms , the following predominant ambiguity classes for English can be distinguished :	S-83
OWN	noun vs. verb ( share , offer , plan ) ,	S-84
OWN	adjective vs. noun ( public , million , high ) ,	S-85
OWN	closed-class vs. noun ( a ) ,	S-86
OWN	adjective vs. noun vs. verb ( return , field ) ,	S-87
OWN	closed-class vs. adverb ( by , about , below ) ,	S-88
OWN	and closed-class vs. adjective vs. adverb ( round , next ) ,	S-89
OWN	For French the major ambiguity types are :	S-90
OWN	noun vs. verb ( affaire , bout , place ) ,	S-91
OWN	adjective vs. noun ( demi , moyen , responsable ,	S-92
OWN	adjective vs. verb ( appliqu , devenu , fabriqu ) ,	S-93
OWN	closed-class vs. adjective ( numeral un ) .	S-94
OWN	The elements of the most frequent ambiguity types for German , however , belong to the same major word classes , with only a few exception such as :	S-95
OWN	closed-class vs. adjective ( numeral einen , einer ) ,	S-96
OWN	and verb vs. adjective ( fehlgeschlagen , bekannt ) ,	S-97
OWN	with the latter reflecting a subtle distinction in the German tag set ( VPP vs. ADJD : participle as modifier vs. non-attributive adjectives ) .	S-98
OWN	The comparison of the most frequent ambiguity types shows a significant difference between the German model on the one hand and the English and French models on the other .	S-99
OWN	In German most of the effort is going into subclassification within major word classes , while in English and French a good deal of disambiguation work is devoted to separate major word classes .	S-100
OWN	The differences in ambiguity types of the models also have effects on the types of errors produced by the German model .	S-101
OWN	Again , errors mainly affect the assignment of words to subclasses within one major word class .	S-102
OWN	Tableshows the most common errors produced by the German model .	S-103
OWN	The entries are sorted by decreasing frequencies relative to the total number of mismatches between the manually and automatically tagged texts .	S-104
OWN	The first column gives the relative frequenciy and the second column lists the tag chosen by the German HMM tagger .	S-105
OWN	In the second column , the number following the slash indicates the number of elements in the equivalence class from which the model had to choose .	S-106
OWN	A missing number indicates that there was only one choice in the lexicon .	S-107
OWN	The third column show the `` correct '' tag , as chosen by the human tagger .	S-108
OWN	The most common ( accumulated ) error is the confusion of proper nouns and common nouns -- a result of the fact that both proper nouns and common nouns are both capitalized in German .	S-109
OWN	The fourth line of tablerepresents a special case of this error for which the HMM model can not be held responsible : no ambiguity was indicated in the lexicon , so the model had nothing to choose from .	S-110
OWN	This occurs most frequently when common nouns are used as proper nouns ( e.g. in die gehobene Mittelklasse plaziert Renault ab 6. Mrz den Safrane ) , where one would not expect to add a tag `` proper noun '' for every noun in the lexicon .	S-111
OWN	The second most frequent error type involves confusion of infinitives and 1 st and 3rd pers. pl. finite present tense forms .	S-112
OWN	These are homographs in German that are notoriously hard to disambiguate within a narrow context .	S-113
OWN	The difficulty of distinguishing between non-attributive , adjectival usage of participles ( i.e. er ist geladen ) and participles proper ( i.e. er hat den Wagen geladen ) was mentioned in the preceding section .	S-114
OWN	In addition a number of these forms may also be used as finite verbs ( i.e. erhalten , gehrt ) , and this is a further source of errors .	S-115
OWN	Almost all of the remaining errors are misassignments within closed classes , including well-known errors due to long distance phenomena , such as those resulting from the confusion of relative pronouns , demonstrative pronouns and articles in sentences like : die einmal fr die Buchproduktion erfaten Texte or : doch der wollte nicht , das falle auf .	S-116
OWN	Despite the hypothesis that the free word order of German leads to poor performance of low order HMM taggers when compared with a language like English , the overall results for German are very much along the lines of comparable implementations for English , if not better .	S-117
OWN	It can be argued that the disadvantage of free word order for HMM taggers is compensated for by richer morphology and the additional disambiguation cue of having upper and lower case initial letters to distinguish POS membership .	S-118
OWN	The latter , however , greatly hinders the recognition of proper nouns , the most common type of error , responsible for approximatelyof the model 'smistakes .	S-119
OWN	It is important to notice that the types of disambiguation carried out by the tagger for German are significantly different from the disambiguation work for English and French .	S-120
OWN	While in English and French a fair number of disambiguations involve separating major POS classes such as verb , noun , and adjective , most of the work performed in the German model involves disambiguation between subclasses of one main category , such as finite vs. infinitive verb , noun vs. proper noun , different sub-categories of pronouns , etc .	S-121
OWN	This finding has consequences for the COMPASS project , where POS disambiguation is employed as one means of disambiguating word senses to facilitate precise dictionary look-up .	S-122
OWN	While this technique helps to confine word senses for English and French , it is of little help for word sense disambiguation in German .	S-123
OWN	However , the German model was useful for the project because a tagged reference corpus was required for lexicographic work in order to adapt existing bilingual dictionaries .	S-124
OWN	The tagger was used to annotate all of the 50 million word German corpora contained on the ECI Multilingual Corpus 1 CD-ROM .	S-125
OWN	I would like to thank Helmut Schmid of the University of Stuttgart for providing extensions of parameter initialization for the Xerox Tagger and Jean-Pierre Chanod and Lauri Kartunnen of the Rank Xerox Research Laboratory Grenoble for making available the French and English tagged texts and lexicons .	S-126
OWN	I would also like to acknowledge valuable advice from Tracy Holloway King and Steven Abney , who commented on earlier versions of this paper .	S-127
OWN	This work has been supported in part by the Ministry for Science and Research of the Land Baden-Wrttemberg under the project `` Corpus Based Development of Lexical Knowledge Bases ( ELWIS ) '' and by the Commission of the European Community under the LRE project `` Adapting Bilingual Dictionaries for Online Assistance ( COMPASS , LRE 62 - 080 ) '' .	S-128
