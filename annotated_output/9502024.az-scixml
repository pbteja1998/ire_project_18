BKG	An extragrammatical sentence is what a normal parser fails to analyze .	A-0
OWN	It is important to recover it using only syntactic information although results of recovery are better if semantic factors are considered .	A-1
OTH	A general algorithm for least-errors recognition , which is based only on syntactic information , was proposed byto deal with the extragrammaticality .	A-2
AIM	We extended this algorithm to recover extragrammatical sentence into grammatical one in running text .	A-3
AIM	Our robust parser with recovery mechanism - extended general algorithm for least-errors recognition - can be easily scaled up and modified because it utilize only syntactic information .	A-4
OWN	To upgrade this robust parser we proposed heuristics through the analysis on the Penn treebank corpus .	A-5
OWN	The experimental result shows 68 % - 77 % accuracy in error recovery .	A-6
BKG	Extragrammatical sentences include patently ungrammatical constructions as well as utterances that may be grammatically acceptable but are beyond the syntactic coverage of a parser , and any other difficult ones that are encountered in parsing.	S-0
BKG	Above examples show that people are used to write same meaningful sentences differently .	S-1
BKG	In addition , people are prone to mistakes in writing sentences .	S-2
BKG	So , the bulk of written sentences are open to the extragrammaticality .	S-3
BKG	In the Penn treebank tree-tagged corpus, for instance , about 80 percents of the rules are concerned with peculiar sentences which include inversive , elliptic , parenthetic , or emphatic phrases .	S-4
BKG	For example , we can drive a rule VPvb NP comma rb comma PP from the following sentence .	S-5
BKG	A robust parser is one that can analyze these extragrammatical sentences without failure .	S-6
BKG	However , if we try to preserve robustness by adding such rules whenever we encounter an extragrammatical sentence , the rulebase will grow up rapidly , and thus processing and maintaining the excessive number of rules will become inefficient and impractical .	S-7
BKG	Therefore , extragrammatical sentences should be handled by some recovery mechanism ( s ) rather than by a set of additional rules .	S-8
OTH	Many researchers have attempted several techniques to deal with extragrammatical sentences such as Augmented Transition Network ( ATN ), network-based semantic grammar, partial pattern matching, conceptual case frame, and multiple cooperating methods.	S-9
OTH	Above mentioned techniques take into account various semantic factors depending on specific domains on question in recovering extragrammatical sentences .	S-10
CTR	Whereas they can provide even better solutions intrinsically , they are usually ad-hoc and are lack of extensibility .	S-11
OWN	Therefore , it is important to recover extragrammatical sentences using syntactic factors only , which are independent of any particular system and any particular domain .	S-12
OTH	introduced some chart-based techniques using only syntactic information for extragrammatical sentences .	S-13
OTH	This technique has an advantage that there is no repeating work for the chart to prevent the parser from generating the same edge as the previously existed edge .	S-14
OTH	Also , because the recovery process runs when a normal parser terminates unsuccessfully , the performance of the normal parser does not decrease in case of handling grammatical sentences .	S-15
CTR	However , his experiment was not based on the errors in running texts but on artificial ones which were randomly generated by human .	S-16
CTR	Moreover , only one word error was considered though several word errors can occur simultaneously in the running text .	S-17
OTH	A general algorithm for least-errors recognition, proposed by, is to find out the least number of errors necessary to successful parsing and recover them .	S-18
OTH	Because this algorithm is also syntactically oriented and based on a chart , it has the same advantage as that of's parser .	S-19
OTH	When the original parsing algorithm terminates unsuccessfully , the algorithm begins to assume errors of insertion , deletion and mutation of a word .	S-20
OTH	For any input , including grammatical and extragrammatical sentences , this algorithm can generate the resultant parse tree .	S-21
CTR	At the cost of the complete robustness , however , this algorithm degrades the efficiency of parsing , and generates many intermediate edges .	S-22
AIM	In this paper , we present a robust parser with a recovery mechanism .	S-23
BAS	We extend the general algorithm for least-errors recognition to adopt it as the recovery mechanism in our robust parser .	S-24
OWN	Because our robust parser handle extragrammatical sentences with this syntactic information oriented recovery mechanism , it can be independent of a particular system or particular domain .	S-25
OWN	Also , we present the heuristics to reduce the number of edges so that we can upgrade the performance of our parser .	S-26
TXT	This paper is organized as follows : We first review a general algorithm for least-errors recognition .	S-27
TXT	Then we present the extension of this algorithm , and the heuristics adopted by the robust parser .	S-28
TXT	Next , we describe the implementation of the system and the result of the experiment of parsing real sentences .	S-29
TXT	Finally , we make conclusion with future direction .	S-30
OTH	The general algorithm for least-errors recognition, which is based on Earley 's algorithm , assumes that sentences may have insertion , deletion , and mutation errors of terminal symbols .	S-31
OTH	The objective of this algorithm is to parse input string with the least number of errors .	S-32
OTH	A state used in this algorithm is quadruple, where p is a production number in grammar , j marks a position in, f is a start position of the state in input string , and e is an error value .	S-33
OTH	A final statedenotes recognition of a phrasewith e errors whereis a number of components in rule p .	S-34
OTH	A stateset, where i is the position of the input , is an ordered set of states .	S-35
OTH	States within a stateset are ordered by ascending value of j , within a p within a f ; f takes descending value .	S-36
OTH	When adding to statesets , if stateis a candidate for admission to a stateset which already has a similar memberand e 'e , thenis rejected .	S-37
OTH	However , if, thenis replaced by.	S-38
OTH	The algorithm works as follows : A procedure SCAN is carried out for each state in.	S-39
OTH	SCAN checks various correspondences of input tokenagainst terminal symbols in RHS of rules .	S-40
OTH	Once SCAN is done , COMPLETER substitutes all final states ofinto all other analyses which can use them as components .	S-41
OTH	SCAN	S-42
OTH	SCAN handles states of, checking each input terminal against requirements of states inand various error hypotheses .	S-43
OTH	Figureshows how SCAN processes .	S-44
OTH	Letbe j-th component ofandbe i-th word of input string .	S-45
OTH	perfect match :	S-46
OTH	Ifthen addtoif possible .	S-47
OTH	insertion-error hypothesis :	S-48
OTH	Addtoif possible .	S-49
OTH	is the cost of an insertion-error for a terminal symbol .	S-50
OTH	deletion-error hypothesis :	S-51
OTH	Ifis terminal , then addtoif possible .	S-52
OTH	is the cost of a deletion-error for a terminal symbol .	S-53
OTH	mutation-error hypothesis :	S-54
OTH	Ifis terminal but not equal to, then addtoif possible .	S-55
OTH	is the cost of a mutation-error for a terminal symbol .	S-56
OTH	COMPLETER	S-57
OTH	COMPLETER handles substitution of final states inlike that of original Earley 's algorithm .	S-58
OTH	Each final state means the recognition of a nonterminal .	S-59
OTH	The algorithm in sectioncan analyze any input string with the least number of errors .	S-60
CTR	But this algorithm can handle only the errors of terminal symbols because it doesn't consider the errors of nonterminal nodes .	S-61
CTR	In the real text , however , the insertion , deletion , or inversion of a phrase - namely , nonterminal node - occurs more frequently .	S-62
AIM	So , we extend the original algorithm in order to handle the errors of nonterminal symbols as well .	S-63
OWN	In our extended algorithm , the same SCAN as that of the original algorithm is used , while COMPLETER is modified and extended .	S-64
OWN	Figureshows the processing of extended-COMPLETER .	S-65
OWN	In figure, [ NP ] denotes the final state whose rule has NP as its LHS .	S-66
OWN	In other words , it means the recognition of a noun phrase .	S-67
OWN	extended-COMPLETER	S-68
OWN	If there is a final statein,	S-69
OWN	phrase perfect match	S-70
OWN	If there exists a stateinandthen addinto.	S-71
OWN	phrase insertion-error hypothesis	S-72
OWN	If there exists a stateinthen addintoif possible .	S-73
OWN	is the cost of a insertion-error for a nonterminal symbol .	S-74
OWN	phrase deletion-error hypothesis	S-75
OWN	If there exists a stateinandis a nonterminal then addintoif possible .	S-76
OWN	is the cost of a deletion-error for a nonterminal symbol .	S-77
OWN	phrase mutation-error hypothesis	S-78
OWN	If there exists a stateinandis a nonterminal but not equal tothen addintoif possible .	S-79
OWN	is the cost of a mutation-error for a nonterminal symbol .	S-80
OWN	The extended least-errors recognition algorithm can handle not only terminal errors but also nonterminal errors .	S-81
OWN	The robust parser using the extended least-errors recognition algorithm overgenerates many error-hypothesis edges during parsing process .	S-82
OWN	To cope with this problem , we adjust error values according to the following heuristics .	S-83
OWN	Edges with more error values are regarded as less important ones , so that those edges are processed later than those of less error values .	S-84
OWN	Heuristics 1 : error types	S-85
OWN	The analysis on 3,538 sentences of the Penn treebank corpus WSJ shows that there are 498 sentences with phrase deletions and 224 sentences with phrase insertions .	S-86
OWN	So , we assign less error value to the deletion-error hypothesis edge than to the insertion - and mutation-errors .	S-87
OWN	whereis the error cost of a terminal symbol ,is the error cost of a nonterminal symbol .	S-88
OWN	Heuristics 2 : fiducial nonterminal	S-89
OWN	People often make mistakes in writing English .	S-90
OWN	These mistakes usually take place rather between small constituents such as a verbal phrase , an adverbial phrase and noun phrase than within small constituents themselves .	S-91
OWN	The possibility of error occurrence within noun phrases are lower than between a noun phrase and a verbal phrase , a preposition phrase , an adverbial phrase .	S-92
OWN	So , we assume some phrases , for example noun phrases , as fiducial nonterminals , which means error-free nonterminals .	S-93
OWN	When handling sentences , the robust parser assings more error values () to the error hypothesis edge occurring within a fiducial nonterminal .	S-94
OWN	Heuristics 3 : kinds of terminal symbols	S-95
OWN	Some terminal symbols like punctuation symbols , conjunctions and particles are often misused .	S-96
OWN	So , the robust parser assigns less error values () to the error hypothesis edges with these symbols than to the other terminal symbols .	S-97
OWN	Heuristics 4 : inserted phrases between commas or parentheses	S-98
OWN	Most of inserted phrases are surrounded by commas or parentheses .	S-99
OWN	For example ,	S-100
OWN	We will assign less error values () to the insertion-error hypothesis edges of nonterminals which are embraced by comma or parenthesis .	S-101
OWN	andare weights for the error of terminal nodes , andis a weight for the error of nonterminal nodes .	S-102
OWN	The error value e of an edge is calculated as follows .	S-103
OWN	All error values are additive .	S-104
OWN	The error value e for a rule, where a is a terminal node and A is a nonterminal node , is	S-105
OWN	where,andis an error value of a child edge .	S-106
OWN	By these heuristics , our robust parser can process only plausible edges first , instead of processing all generated edges at the same time , so that we can enhance the performance of the robust parser and result in the great reduction in the number of resultant trees .	S-107
OWN	Our robust parsing system is composed of two modules .	S-108
OWN	One module is a normal parser which is the bottom-up chart parser .	S-109
OWN	The other is a robust parser with the error recovery mechanism proposed herein .	S-110
OWN	At first , an input sentence is processed by the normal parser .	S-111
OWN	If the sentence is within the grammatical coverage of the system , the normal parser succeed to analyze it .	S-112
OWN	Otherwise , the normal parser fails , and then the robust parser starts to execute with edges generated by the normal parser .	S-113
OWN	The result of the robust parser is the parse trees which are within the grammatical coverage of the system .	S-114
OWN	The overview of the system is shown in figure.	S-115
OWN	To show usefulness of the robust parser proposed in this paper , we made some experiments .	S-116
OWN	Rule	S-117
OWN	We can derive 4,958 rules and their frequencies out of 14,137 sentences in the Penn treebank tree-tagged corpus , the Wall Street Journal .	S-118
OWN	The average frequency of each rule is 48 times in the corpus .	S-119
OWN	Of these rules , we remove rules which occurs fewer times than the average frequency in the corpus , and then only 192 rules are left .	S-120
OWN	These removed rules are almost for peculiar sentences and the left rules are very general rules .	S-121
OWN	We can show that our robust parser can compensate for lack of rules using only 192 rules with the recovery mechanism and heuristics .	S-122
OWN	Test set	S-123
OWN	First , 1,000 sentences are selected randomly from the WSJ corpus , which we have referred to in proposing the robust parser .	S-124
OWN	Of these sentences , 410 are failed in normal parsing , and are processed again by the robust parser .	S-125
OWN	To show the validity of these heuristics , we compare the result of the robust parser using heuristics with one not using heuristics .	S-126
OWN	Second , to show the adaptability of our robust parser ,	S-127
OWN	same experiments are carried out on 1,000 sentences from the ATIS corpus in Penn treebank , which we haven't referred to when we propose the robust parser .	S-128
OWN	Among 1,000 sentences from the ATIS , 465 sentences are processed by the robust parser after the failure of the normal parsing .	S-129
OWN	Parameter adjustment	S-130
OWN	We chose the best parameters of heuristics by executing several experiments .	S-131
OWN	Accuracy is measured as the percentage of constituents in the test sentences which do not cross any Penn treebank constituents.	S-132
OWN	Tableshows the results of the robust parser on WSJ .	S-133
OWN	In table, 5th , 6th and 7th raw mean that the percentage of sentences which have no crossing constituents , less than one crossing and less than two crossing respectively .	S-134
OWN	With heuristics , our robust parser can enhance the processing time and reduce the number of edges .	S-135
OWN	Also , the accuracy is improved from 72.8 % to 77.1 % even if the heuristics differentiate edges and prefer some edges .	S-136
OWN	It shows that the proposed heuristics is valid in parsing the real sentences .	S-137
OWN	The experiment says that our robust parser with heuristics can recover perfectly about 23 sentences out of 100 sentences which are just failed in normal parsing , as the percentage of no-crossing sentences is about 23.28 .	S-138
OWN	Tableis the results of the robust parser on ATIS which we did not refer to before .	S-139
OWN	The accuracy of the result on ATIS is lower than WSJ because the parameters of the heuristics are adjusted not by ATIS itself but by WSJ .	S-140
OWN	However , the percentage of sentences with constituents crossing less than 2 is higher than the WSJ , as sentences of ATIS are more or less simple .	S-141
OWN	The experimental results of our robust parser show high accuracy in recovery even though 96 % of total rules are removed .	S-142
OWN	It is impossible to construct complete grammar rules in the real parsing system to succeed in analyzing every real sentence .	S-143
OWN	So , parsing systems are likely to have extragrammatical sentences which cannot be analyzed by the systems .	S-144
OWN	Our robust parser can recover these extragrammatical sentences with 68 - 77 % accuracy .	S-145
OWN	It is very interesting that parameters of heuristics reflect the characteristics of the test corpus .	S-146
OWN	For example , if people tend to write sentences with inserted phrases , then the parametermust increase .	S-147
OWN	Therefore we can get better results if the parameter are fitted to the characteristics of the corpus .	S-148
AIM	In this paper , we have presented the robust parser with the extended least-errors recognition algorithm as the recovery mechanism .	S-149
OWN	This robust parser can easily be scaled up and applied to various domains because this parser depends only on syntactic factors .	S-150
OWN	To enhance the performance of the robust parser for extragrammatical sentences , we proposed several heuristics .	S-151
OWN	The heuristics assign the error values to each error-hypothesis edge , and edges which has less error values are processed first .	S-152
OWN	So , not all the generated edges are processed by the robust parser , but the most plausible parse trees can be generated first .	S-153
OWN	The accuracy of the recovery in our robust parser is about 68 % - 77 % .	S-154
OWN	Hence , this parser is suitable for systems in real application areas .	S-155
OWN	Our short term goal is to propose an automatic method that can learn parameter values of heuristics by analyzing the corpus .	S-156
OWN	We expect that automatically learned values of parameters can upgrade the performance of the parser .	S-157
OWN	This work was supported ( in part ) by Korea Science and Engineering Foundation ( KOSEF ) through Center for Artificial Intelligence Research ( CAIR ) , the Engineering Research Center ( ERC ) of Excellence Program .	S-158
