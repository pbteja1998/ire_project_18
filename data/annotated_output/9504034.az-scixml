AIM	We describe a corpus-based induction algorithm for probabilistic context-free grammars .	A-0
OWN	The algorithm employs a greedy heuristic search within a-ian framework , and a post-pass using the Inside-Outside algorithm .	A-1
CTR	We compare the performance of our algorithm to n-gram models and the Inside-Outside algorithm in three language modeling tasks .	A-2
CTR	In two of the tasks , the training data is generated by a probabilistic context-free grammar and in both tasks our algorithm outperforms the other techniques .	A-3
CTR	The third task involves naturally-occurring data , and in this task our algorithm does not perform as well as n-gram models but vastly outperforms the Inside-Outside algorithm .	A-4
BKG	In applications such as speech recognition , handwriting recognition , and spelling correction , performance is limited by the quality of the language model utilized,,,.	S-0
BKG	However , static language modeling performance has remained basically unchanged since the advent of n-gram language models forty years ago.	S-1
CTR	Yet , n-gram language models can only capture dependencies within an n-word window , where currently the largest practical n for natural language is three , and many dependencies in natural language occur beyond a three-word window .	S-2
CTR	In addition , n-gram models are extremely large , thus making them difficult to implement efficiently in memory-constrained applications .	S-3
OTH	An appealing alternative is grammar-based language models .	S-4
OTH	Language models expressed as a probabilistic grammar tend to be more compact than n-gram language models , and have the ability to model long-distance dependencies,,.	S-5
CTR	However , to date there has been little success in constructing grammar-based language models competitive with n-gram models in problems of any magnitude .	S-6
AIM	In this paper , we describe a corpus-based induction algorithm for probabilistic context-free grammars that outperforms n-gram models and the Inside-Outside algorithmin medium-sized domains .	S-7
CTR	This result marks the first time a grammar-based language model has surpassed n-gram modeling in a task of at least moderate size .	S-8
OWN	The algorithm employs a greedy heuristic search within a-ian framework , and a post-pass using the Inside-Outside algorithm .	S-9
BKG	Grammar induction can be framed as a search problem , and has been framed as such almost without exception in past research.	S-10
BKG	The search space is taken to be some class of grammars ; for example , in our work we search within the space of probabilistic context-free grammars .	S-11
BKG	The objective function is taken to be some measure dependent on the training data ; one generally wants to find a grammar that in some sense accurately models the training data .	S-12
OTH	Most work in language modeling , including n-gram models and the Inside-Outside algorithm , falls under the maximum-likelihood paradigm , where one takes the objective function to be the likelihood of the training data given the grammar .	S-13
CTR	However , the optimal grammar under this objective function is one which generates only strings in the training data and no other strings .	S-14
CTR	Such grammars are poor language models , as they overfit the training data and do not model the language at large .	S-15
OTH	In n-gram models and the Inside-Outside algorithm , this issue is evaded by bounding the size and form of the grammars considered , so that the `` optimal '' grammar cannot be expressed .	S-16
CTR	However , in our work we do not wish to limit the size of the grammars considered .	S-17
CTR	The basic shortcoming of the maximum-likelihood objective function is that it does not encompass the compelling intuition behind Occam 's Razor , that simpler ( or smaller ) grammars are preferable over complex ( or larger ) grammars .	S-18
OTH	A factor in the objective function that favors smaller grammars over large can prevent the objective function from preferring grammars that overfit the training data .	S-19
OTH	presents a-ian grammar induction framework that includes such a factor in a motivated manner .	S-20
BKG	The goal of grammar induction is taken to be finding the grammar with the largest a posteriori probability given the training data , that is , finding the grammar G ' where	S-21
BKG	and where we denote the training data as O , for observations .	S-22
OWN	As it is unclear how to estimatedirectly , we apply's Rule and get	S-23
OWN	Hence , we can frame the search for G ' as a search with the objective function, the likelihood of the training data multiplied by the prior probability of the grammar .	S-24
OWN	We satisfy the goal of favoring smaller grammars by choosing a prior that assigns higher probabilities to such grammars .	S-25
BAS	In particular ,proposes the use of the universal a priori probability, which is closely related to the minimum description length principle later proposed by.	S-26
OTH	In the case of grammatical language modeling , this corresponds to taking	S-27
OTH	whereis the length of the description of the grammar in bits .	S-28
OTH	The universal a priori probability has many elegant properties , the most salient of which is that it dominates all other enumerable probability distributions multiplicatively .	S-29
OWN	As described above , we take grammar induction to be the search for the grammar G ' that optimizes the objective function.	S-30
OWN	While this framework does not restrict us to a particular grammar formalism , in our work we consider only probabilistic context-free grammars .	S-31
OWN	We assume a simple greedy search strategy .	S-32
OWN	We maintain a single hypothesis grammar which is initialized to a small , trivial grammar .	S-33
OWN	We then try to find a modification to the hypothesis grammar , such as the addition of a grammar rule , that results in a grammar with a higher score on the objective function .	S-34
OWN	When we find a superior grammar , we make this the new hypothesis grammar .	S-35
OWN	We repeat this process until we can no longer find a modification that improves the current hypothesis grammar .	S-36
OWN	For our initial grammar , we choose a grammar that can generate any string , to assure that the grammar can cover the training data .	S-37
OWN	The initial grammar is listed in Table.	S-38
OWN	The sentential symbol S expands to a sequence of X 's , where X expands to every other nonterminal symbol in the grammar .	S-39
OWN	Initially , the set of nonterminal symbols consists of a different nonterminal symbol expanding to each terminal symbol .	S-40
OWN	Notice that this grammar models a sentence as a sequence of independently generated nonterminal symbols .	S-41
OWN	We maintain this property throughout the search process , that is , for every symbol A ' that we add to the grammar , we also add a rule.	S-42
OWN	This assures that the sentential symbol can expand to every symbol ; otherwise , adding a symbol will not affect the probabilities that the grammar assigns to strings .	S-43
OWN	We use the term move set to describe the set of modifications we consider to the current hypothesis grammar to hopefully produce a superior grammar .	S-44
OWN	Our move set includes the following moves :	S-45
OWN	Move 1 : Create a rule of the form	S-46
OWN	Move 2 : Create a rule of the form	S-47
OWN	For any context-free grammar , it is possible to express a weakly equivalent grammar using only rules of these forms .	S-48
OWN	As mentioned before , with each new symbol A we also create a rule.	S-49
OWN	Consider the task of calculating the objective functionfor some grammar G .	S-50
OWN	Calculatingis inexpensive ; however , calculatingrequires a parsing of the entire training data .	S-51
OWN	We cannot afford to parse the training data for each grammar considered ; indeed , to ever be practical for data sets of millions of words , it seems likely that we can only afford to parse the data once .	S-52
OWN	To achieve this goal , we employ several approximations .	S-53
OWN	First , notice that we do not ever need to calculate the actual value of the objective function ; we need only to be able to distinguish when a move applied to the current hypothesis grammar produces a grammar that has a higher score on the objective function , that is , we need only to be able to calculate the difference in the objective function resulting from a move .	S-54
OWN	This can be done efficiently if we can quickly approximate how the probability of the training data changes when a move is applied .	S-55
OWN	To make this possible , we approximate the probability of the training databy the probability of the single most probable parse , or Viterbi parse , of the training data .	S-56
OWN	Furthermore , instead of recalculating the Viterbi parse of the training data from scratch when a move is applied , we use heuristics to predict how a move will change the Viterbi parse .	S-57
OWN	For example , consider the case where the training data consists of the two sentences	S-58
OWN	In Figure, we display the Viterbi parse of this data under the initial hypothesis grammar used in our algorithm .	S-59
OWN	Now , let us consider the move of adding the rule	S-60
OWN	to the initial grammar ( as well as the concomitant rule) .	S-61
OWN	A reasonable heuristic for predicting how the Viterbi parse will change is to replace adjacent X 's that expand toandrespectively with a single X that expands to B , as displayed in Figure.	S-62
OWN	This is the actual heuristic we use for moves of the form, and we have analogous heuristics for each move in our move set .	S-63
OWN	By predicting the differences in the Viterbi parse resulting from a move , we can quickly estimate the change in the probability of the training data .	S-64
OWN	Notice that our predicted Viterbi parse can stray a great deal from the actual Viterbi parse , as errors can accumulate as move after move is applied .	S-65
OWN	To minimize these effects , we process the training data incrementally .	S-66
OWN	Using our initial hypothesis grammar , we parse the first sentence of the training data and search for the optimal grammar over just that one sentence using the described search framework .	S-67
OWN	We use the resulting grammar to parse the second sentence , and then search for the optimal grammar over the first two sentences using the last grammar as the starting point .	S-68
OWN	We repeat this process , parsing the next sentence using the best grammar found on the previous sentences and then searching for the best grammar taking into account this new sentence , until the entire training corpus is covered .	S-69
OWN	Delaying the parsing of a sentence until all of the previous sentences are processed should yield more accurate Viterbi parses during the search process than if we simply parse the whole corpus with the initial hypothesis grammar .	S-70
OWN	In addition , we still achieve the goal of parsing each sentence but once .	S-71
TXT	In this section , we describe how the parameters of our grammar , the probabilities associated with each grammar rule , are set .	S-72
OWN	Ideally , in evaluating the objective function for a particular grammar we should use its optimal parameter settings given the training data , as this is the full score that the given grammar can achieve .	S-73
OWN	However , searching for optimal parameter values is extremely expensive computationally .	S-74
OWN	Instead , we grossly approximate the optimal values by deterministically setting parameters based on the Viterbi parse of the training data parsed so far .	S-75
OWN	We rely on the post-pass , described later , to refine parameter values .	S-76
OWN	Referring to the rules in Table, the parameteris set to an arbitrary small constant .	S-77
OWN	The values of the parametersare set to the ( smoothed ) frequency of thereduction in the Viterbi parse of the data seen so far .	S-78
OWN	The remaining symbols are set to expand uniformly among their possible expansions .	S-79
OWN	Consider the move of creating a rule of the form.	S-80
OWN	This corresponds todifferent specific rules that might be created , where k is the current number of symbols in the grammar .	S-81
OWN	As it is too computationally expensive to consider each of these rules at every point in the search , we use heuristics to constrain which moves are appraised .	S-82
OWN	For the left-hand side of a rule , we always create a new symbol .	S-83
OWN	This heuristic selects the optimal choice the vast majority of the time ; however , under this constraint the moves described earlier in this section cannot yield arbitrary context-free languages .	S-84
OWN	To partially address this , we add the move	S-85
OWN	Move 3 : Create a rule of the form	S-86
OWN	With this iteration move , we can construct grammars that generate arbitrary regular languages .	S-87
OWN	As yet , we have not implemented moves that enable the construction of arbitrary context-free grammars ; this belongs to future work .	S-88
OWN	To constrain the symbols we consider on the right-hand side of a new rule , we use what we call triggers .	S-89
OWN	A trigger is a phenomenon in the Viterbi parse of a sentence that is indicative that a particular move might lead to a better grammar .	S-90
OWN	For example , in Figurethe fact that the symbolsandoccur adjacently is indicative that it could be profitable to create a rule.	S-91
OWN	We have developed a set of triggers for each move in our move set , and only consider a specific move if it is triggered in the sentence currently being parsed in the incremental processing .	S-92
OWN	A conspicuous shortcoming in our search framework is that the grammars in our search space are fairly unexpressive .	S-93
OWN	Firstly , recall that our grammars model a sentence as a sequence of independently generated symbols ; however , in language there is a large dependence between adjacent constituents .	S-94
OWN	Furthermore , the only free parameters in our search are the parameters; all other symbols ( except S ) are fixed to expand uniformly .	S-95
OWN	These choices were necessary to make the search tractable .	S-96
OWN	To address this issue , we use an Inside-Outside algorithm post-pass .	S-97
OWN	Our methodology is derived from that described by.	S-98
OWN	We create n new nonterminal symbols, and create all rules of the form :	S-99
OWN	denotes the set of nonterminal symbols acquired in the initial grammar induction phase , andis taken to be the new sentential symbol .	S-100
OWN	These new rules replace the first three rules listed in Table.	S-101
OWN	The parameters of these rules are initialized randomly .	S-102
OWN	Using this grammar as the starting point , we run the Inside-Outside algorithm on the training data until convergence .	S-103
OWN	In other words , instead of using the naiverule to attach symbols together in parsing data , we now use therules and depend on the Inside-Outside algorithm to train these randomly initialized rules intelligently .	S-104
OWN	This post-pass allows us to express dependencies between adjacent symbols .	S-105
OWN	In addition , it allows us to train parameters that were fixed during the initial grammar induction phase .	S-106
BAS	As mentioned , this work employs the-ian grammar induction framework described by,.	S-107
CTR	However ,does not specify a concrete search algorithm and only makes suggestions as to its nature .	S-108
OTH	Similar research includes work byand.	S-109
OTH	This work also employs a heuristic search within a-ian framework .	S-110
CTR	However , a different prior probability on grammars is used , and the algorithms are only efficient enough to be applied to small data sets .	S-111
OTH	The grammar induction algorithms most successful in language modeling include the Inside-Outside algorithm,,, a special case of the Expectation-Maximization algorithm, and work by.	S-112
OTH	In the latter work ,uses a heuristic search procedure similar to ours , but a very different search criteria .	S-113
CTR	To our knowledge , neither algorithm has surpassed the performance of n-gram models in a language modeling task of substantial scale .	S-114
CTR	To evaluate our algorithm , we compare the performance of our algorithm to that of n-gram models and the Inside-Outside algorithm .	S-115
OTH	For n-gram models , we triedfor each domain .	S-116
OTH	For smoothing a particular n-gram model , we took a linear combination of all lower order n-gram models .	S-117
OTH	In particular , we follow standard practice,,and take the smoothed i-gram probability to be a linear combination of the i-gram frequency in the training data and the smoothed ( i - 1 ) - gram probability , that is ,	S-118
OTH	where c ( W ) denotes the count of the word sequence W in the training data .	S-119
OTH	The smoothing parametersare trained through the Forward-Backward algorithmon held-out data .	S-120
OTH	Parametersare tied together for similar c to prevent data sparsity .	S-121
OTH	For the Inside-Outside algorithm , we follow the methodology described by.	S-122
OTH	For a given n , we create a probabilistic context-free grammar consisting of all Chomsky normal form rules over the n nonterminal symbolsand the given terminal symbols , that is , all rules	S-123
OTH	where T denotes the set of terminal symbols in the domain .	S-124
OTH	All parameters are initialized randomly .	S-125
OTH	From this starting point , the Inside-Outside algorithm is run until convergence .	S-126
OTH	For smoothing , we combine the expansion distribution of each symbol with a uniform distribution , that is , we take the smoothed parameterto be	S-127
OTH	wheredenotes the unsmoothed parameter .	S-128
OTH	The valueis the number of different ways a symbol expands under themethodology .	S-129
OTH	The parameteris trained through the Inside-Outside algorithm on held-out data .	S-130
OTH	This smoothing is also performed on the Inside-Outside post-pass of our algorithm .	S-131
OTH	For each domain , we tried.	S-132
OWN	Because of the computational demands of our algorithm , it is currently impractical to apply it to large vocabulary or large training set problems .	S-133
OWN	However , we present the results of our algorithm in three medium-sized domains .	S-134
OWN	In each case , we use 4500 sentences for training , with 500 of these sentences held out for smoothing .	S-135
OWN	We test on 500 sentences , and measure performance by the entropy of the test data .	S-136
OWN	In the first two domains , we created the training and test data artificially so as to have an ideal grammar in hand to benchmark results .	S-137
OWN	In particular , we used a probabilistic grammar to generate the data .	S-138
OWN	In the first domain , we created this grammar by hand ; the grammar was a small English-like probabilistic context-free grammar consisting of roughly 10 nonterminal symbols , 20 terminal symbols , and 30 rules .	S-139
OWN	In the second domain , we derived the grammar from manually parsed text .	S-140
OWN	From a million words of parsed Wall Street Journal data from the Penn treebank , we extracted the 20 most frequently occurring symbols , and the 10 most frequently occurring rules expanding each of these symbols .	S-141
OWN	For each symbol that occurs on the right-hand side of a rule but which was not one of the most frequent 20 symbols , we create a rule that expands that symbol to a unique terminal symbol .	S-142
OWN	After removing unreachable rules , this yields a grammar of roughly 30 nonterminals , 120 terminals , and 160 rules .	S-143
OWN	Parameters are set to reflect the frequency of the corresponding rule in the parsed corpus .	S-144
OWN	For the third domain , we took English text and reduced the size of the vocabulary by mapping each word to its part-of-speech tag .	S-145
OWN	We used tagged Wall Street Journal text from the Penn treebank , which has a tag set size of about fifty .	S-146
OWN	In Tables-, we summarize our results .	S-147
OWN	The ideal grammar denotes the grammar used to generate the training and test data .	S-148
OWN	For each algorithm , we list the best performance achieved over all n tried , and the best n column states which value realized this performance .	S-149
CTR	We achieve a moderate but significant improvement in performance over n-gram models and the Inside-Outside algorithm in the first two domains , while in the part-of-speech domain we are outperformed by n-gram models but we vastly outperform the Inside-Outside algorithm .	S-150
OWN	In Table, we display a sample of the number of parameters and execution time ( on a Decstation 5000 / 33 ) associated with each algorithm .	S-151
OWN	We choose n to yield approximately equivalent performance for each algorithm .	S-152
OWN	The first pass row refers to the main grammar induction phase of our algorithm , and the post-pass row refers to the Inside-Outside post-pass .	S-153
CTR	Notice that our algorithm produces a significantly more compact model than the n-gram model , while running significantly faster than the Inside-Outside algorithm even though we use an Inside-Outside post-pass .	S-154
OWN	Part of this discrepancy is due to the fact that we require a smaller number of new nonterminal symbols to achieve equivalent performance , but we have also found that our post-pass converges more quickly even given the same number of nonterminal symbols .	S-155
CTR	Our algorithm consistently outperformed the Inside-Outside algorithm in these experiments .	S-156
OWN	While we partially attribute this difference to using a-ian instead of maximum-likelihood objective function , we believe that part of this difference results from a more effective search strategy .	S-157
OWN	In particular , though both algorithms employ a greedy hill-climbing strategy , our algorithm gains an advantage by being able to add new rules to the grammar .	S-158
OTH	In the Inside-Outside algorithm , the gradient descent search discovers the `` nearest '' local minimum in the search landscape to the initial grammar .	S-159
OTH	If there are k rules in the grammar and thus k parameters , then the search takes place in a fixed k-dimensional space.	S-160
OWN	In our algorithm , it is possible to expand the hypothesis grammar , thus increasing the dimensionality of the parameter space that is being searched .	S-161
OWN	An apparent local minimum in the spacemay no longer be a local minimum in the space; the extra dimension may provide a pathway for further improvement of the hypothesis grammar .	S-162
CTR	Hence , our algorithm should be less prone to suboptimal local minima than the Inside-Outside algorithm .	S-163
CTR	Outperforming n-gram models in the first two domains demonstrates that our algorithm is able to take advantage of the grammatical structure present in data .	S-164
CTR	However , the superiority of n-gram models in the part-of-speech domain indicates that to be competitive in modeling naturally-occurring data , it is necessary to model collocational information accurately .	S-165
OWN	We need to modify our algorithm to more aggressively model n-gram information .	S-166
BKG	This research represents a step forward in the quest for developing grammar-based language models for natural language .	S-167
AIM	We induce models that , while being substantially more compact , outperform n-gram language models in medium-sized domains .	S-168
OWN	The algorithm runs essentially in time and space linear in the size of the training data , so larger domains are within our reach .	S-169
BAS	However , we feel the largest contribution of this work does not lie in the actual algorithm specified , but rather in its indication of the potential of the induction framework described byin 1964 .	S-170
OWN	We have implemented only a subset of the moves that we have developed , and inspection of our results gives reason to believe that these additional moves may significantly improve the performance of our algorithm .	S-171
OTH	's induction framework is not restricted to probabilistic context-free grammars .	S-172
OWN	After completing the implementation of our move set , we plan to explore the modeling of context-sensitive phenomena .	S-173
BAS	This work demonstrates that's elegant framework deserves much further consideration .	S-174
