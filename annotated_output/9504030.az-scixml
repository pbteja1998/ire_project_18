CTR	Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text , as is evidenced by their poor performance on domains like the Wall Street Journal , and by the movement away from parsing-based approaches to text-processing in general .	A-0
AIM	In this paper , I describe SPATTER , a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result .	A-1
OWN	This work is based on the following premises :	A-2
OWN	grammars are too complex and detailed to develop manually for most interesting domains ;	A-3
OWN	parsing models must rely heavily on lexical and contextual information to analyze sentences accurately ; and	A-4
OWN	existing n-gram modeling techniques are inadequate for parsing models .	A-5
CTR	In experiments comparing SPATTER with IBM 's computer manuals parser , SPATTER significantly outperforms the grammar-based parser .	A-6
OWN	Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures , SPATTER achieves 86 % precision , 86 % recall , and 1.3 crossing brackets per sentence for sentences of 40 words or less , and 91 % precision , 90 % recall , and 0.5 crossing brackets for sentences between 10 and 20 words in length .	A-7
BKG	Parsing a natural language sentence can be viewed as making a sequence of disambiguation decisions : determining the part-of-speech of the words , choosing between possible constituent structures , and selecting labels for the constituents .	S-0
OTH	Traditionally , disambiguation problems in parsing have been addressed by enumerating possibilities and explicitly declaring knowledge which might aid the disambiguation process .	S-1
CTR	However , these approaches have proved too brittle for most interesting natural language problems .	S-2
AIM	This work addresses the problem of automatically discovering the disambiguation criteria for all of the decisions made during the parsing process , given the set of possible features which can act as disambiguators .	S-3
OWN	The candidate disambiguators are the words in the sentence , relationships among the words , and relationships among constituents already constructed in the parsing process .	S-4
OWN	Since most natural language rules are not absolute , the disambiguation criteria discovered in this work are never applied deterministically .	S-5
OWN	Instead , all decisions are pursued non-deterministically according to the probability of each choice .	S-6
OWN	These probabilities are estimated using statistical decision tree models .	S-7
OWN	The probability of a complete parse tree ( T ) of a sentence ( S ) is the product of each decision () conditioned on all previous decisions :	S-8
OWN	Each decision sequence constructs a unique parse , and the parser selects the parse whose decision sequence yields the highest cumulative probability .	S-9
OWN	By combining a stack decoder search with a breadth-first algorithm with probabilistic pruning , it is possible to identify the highest-probability parse for any sentence using a reasonable amount of memory and time .	S-10
AIM	The claim of this work is that statistics from a large corpus of parsed sentences combined with information-theoretic classification and training algorithms can produce an accurate natural language parser without the aid of a complicated knowledge base or grammar .	S-11
AIM	This claim is justified by constructing a parser , called SPATTER ( Statistical PATTErn Recognizer ) , based on very limited linguistic information , and comparing its performance to a state-of-the-art grammar-based parser on a common task .	S-12
OWN	It remains to be shown that an accurate broad-coverage parser can improve the performance of a text processing application .	S-13
OWN	This will be the subject of future experiments .	S-14
CTR	One of the important points of this work is that statistical models of natural language should not be restricted to simple , context-insensitive models .	S-15
CTR	In a problem like parsing , where long-distance lexical information is crucial to disambiguate interpretations accurately , local models like probabilistic context-free grammars are inadequate .	S-16
BAS	This work illustrates that existing decision-tree technology can be used to construct and estimate models which selectively choose elements of the context which contribute to disambiguation decisions , and which have few enough parameters to be trained using existing resources .	S-17
TXT	I begin by describing decision-tree modeling , showing that decision-tree models are equivalent to interpolated n-gram models .	S-18
TXT	Then I briefly describe the training and parsing procedures used in SPATTER .	S-19
TXT	Finally , I present some results of experiments comparing SPATTER with a grammarian 's rule-based statistical parser , along with more recent results showing SPATTER applied to the Wall Street Journal domain .	S-20
OWN	Much of the work in this paper depends on replacing human decision-making skills with automatic decision-making algorithms .	S-21
OWN	The decisions under consideration involve identifying constituents and constituent labels in natural language sentences .	S-22
BKG	Grammarians , the human decision-makers in parsing , solve this problem by enumerating the features of a sentence which affect the disambiguation decisions and indicating which parse to select based on the feature values .	S-23
BKG	The grammarian is accomplishing two critical tasks : identifying the features which are relevant to each decision , and deciding which choice to select based on the values of the relevant features .	S-24
OTH	Decision-tree classification algorithms account for both of these tasks , and they also accomplish a third task which grammarians classically find difficult .	S-25
OTH	By assigning a probability distribution to the possible choices , decision trees provide a ranking system which not only specifies the order of preference for the possible choices , but also gives a measure of the relative likelihood that each choice is the one which should be selected .	S-26
OTH	A decision tree is a decision-making device which assigns a probability to each of the possible choices based on the context of the decision :, where f is an element of the future vocabulary ( the set of choices ) and h is a history ( the context of the decision ) .	S-27
OTH	This probabilityis determined by asking a sequence of questionsabout the context , where the ith question asked is uniquely determined by the answers to the i - 1 previous questions .	S-28
OTH	For instance , consider the part-of-speech tagging problem .	S-29
OTH	The first question a decision tree might ask is :	S-30
OTH	What is the word being tagged ?	S-31
OTH	If the answer is the , then the decision tree needs to ask no more questions ; it is clear that the decision tree should assign the tagwith probability 1 .	S-32
OTH	If , instead , the answer to question 1 is bear , the decision tree might next ask the question : 2 .	S-33
OTH	What is the tag of the previous word ?	S-34
OTH	If the answer to question 2 is determiner , the decision tree might stop asking questions and assign the tagwith very high probability , and the tagwith much lower probability .	S-35
OTH	However , if the answer to question 2 is noun , the decision tree would need to ask still more questions to get a good estimate of the probability of the tagging decision .	S-36
OTH	The decision tree described in this paragraph is shown in Figure.	S-37
OTH	Each question asked by the decision tree is represented by a tree node ( an oval in the figure ) and the possible answers to this question are associated with branches emanating from the node .	S-38
OTH	Each node defines a probability distribution on the space of possible decisions .	S-39
OTH	A node at which the decision tree stops asking questions is a leaf node .	S-40
OTH	The leaf nodes represent the unique states in the decision-making problem , i.e. all contexts which lead to the same leaf node have the same probability distribution for the decision .	S-41
OTH	A decision-tree model is not really very different from an interpolated n-gram model .	S-42
OTH	In fact , they are equivalent in representational power .	S-43
OTH	The main differences between the two modeling techniques are how the models are parameterized and how the parameters are estimated .	S-44
OTH	First , let 's be very clear on what we mean by an n-gram model .	S-45
OTH	Usually , an n-gram model refers to a Markov process where the probability of a particular token being generating is dependent on the values of the previous n - 1 tokens generated by the same process .	S-46
OTH	By this definition , an n-gram model hasparameters , where | W | is the number of unique tokens generated by the process .	S-47
OTH	However , here let 's define an n-gram model more loosely as a model which defines a probability distribution on a random variable given the values of n - 1 random variables ,There is no assumption in the definition that any of the random variables F orrange over the same vocabulary .	S-48
OTH	The number of parameters in this n-gram model is.	S-49
OTH	Using this definition , an n-gram model can be represented by a decision-tree model with n - 1 questions .	S-50
OTH	For instance , the part-of-speech tagging modelcan be interpreted as a 4-gram model , whereis the variable denoting the word being tagged ,is the variable denoting the tag of the previous word , andis the variable denoting the tag of the word two words back .	S-51
OTH	Hence , this 4-gram tagging model is the same as a decision-tree model which always asks the sequence of 3 questions :	S-52
OTH	What is the word being tagged ?	S-53
OTH	What is the tag of the previous word ?	S-54
OTH	What is the tag of the word two words back ?	S-55
OTH	But can a decision-tree model be represented by an n-gram model ?	S-56
OTH	No , but it can be represented by an interpolated n-gram model .	S-57
TXT	The proof of this assertion is given in the next section .	S-58
OTH	The standard approach to estimating an n-gram model is a two step process .	S-59
OTH	The first step is to count the number of occurrences of each n-gram from a training corpus .	S-60
OTH	This process determines the empirical distribution ,	S-61
OTH	The second step is smoothing the empirical distribution using a separate , held-out corpus .	S-62
OTH	This step improves the empirical distribution by finding statistically unreliable parameter estimates and adjusting them based on more reliable information .	S-63
OTH	A commonly-used technique for smoothing is deleted interpolation .	S-64
OTH	Deleted interpolation estimates a modelby using a linear combination of empirical modelswhereandfor all	S-65
OTH	For example , a modelmight be interpolated as follows :	S-66
OTH	wherefor all histories.	S-67
OTH	The optimal values for thefunctions can be estimated using the forward-backward algorithm.	S-68
OWN	A decision-tree model can be represented by an interpolated n-gram model as follows .	S-69
OWN	A leaf node in a decision tree can be represented by the sequence of question answers , or history values , which leads the decision tree to that leaf .	S-70
OWN	Thus , a leaf node defines a probability distribution based on values of those questions :whereandand whereis the answer to one of the questions asked on the path from the root to the leaf .	S-71
OWN	But this is the same as one of the terms in the interpolated n-gram model .	S-72
OWN	So , a decision tree can be defined as an interpolated n-gram model where thefunction is defined as :	S-73
OWN	The point of showing the equivalence between n-gram models and decision-tree models is to make clear that the power of decision-tree models is not in their expressiveness , but instead in how they can be automatically acquired for very large modeling problems .	S-74
OTH	As n grows , the parameter space for an n-gram model grows exponentially , and it quickly becomes computationally infeasible to estimate the smoothed model using deleted interpolation .	S-75
OTH	Also , as n grows large , the likelihood that the deleted interpolation process will converge to an optimal or even near-optimal parameter setting becomes vanishingly small .	S-76
OTH	On the other hand , the decision-tree learning algorithm increases the size of a model only as the training data allows .	S-77
OTH	Thus , it can consider very large history spaces , i.e. n-gram models with very large n. Regardless of the value of n , the number of parameters in the resulting model will remain relatively constant , depending mostly on the number of training examples .	S-78
OTH	The leaf distributions in decision trees are empirical estimates , i.e. relative-frequency counts from the training data .	S-79
CTR	Unfortunately , they assign probability zero to events which can possibly occur .	S-80
OWN	Therefore , just as it is necessary to smooth empirical n-gram models , it is also necessary to smooth empirical decision-tree models .	S-81
BAS	The decision-tree learning algorithms used in this work were developed over the past 15 years by the IBM Speech Recognition group.	S-82
BAS	The growing algorithm is an adaptation of the CART algorithm in.	S-83
OTH	For detailed descriptions and discussions of the decision-tree algorithms used in this work , see.	S-84
OWN	An important point which has been omitted from this discussion of decision trees is the fact that only binary questions are used in these decision trees .	S-85
OWN	A question which has k values is decomposed into a sequence of binary questions using a classification tree on those k values .	S-86
OWN	For example , a question about a word is represented as 30 binary questions .	S-87
BAS	These 30 questions are determined by growing a classification tree on the word vocabulary as described in.	S-88
OTH	The 30 questions represent 30 different binary partitions of the word vocabulary , and these questions are defined such that it is possible to identify each word by asking all 30 questions .	S-89
OTH	For more discussion of the use of binary decision-tree questions , see.	S-90
OWN	The SPATTER parsing algorithm is based on interpreting parsing as a statistical pattern recognition process .	S-91
OWN	A parse tree for a sentence is constructed by starting with the sentence 's words as leaves of a tree structure , and labeling and extending nodes these nodes until a single-rooted , labeled tree is constructed .	S-92
OWN	This pattern recognition process is driven by the decision-tree models described in the previous section .	S-93
OWN	A parse tree can be viewed as an n-ary branching tree , with each node in a tree labeled by either a non-terminal label or a part-of-speech label .	S-94
OWN	If a parse tree is interpreted as a geometric pattern , a constituent is no more than a set of edges which meet at the same tree node .	S-95
OWN	For instance , the noun phrase , `` a brown cow , '' consists of an edge extending to the right from `` a , '' an edge extending to the left from `` cow , '' and an edge extending straight up from `` brown '' .	S-96
OWN	In SPATTER , a parse tree is encoded in terms of four elementary components , or features : words , tags , labels , and extensions .	S-97
OWN	Each feature has a fixed vocabulary , with each element of a given feature vocabulary having a unique representation .	S-98
OWN	The word feature can take on any value of any word .	S-99
OWN	The tag feature can take on any value in the part-of-speech tag set .	S-100
OWN	The label feature can take on any value in the non-terminal set .	S-101
OWN	The extension can take on any of the following five values :	S-102
OWN	right - the node is the first child of a constituent ;	S-103
OWN	left - the node is the last child of a constituent ;	S-104
OWN	up - the node is neither the first nor the last child of a constituent ;	S-105
OWN	unary - the node is a child of a unary constituent ;	S-106
OWN	root - the node is the root of the tree .	S-107
OWN	For an n word sentence , a parse tree has n leaf nodes , where the word feature value of the ith leaf node is the ith word in the sentence .	S-108
OWN	The word feature value of the internal nodes is intended to contain the lexical head of the node 's constituent .	S-109
OWN	A deterministic lookup table based on the label of the internal node and the labels of the children is used to approximate this linguistic notion .	S-110
OWN	The SPATTER representation of the sentence	S-111
OWN	is shown in Figure.	S-112
OWN	The nodes are constructed bottom-up from left-to-right , with the constraint that no constituent node is constructed until all of its children have been constructed .	S-113
OWN	The order in which the nodes of the example sentence are constructed is indicated in the figure .	S-114
OWN	SPATTER consists of three main decision-tree models : a part-of-speech tagging model , a node-extension model , and a node-labeling model .	S-115
OWN	Each of these decision-tree models are grown using the following questions , where X is one of word , tag , label , or extension , and Y is either left and right :	S-116
OWN	What is the X at the current node ?	S-117
OWN	What is the X at the node to the Y ?	S-118
OWN	What is the X at the node two nodes to the Y ?	S-119
OWN	What is the X at the current node 's first child from the Y ?	S-120
OWN	What is the X at the current node 's second child from the Y ?	S-121
OWN	For each of the nodes listed above , the decision tree could also ask about the number of children and span of the node .	S-122
OWN	For the tagging model , the values of the previous two words and their tags are also asked , since they might differ from the head words of the previous two constituents .	S-123
OWN	The training algorithm proceeds as follows .	S-124
OWN	The training corpus is divided into two sets , approximately 90 % for tree growing and 10 % for tree smoothing .	S-125
OWN	For each parsed sentence in the tree growing corpus , the correct state sequence is traversed .	S-126
OWN	Each state transition fromtois an event ; the history is made up of the answers to all of the questions at stateand the future is the value of the action taken from stateto stateEach event is used as a training example for the decision-tree growing process for the appropriate feature 's tree ( e.g. each tagging event is used for growing the tagging tree , etc .	S-127
OWN	) .	S-128
BAS	After the decision trees are grown , they are smoothed using the tree smoothing corpus using a variation of the deleted interpolation algorithm described in.	S-129
OWN	The parsing procedure is a search for the highest probability parse tree .	S-130
OWN	The probability of a parse is just the product of the probability of each of the actions made in constructing the parse , according to the decision-tree models .	S-131
OWN	Because of the size of the search space , ( roughlywhere | T | is the number of part-of-speech tags , n is the number of words in the sentence , and | N | is the number of non-terminal labels ) , it is not possible to compute the probability of every parse .	S-132
OWN	However , the specific search algorithm used is not very important , so long as there are no search errors .	S-133
OWN	A search error occurs when the the highest probability parse found by the parser is not the highest probability parse in the space of all parses .	S-134
OWN	SPATTER 's search procedure uses a two phase approach to identify the highest probability parse of a sentence .	S-135
OWN	First , the parser uses a stack decoding algorithm to quickly find a complete parse for the sentence .	S-136
OWN	Once the stack decoder has found a complete parse of reasonable probability () , it switches to a breadth-first mode to pursue all of the partial parses which have not been explored by the stack decoder .	S-137
OWN	In this second mode , it can safely discard any partial parse which has a probability lower than the probability of the highest probability completed parse .	S-138
OWN	Using these two search modes , SPATTER guarantees that it will find the highest probability parse .	S-139
OWN	The only limitation of this search technique is that , for sentences which are modeled poorly , the search might exhaust the available memory before completing both phases .	S-140
OWN	However , these search errors conveniently occur on sentences which SPATTER is likely to get wrong anyway , so there isn't much performance lossed due to the search errors .	S-141
OWN	Experimentally , the search algorithm guarantees the highest probability parse is found for over 96 % of the sentences parsed .	S-142
OWN	In the absence of an NL system , SPATTER can be evaluated by comparing its top-ranking parse with the treebank analysis for each test sentence .	S-143
OWN	The parser was applied to two different domains , IBM Computer Manuals and the Wall Street Journal .	S-144
BAS	The first experiment uses the IBM Computer Manuals domain , which consists of sentences extracted from IBM computer manuals .	S-145
OTH	The training and test sentences were annotated by the University of Lancaster .	S-146
OTH	The Lancaster treebank uses 195 part-of-speech tags and 19 non-terminal labels .	S-147
OTH	This treebank is described in great detail in.	S-148
OWN	The main reason for applying SPATTER to this domain is that IBM had spent the previous ten years developing a rule-based , unification-style probabilistic context-free grammar for parsing this domain .	S-149
OWN	The purpose of the experiment was to estimate SPATTER 's ability to learn the syntax for this domain directly from a treebank , instead of depending on the interpretive expertise of a grammarian .	S-150
OWN	The parser was trained on the first 30,800 sentences from the Lancaster treebank .	S-151
OWN	The test set included 1,473 new sentences , whose lengths range from 3 to 30 words , with a mean length of 13.7 words .	S-152
OWN	These sentences are the same test sentences used in the experiments reported for IBM 's parser in.	S-153
OWN	In, IBM 's parser was evaluated using the 0-crossing - brackets measure , which represents the percentage of sentences for which none of the constituents in the parser 's parse violates the constituent boundaries of any constituent in the correct parse .	S-154
CTR	After over ten years of grammar development , the IBM parser achieved a 0-crossing - brackets score of 69 % .	S-155
CTR	On this same test set , SPATTER scored 76 % .	S-156
OWN	The experiment is intended to illustrate SPATTER 's ability to accurately parse a highly-ambiguous , large-vocabulary domain .	S-157
OWN	These experiments use the Wall Street Journal domain , as annotated in the Penn Treebank , version 2 .	S-158
OWN	The Penn Treebank uses 46 part-of-speech tags and 27 non-terminal labels .	S-159
OWN	The WSJ portion of the Penn Treebank is divided into 25 sections , numbered 00 - 24 .	S-160
OWN	In these experiments , SPATTER was trained on sections 02 - 21 , which contains approximately 40,000 sentences .	S-161
OWN	The test results reported here are from section 00 , which contains 1920 sentences .	S-162
OWN	Sections 01 , 22 , 23 , and 24 will be used as test data in future experiments .	S-163
OWN	The Penn Treebank is already tokenized and sentence detected by human annotators , and thus the test results reported here reflect this .	S-164
OWN	SPATTER parses word sequences , not tag sequences .	S-165
OWN	Furthermore , SPATTER does not simply pre-tag the sentences and use only the best tag sequence in parsing .	S-166
OWN	Instead , it uses a probabilistic model to assign tags to the words , and considers all possible tag sequences according to the probability they are assigned by the model .	S-167
OWN	No information about the legal tags for a word are extracted from the test corpus .	S-168
OWN	In fact , no information other than the words is used from the test corpus .	S-169
OWN	For the sake of efficiency , only the sentences of 40 words or fewer are included in these experiments .	S-170
OWN	For this test set , SPATTER takes on average 12 seconds per sentence on an SGI R4400 with 160 megabytes of RAM .	S-171
OWN	To evaluate SPATTER 's performance on this domain , I am using the PARSEVAL measures , as defined in.	S-172
OWN	Precision	S-173
OWN	Recall	S-174
OWN	Crossing Brackets	S-175
OWN	no . of constituents which violate constituent boundaries with a constituent in the treebank parse .	S-176
OWN	The precision and recall measures do not consider constituent labels in their evaluation of a parse , since the treebank label set will not necessarily coincide with the labels used by a given grammar .	S-177
OWN	Since SPATTER uses the same syntactic label set as the Penn Treebank , it makes sense to report labelled precision and labelled recall .	S-178
OWN	These measures are computed by considering a constituent to be correct if and only if it 's label matches the label in the treebank .	S-179
OWN	Tableshows the results of SPATTER evaluated against the Penn Treebank on the Wall Street Journal section 00 .	S-180
OWN	Figures,, andillustrate the performance of SPATTER as a function of sentence length .	S-181
OWN	SPATTER 's performance degrades slowly for sentences up to around 28 words , and performs more poorly and more erratically as sentences get longer .	S-182
OWN	Figureindicates the frequency of each sentence length in the test corpus .	S-183
OWN	Regardless of what techniques are used for parsing disambiguation , one thing is clear : if a particular piece of information is necessary for solving a disambiguation problem , it must be made available to the disambiguation mechanism .	S-184
OWN	The words in the sentence are clearly necessary to make parsing decisions , and in some cases long-distance structural information is also needed .	S-185
CTR	Statistical models for parsing need to consider many more features of a sentence than can be managed by n-gram modeling techniques and many more examples than a human can keep track of .	S-186
AIM	The SPATTER parser illustrates how large amounts of contextual information can be incorporated into a statistical model for parsing by applying decision-tree learning algorithms to a large annotated corpus .	S-187
