<?xml version='1.0' encoding='ISO-8859-1'?>
<!DOCTYPE PAPER SYSTEM "paper-structure.dtd">
<PAPER>
<METADATA>
<FILENO>9503015</FILENO>
<APPEARED><CONFERENCE>EACL</CONFERENCE><YEAR>1995</YEAR></APPEARED>
<CLASSIFICATION> Lg.Pr.Gr.Ps </CLASSIFICATION>
</METADATA>
<TITLE> Incremental Interpretation of Categorial Grammar </TITLE>
<AUTHORLIST>
<AUTHOR>David Milward</AUTHOR>
</AUTHORLIST>
<ABSTRACT>
<A-S ID='A-0' DOCUMENTC='S-163' AZ='AIM'> The paper describes a parser for Categorial Grammar which provides fully word by word incremental interpretation . </A-S>
<A-S ID='A-1' AZ='OWN'> The parser does not require fragments of sentences to form constituents , and thereby avoids problems of spurious ambiguity . </A-S>
<A-S ID='A-2' AZ='OWN'> The paper includes a brief discussion of the relationship between basic Categorial Grammar and other formalisms such as HPSG , Dependency Grammar and the Lambek Calculus . </A-S>
<A-S ID='A-3' AZ='OWN'> It also includes a discussion of some of the issues which arise when parsing lexicalised grammars , and the possibilities for using statistical techniques for tuning to particular languages . </A-S>
</ABSTRACT>
<BODY>
<DIV DEPTH='1'>
<HEADER ID='H-0'> Introduction </HEADER>
<P>
<S ID='S-0' AZ='BKG'> There is a large body of psycholinguistic evidence which suggests that meaning can be extracted before the end of a sentence , and before the end of phrasal constituents <REF TYPE='P'>Marslen-Wilson 1973</REF> , <REF TYPE='P'>Tanenhaus et al. 1990</REF> . </S>
<S ID='S-1' AZ='BKG'> There is also recent evidence suggesting that , during speech processing , partial interpretations can be built extremely rapidly , even before words are completed <REF TYPE='P'>Spivey-Knowlton et al. 1994</REF> . </S>
<S ID='S-2' AZ='BKG'> There are also potential computational applications for incremental interpretation , including early parse filtering using statistics based on logical form plausibility , and interpretation of fragments of dialogues ( a survey is provided by <REF TYPE='A' SELF="YES">Milward and Cooper 1994</REF> , henceforth referred to as <REFAUTHOR>M and C</REFAUTHOR> ) . </S>
</P>
<P>
<S ID='S-3' AZ='BKG'> In the current computational and psycholinguistic literature there are two main approaches to the incremental construction of logical forms . </S>
<S ID='S-4' AZ='OTH'> One approach is to use a grammar with ` non-standard ' constituency , so that an initial fragment of a sentence , such as John likes , can be treated as a constituent , and hence be assigned a type and a semantics . </S>
<S ID='S-5' AZ='OTH'> This approach is exemplified by Combinatory Categorial Grammar , CCG <REF TYPE='P'>Steedman 1991</REF> , which takes a basic CG with just application , and adds various new ways of combining elements together . </S>
<S ID='S-6' AZ='OTH'> Incremental interpretation can then be achieved using a standard bottom-up shift reduce parser , working from left to right along the sentence . </S>
<S ID='S-7' AZ='OTH'> The alternative approach , exemplified by the work of <REFAUTHOR>Stabler</REFAUTHOR> on top-down parsing <REF TYPE='P'>Stabler 1991</REF> , and <REFAUTHOR>Pulman</REFAUTHOR> on left-corner parsing <REF TYPE='P'>Pulman 1986</REF> is to associate a semantics directly with the partial structures formed during a top-down or left-corner parse . </S>
<S ID='S-8' AZ='OTH'> For example , a syntax tree missing a noun phrase , such as the following  </S>
</P>
<IMAGE ID='I-0'/>
<P>
<S ID='S-9' AZ='OTH'> can be given a semantics as a function from entities to truth values i.e. <EQN/> x. likes ( john , x ) , without having to say that John likes is a constituent . </S>
</P>
<P>
<S ID='S-10' AZ='CTR'> Neither approach is without problems . </S>
<S ID='S-11' AZ='CTR'> If a grammar is augmented with operations which are powerful enough to make most initial fragments constituents , then there may be unwanted interactions with the rest of the grammar ( examples of this in the case of CCG and the Lambek Calculus are given in Section <CREF/> ) . </S>
<S ID='S-12' AZ='CTR'> The addition of extra operations also means that , for any given reading of a sentence there will generally be many different possible derivations ( so-called ` spurious ' ambiguity ) , making simple parsing strategies such as shift-reduce highly inefficient . </S>
</P>
<P>
<S ID='S-13' AZ='CTR'> The limitations of the parsing approaches become evident when we consider grammars with left recursion . </S>
<S ID='S-14' AZ='CTR'> In such cases a simple top-down parser will be incomplete , and a left corner parser will resort to buffering the input ( so won't be fully word-by-word ) . </S>
<S ID='S-15' AZ='OTH'> <REFAUTHOR>M and C</REFAUTHOR> illustrate the problem by considering the fragment Mary thinks John . </S>
<S ID='S-16' AZ='OTH'> This has a small number of possible semantic representations ( the exact number depending upon the grammar ) e.g. </S>
</P>
<IMAGE ID='I-1'/>
<P>
<S ID='S-17' AZ='OTH'> The second representation is appropriate if the sentence finishes with a sentential modifier . </S>
<S ID='S-18' AZ='OTH'> The third allows there to be a verb phrase modifier . </S>
</P>
<P>
<S ID='S-19' AZ='BKG'> If the semantic representation is to be read off syntactic structure , then the parser must provide a single syntax tree ( possibly with empty nodes ) . </S>
<S ID='S-20' AZ='BKG'> However , there are actually any number of such syntax trees corresponding to , for example , the first semantic representation , since the np and the s can be arbitrarily far apart . </S>
<S ID='S-21' AZ='BKG'> The following tree is suitable for the sentence Mary thinks John shaves but not for e.g. Mary thinks John coming here was a mistake . </S>
</P>
<IMAGE ID='I-2'/>
<P>
<S ID='S-22' AZ='OTH'> <REFAUTHOR>M and C</REFAUTHOR> suggest various possibilities for packing the partial syntax trees , including using Tree Adjoining Grammar <REF TYPE='P'>Joshi 1987</REF> or Description Theory <REF TYPE='P'>Marcus et al. 1983</REF> . </S>
<S ID='S-23' AZ='OTH'> One further possibility is to choose a single syntax tree , and to use destructive tree operations later in the parse . </S>
</P>
<P>
<S ID='S-24' AZ='BAS'> The approach which we will adopt here is based on <REF  SELF="YES" TYPE='A'>Milward 1992</REF>, <REF  SELF="YES" TYPE='A'>Milward 1994a</REF> . </S>
<S ID='S-25' AZ='OWN'> Partial syntax trees can be regarded as performing two main roles . </S>
<S ID='S-26' AZ='OWN'> The first is to provide syntactic information which guides how the rest of the sentence can be integrated into the tree . </S>
<S ID='S-27' AZ='OWN'> The second is to provide a basis for a semantic representation . </S>
<S ID='S-28' AZ='OWN'> The first role can be captured using syntactic types , where each type corresponds to a potentially infinite number of partial syntax trees . </S>
<S ID='S-29' AZ='OWN'> The second role can be captured by the parser constructing semantic representations directly . </S>
<S ID='S-30' AZ='OWN'> The general processing model therefore consists of transitions of the form : </S>
</P>
<IMAGE ID='I-3'/>
<P>
<S ID='S-31' AZ='OWN'> This provides a state-transition or dynamic model of processing , with each state being a pair of a syntactic type and a semantic value . </S>
</P>
<P>
<S ID='S-32' AZ='CTR'> The main difference between our approach and that of <REF TYPE='A' SELF="YES">Milward 1992</REF> , <REF TYPE='A' SELF="YES">Milward 1994a</REF> is that it is based on a more expressive grammar formalism , Applicative Categorial Grammar , as opposed to Lexicalised Dependency Grammar . </S>
<S ID='S-33' AZ='OTH'> Applicative Categorial Grammars allow categories to have arguments which are themselves functions ( e.g. very can be treated as a function of a function , and given the type (n/n)/(n/n) when used as an adjectival modifier ) . </S>
<S ID='S-34' AZ='OTH'> The ability to deal with functions of functions has advantages in enabling more elegant linguistic descriptions , and in providing one kind of robust parsing : the parser never fails until the last word , since there could always be a final word which is a function over all the constituents formed so far . </S>
<S ID='S-35' AZ='CTR'> However , there is a corresponding problem of far greater non-determinism , with even unambiguous words allowing many possible transitions . </S>
<S ID='S-36' AZ='OWN'> It therefore becomes crucial to either perform some kind of ambiguity packing , or language tuning . </S>
<S ID='S-37' AZ='TXT'> This will be discussed in the final section of the paper . </S>
</P>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-1'> Applicative Categorial Grammar </HEADER>
<P>
<S ID='S-38' AZ='OTH'> Applicative Categorial Grammar is the most basic form of Categorial Grammar , with just a single combination rule corresponding to function application . </S>
<S ID='S-39' AZ='OTH'> It was first applied to linguistic description by <REFAUTHOR>Adjukiewicz</REFAUTHOR> and <REFAUTHOR>Bar-Hillel</REFAUTHOR> in the 1950s . </S>
<S ID='S-40' AZ='OTH'> Although it is still used for linguistic description <REF TYPE='P'>Bouma and van Noord 1994</REF> , it has been somewhat overshadowed in recent years by HPSG <REF TYPE='P'>Pollard and Sag 1994</REF> , and by Lambek Categorial Grammars <REF TYPE='P'>Lambek 1958</REF> . </S>
<S ID='S-41' AZ='OTH'> It is therefore worth giving some brief indications of how it fits in with these developments . </S>
</P>
<P>
<S ID='S-42' AZ='OTH'> The first directed Applicative CG was proposed by <REF TYPE='A'>Bar-Hillel 1953</REF> . </S>
<S ID='S-43' AZ='OTH'> Functional types included a list of arguments to the left , and a list of arguments to the right . </S>
<S ID='S-44' AZ='OTH'> Translating <REFAUTHOR>Bar-Hillel</REFAUTHOR> 's notation into a feature based notation similar to that in HPSG <REF TYPE='P'>Pollard and Sag 1994</REF> , we obtain the following category for a ditransitive verb such as put : </S>
</P>
<IMAGE ID='I-4'/>
<P>
<S ID='S-45' AZ='OTH'> The list of arguments to the left are gathered under the feature , l , and those to the right , an np and a pp in that order , under the feature r . </S>
<S ID='S-46' AZ='OTH'> <REFAUTHOR>Bar-Hillel</REFAUTHOR> employed a single application rule , which corresponds to the following : </S>
</P>
<IMAGE ID='I-5'/>
<P>
<S ID='S-47' AZ='OTH'> The result was a system which comes very close to the formalised dependency grammars of <REF TYPE='A'>Gaifman 1965</REF> and <REF TYPE='A'>Hays 1964</REF> . </S>
<S ID='S-48' AZ='OTH'> The only real difference is that <REFAUTHOR>Bar-Hillel</REFAUTHOR> allowed arguments to themselves be functions . </S>
<S ID='S-49' AZ='OTH'> For example , an adverb such as slowly could be given the type </S>
</P>
<IMAGE ID='I-6'/>
<P>
<S ID='S-50' AZ='CTR'> An unfortunate aspect of <REFAUTHOR>Bar-Hillel</REFAUTHOR> 's first system was that the application rule only ever resulted in a primitive type . </S>
<S ID='S-51' AZ='CTR'> Hence , arguments with functional types had to correspond to single lexical items : there was no way to form the type np <EQN/> s for a non-lexical verb phrase such as likes Mary . </S>
</P>
<P>
<S ID='S-52' AZ='OTH'> Rather than adapting the Application Rule to allow functions to be applied to one argument at a time , <REFAUTHOR>Bar-Hillel</REFAUTHOR> 's second system ( often called AB Categorial Grammar , or <REFAUTHOR>Adjukiewicz</REFAUTHOR>  / <REFAUTHOR>Bar-Hillel</REFAUTHOR> CG , <REF TYPE='P'>Bar-Hillel 1964</REF> ) adopted a ` Curried ' notation , and this has been adopted by most CGs since . </S>
<S ID='S-53' AZ='OTH'> To represent a function which requires an np on the left , and an np and a pp to the right , there is a choice of the following three types using Curried notation : </S>
</P>
<IMAGE ID='I-7'/>
<IMAGE ID='I-8'/>
<IMAGE ID='I-9'/>
<P>
<S ID='S-54' AZ='OTH'> Most CGs either choose the third of these ( to give a vp structure ) , or include a rule of Associativity which means that the types are interchangeable ( in the Lambek Calculus , Associativity is a consequence of the calculus , rather than being specified separately ) . </S>
<S ID='S-55' AZ='OTH'> The main impetus to change Applicative CG came from the work of <REF TYPE='A'>Ades and Steedman 1982</REF> . </S>
<S ID='S-56' AZ='OTH'> <REFAUTHOR>Ades and Steedman</REFAUTHOR> noted that the use of function composition allows CGs to deal with unbounded dependency constructions . </S>
<S ID='S-57' AZ='OTH'> Function composition enables a function to be applied to its argument , even if that argument is incomplete e.g. </S>
</P>
<IMAGE ID='I-10'/>
<P>
<S ID='S-58' AZ='OTH'> This allows peripheral extraction , where the ` gap ' is at the start or the end of e.g. a relative clause . </S>
<S ID='S-59' AZ='OTH'> Variants of the composition rule were proposed in order to deal with non-peripheral extraction , but this led to unwanted effects elsewhere in the grammar <REF TYPE='P'>Bouma 1987</REF> . </S>
<S ID='S-60' AZ='OTH'> Subsequent treatments of non-peripheral extraction based on the Lambek Calculus ( where standard composition is built in : it is a rule which can be proven from the calculus ) have either introduced an alternative to the forward and backward slashes i.e. / and <EQN/> for normal args , <EQN/> for wh-args <REF TYPE='P'>Moortgat 1988</REF> , or have introduced so called modal operators on the wh-argument <REF TYPE='P'>Morrill et al. 1990</REF> . </S>
<S ID='S-61' AZ='OTH'> Both techniques can be thought of as marking the wh-arguments as requiring special treatment , and therefore do not lead to unwanted effects elsewhere in the grammar . </S>
</P>
<P>
<S ID='S-62' AZ='CTR'> However , there are problems with having just composition , the most basic of the non-applicative operations . </S>
<S ID='S-63' AZ='OTH'> In CGs which contain functions of functions ( such as very , or slowly ) , the addition of composition adds both new analyses of sentences , and new strings to the language . </S>
<S ID='S-64' AZ='OTH'> This is due to the fact that composition can be used to form a function , which can then be used as an argument to a function of a function . </S>
<S ID='S-65' AZ='OTH'> For example , if the two types , n / n and n / n are composed to give the type n / n , then this can be modified by an adjectival modifier of type ( n / n ) / ( n / n ) . </S>
<S ID='S-66' AZ='CTR'> Thus , the noun very old dilapidated car can get the unacceptable bracketing , [ [ very [ old dilapidated ] ] car ] . </S>
<S ID='S-67' AZ='OTH'> Associative CGs with Composition , or the Lambek Calculus also allow strings such as boy with the to be given the type n / n predicting very boy with the car to be an acceptable noun . </S>
<S ID='S-68' AZ='OTH'> Although individual examples might be possible to rule out using appropriate features , it is difficult to see how to do this in general whilst retaining a calculus suitable for incremental interpretation . </S>
</P>
<P>
<S ID='S-69' AZ='OWN'> If wh-arguments need to be treated specially anyway ( to deal with non-peripheral extraction ) , and if composition as a general rule is problematic , this suggests we should perhaps return to grammars which use just Application as a general operation , but have a special treatment for wh-arguments . </S>
<S ID='S-70' AZ='OWN'> Using the non-Curried notation of <REFAUTHOR>Bar-Hillel</REFAUTHOR> , it is more natural to use a separate wh-list than to mark wh-arguments individually . </S>
<S ID='S-71' AZ='OWN'> For example , the category appropriate for relative clauses with a noun phrase gap would be : </S>
</P>
<IMAGE ID='I-11'/>
<P>
<S ID='S-72' AZ='OWN'> It is then possible to specify operations which act as purely applicative operations with respect to the left and right arguments lists , but more like composition with respect to the wh-list . </S>
<S ID='S-73' AZ='OTH'> This is very similar to the way in which wh-movement is dealt with in GPSG <REF TYPE='P'>Gazdar et al. 1985</REF> and HPSG , where wh-arguments are treated using slash mechanisms or feature inheritance principles which correspond closely to function composition . </S>
</P>
<P>
<S ID='S-74' AZ='CTR'> Given that our arguments have produced a categorial grammar which looks very similar to HPSG , why not use HPSG rather than Applicative CG ? </S>
<S ID='S-75' AZ='CTR'> The main reason is that Applicative CG is a much simpler formalism , which can be given a very simple syntax semantics interface , with function application in syntax mapping to function application in semantics <EQN/> . </S>
<S ID='S-76' AZ='OWN'> This in turn makes it relatively easy to provide proofs of soundness and completeness for an incremental parsing algorithm . </S>
<S ID='S-77' AZ='OWN'> Ultimately , some of the techniques developed here should be able to be extended to more complex formalisms such as HPSG . </S>
</P>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-2'> AB Categorial grammar with Associativity ( AACG ) </HEADER>
<P>
<S ID='S-78' AZ='BAS'> In this section we define a grammar similar to <REFAUTHOR>Bar-Hillel</REFAUTHOR> 's first grammar . </S>
<S ID='S-79' AZ='CTR'> However , unlike <REFAUTHOR>Bar-Hillel</REFAUTHOR> , we allow one argument to be absorbed at a time . </S>
<S ID='S-80' AZ='OWN'> The resulting grammar is equivalent to AB Categorial Grammar plus associativity . </S>
</P>
<P>
<S ID='S-81' AZ='OWN'> The categories of the grammar are defined as follows : </S>
</P>
<P>
<S ID='S-82' TYPE='ITEM' AZ='OWN' > If X is a syntactic type ( e.g. s , np ) , then <EQN/> is a category . </S>
<S ID='S-83' TYPE='ITEM' AZ='OWN' > If X is a syntactic type , and L and R are lists of categories , then <EQN/> is a category . </S>
</P>
<P>
<S ID='S-84' AZ='OWN'> Application to the right is defined by the rule : </S>
</P>
<IMAGE ID='I-12'/>
<P>
<S ID='S-85' AZ='OWN'> Application to the left is defined by the rule : </S>
</P>
<IMAGE ID='I-13'/>
<P>
<S ID='S-86' AZ='OWN'> The basic grammar provides some spurious derivations , since sentences such as John likes Mary can be bracketed as either ( ( John likes ) Mary ) or ( John ( likes Mary ) ) . </S>
<S ID='S-87' AZ='OWN'> However , we will see that these spurious derivations do not translate into spurious ambiguity in the parser , which maps from strings of words directly to semantic representations . </S>
</P>
<IMAGE ID='I-14'/>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-3'> An Incremental Parser </HEADER>
<P>
<S ID='S-88' AZ='OWN'> Most parsers which work left to right along an input string can be described in terms of state transitions i.e. by rules which say how the current parsing state ( e.g. a stack of categories , or a chart ) can be transformed by the next word into a new state . </S>
<S ID='S-89' AZ='OWN'> Here this will be made particularly explicit , with the parser described in terms of just two rules which take a state , a new word and create a new state . </S>
<S ID='S-90' AZ='OWN'> There are two unusual features . </S>
<S ID='S-91' AZ='OWN'> Firstly , there is nothing equivalent to a stack mechanism : at all times the state is characterised by a single syntactic type , and a single semantic value , not by some stack of semantic values or syntax trees which are waiting to be connected together . </S>
<S ID='S-92' AZ='OWN'> Secondly , all transitions between states occur on the input of a new word : there are no ` empty ' transitions ( such as the reduce step of a shift-reduce parser ) . </S>
</P>
<P>
<S ID='S-93' AZ='OWN'> The two rules , which are given in Figure <CREF/> , are difficult to understand in their most general form . </S>
<S ID='S-94' AZ='OWN'> Here we will work upto the rules gradually , by considering which kinds of rules we might need in particular instances . </S>
<S ID='S-95' AZ='OWN'> Consider the following pairing of sentence fragments with their simplest possible CG type : </S>
</P>
<IMAGE ID='I-15'/>
<P>
<S ID='S-96' AZ='OWN'> Now consider taking each type as a description of the state that the parser is in after absorbing the fragment . </S>
<S ID='S-97' AZ='OWN'> We obtain a sequence of transitions as follows : </S>
</P>
<IMAGE ID='I-16'/>
<P>
<S ID='S-98' AZ='OWN'> If an embedded sentence such as John likes Sue is a mapping from an s / s to an s , this suggests that it might be possible to treat all sentences as mapping from some category expecting an s to that category i.e. from X / s to X . </S>
<S ID='S-99' AZ='OWN'>  Similarly , all noun phrases might be treated as mappings from an X / np to an X . </S>
<S ID='S-100' AZ='OWN'> Now consider individual transitions . </S>
<S ID='S-101' AZ='OWN'> The simplest of these is where the type of argument expected by the state is matched by the next word i.e. </S>
</P>
<IMAGE ID='I-17'/>
<P>
<S ID='S-102' AZ='OWN'>  This can be generalised to the following rule , which is similar to Function Application in standard CG </S>
</P>
<IMAGE ID='I-18'/>
<P>
<S ID='S-103' AZ='OWN'> A similar transition occurs for likes . </S>
<S ID='S-104' AZ='OWN'> Here an np <EQN/> s was expected , but likes only provides part of this : it requires an np to the right to form an np <EQN/> s . </S>
<S ID='S-105' AZ='OWN'> Thus after likes is absorbed the state category will need to expect an np . </S>
<S ID='S-106' AZ='OWN'> The rule required is similar to Function Composition in CG i.e. </S>
</P>
<IMAGE ID='I-19'/>
<IMAGE ID='I-20'/>
<P>
<S ID='S-107' AZ='OWN'> Considering this informally in terms of tree structures , what is happening is the replacement of an empty node in a partial tree by a second partial tree i.e. </S>
</P>
<IMAGE ID='I-21'/>
<P>
<S ID='S-108' AZ='OWN'> The two rules specified so far need to be further generalised to allow for the case where a lexical item has more than one argument ( e.g. if we replace likes by a di-transitive such as gives or a tri-transitive such as bets ) . </S>
<S ID='S-109' AZ='OWN'> This is relatively trivial using a non-curried notation similar to that used for AACG . </S>
<S ID='S-110' AZ='OWN'> What we obtain is the single rule of State-Application , which corresponds to application when the list of arguments , R <EQN/> , is empty , to function composition when R <EQN/> is of length one , and to n-ary composition when R <EQN/> is of length n . </S>
<S ID='S-111' AZ='OWN'> The only change needed from AACG notation is the inclusion of an extra feature list , the h list , which stores information about which arguments are waiting for a head ( the reasons for this will be explained later ) . </S>
<S ID='S-112' AZ='OWN'> The lexicon is identical to that for a standard AACG , except for having h-lists which are always set to empty . </S>
</P>
<P>
<S ID='S-113' AZ='OWN'> Now consider the first transition . </S>
<S ID='S-114' AZ='OWN'> Here a sentence was expected , but what was encountered was a noun phrase , John . </S>
<S ID='S-115' AZ='OWN'> The appropriate rule in CG notation would be : </S>
</P>
<IMAGE ID='I-22'/>
<P>
<S ID='S-116' AZ='OWN'> This rule states that if looking for a Y and get a Z then look for a Y which is missing a Z . </S>
<S ID='S-117' AZ='OWN'> In tree structure terms we have : </S>
</P>
<IMAGE ID='I-23'/>
<P>
<S ID='S-118' AZ='OWN'> The rule of State-Prediction is obtained by further generalising to allow the lexical item to have missing arguments , and for the expected argument to have missing arguments . </S>
</P>
<P>
<S ID='S-119' AZ='OWN'> State-Application and State-Prediction together provide the basis of a sound and complete parser . </S>
<S ID='S-120' AZ='OWN'> Parsing of sentences is achieved by starting in a state expecting a sentence , and applying the rules non-deterministically as each word is input . </S>
<S ID='S-121' AZ='OWN'> A successful parse is achieved if the final state expects no more arguments . </S>
<S ID='S-122' AZ='OWN'> As an example , reconsider the string John likes Sue . </S>
<S ID='S-123' AZ='OWN'> The sequence of transitions corresponding to John likes Sue being a sentence , is given in Figure <CREF/> . </S>
<S ID='S-124' AZ='OWN'> The transition on encountering John is deterministic : State-Application cannot apply , and State-Prediction can only be instantiated one way . </S>
<S ID='S-125' AZ='OWN'> The result is a new state expecting an argument which , given an np could give an s i.e. an np <EQN/> s . </S>
</P>
<IMAGE ID='I-24'/>
<P>
<S ID='S-126' AZ='OWN'> The transition on input of likes is non-deterministic . </S>
<S ID='S-127' AZ='OWN'> State-Application can apply , as in Figure <CREF/> . </S>
<S ID='S-128' AZ='OWN'> However , State-Prediction can also apply , and can be instantiated in four ways ( these correspond to different ways of cutting up the left and right subcategorisation lists of the lexical entry , likes , i.e. as <EQN/> np <EQN/> or <EQN/> np <EQN/> ) . </S>
<S ID='S-129' AZ='OWN'> One possibility corresponds to the prediction of an s <EQN/> s modifier , a second to the prediction of an ( np <EQN/> s ) <EQN/> ( np <EQN/> s ) modifier ( i.e. a verb phrase modifier ) , a third to there being a function which takes the subject and the verb as separate arguments , and the fourth corresponds to there being a function which requires an s / np argument . </S>
<S ID='S-130' AZ='OWN'> The second of these is perhaps the most interesting , and is given in Figure <CREF/> . </S>
<S ID='S-131' AZ='OWN'> It is the choice of this particular transition at this point which allows verb phrase modification , and hence , assuming the next word is Sue , an implicit bracketing of the string fragment as ( John ( likes Sue ) ) . </S>
<S ID='S-132' AZ='OWN'> Note that if State-Application is chosen , or the first of the State-Prediction possibilities , the fragment John likes Sue retains a flat structure . </S>
<S ID='S-133' AZ='OWN'> If there is to be no modification of the verb phrase , no verb phrase structure is introduced . </S>
<S ID='S-134' AZ='OWN'> This relates to there being no spurious ambiguity : each choice of transition has semantic consequences ; each choice affects whether a particular part of the semantics is to be modified or not . </S>
</P>
<P>
<S ID='S-135' AZ='OWN'> Finally , it is worth noting why it is necessary to use h-lists . </S>
<S ID='S-136' AZ='OWN'> These are needed to distinguish between cases of real functional arguments ( of functions of functions ) , and functions formed by State-Prediction . </S>
<S ID='S-137' AZ='OWN'> Consider the following trees , where the np <EQN/> s node is empty . </S>
</P>
<IMAGE ID='I-25'/>
<P>
<S ID='S-138' AZ='OWN'> Both trees have the same syntactic type , however in the first case we want to allow for there to be an s <EQN/> s modifier of the lower s , but not in the second . </S>
<S ID='S-139' AZ='OWN'> The headed list distinguishes between the two cases , with only the first having an np on its headed list , allowing prediction of an s modifier . </S>
</P>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-4'> Parsing Lexicalised Grammars </HEADER>
<P>
<S ID='S-140' AZ='OWN'> When we consider full sentence processing , as opposed to incremental processing , the use of lexicalised grammars has a major advantage over the use of more standard rule based grammars . </S>
<S ID='S-141' AZ='OWN'> In processing a sentence using a lexicalised formalism we do not have to look at the grammar as a whole , but only at the grammatical information indexed by each of the words . </S>
<S ID='S-142' AZ='OWN'> Thus increases in the size of a grammar don't necessarily effect efficiency of processing , provided the increase in size is due to the addition of new words , rather than increased lexical ambiguity . </S>
<S ID='S-143' AZ='OWN'> Once the full set of possible lexical entries for a sentence is collected , they can , if required , then be converted back into a set of phrase structure rules ( which should correspond to a small subset of the rule based formalism equivalent to the whole lexicalised grammar ) , before being parsing with a standard algorithm such as <REF TYPE='P'>Earley 1970</REF> 's . </S>
</P>
<P>
<S ID='S-144' AZ='OWN'> In incremental parsing we cannot predict which words will appear in the sentence , so cannot use the same technique . </S>
<S ID='S-145' AZ='OWN'> However , if we are to base a parser on the rules given above , it would seem that we gain further . </S>
<S ID='S-146' AZ='OWN'> Instead of grammatical information being localised to the sentence as a whole , it is localised to a particular word in its particular context : there is no need to consider a pp as a start of a sentence if it occurs at the end , even if there is a verb with an entry which allows for a subject pp . </S>
</P>
<P>
<S ID='S-147' AZ='OWN'> However there is a major problem . </S>
<S ID='S-148' AZ='OWN'> As we noted in the last paragraph , it is the nature of parsing incrementally that we don't know what words are to come next . </S>
<S ID='S-149' AZ='OWN'> But here the parser doesn't even use the information that the words are to come from a lexicon for a particular language . </S>
<S ID='S-150' AZ='OWN'> For example , given an input of 3 nps , the parser will happily create a state expecting 3 nps to the left . </S>
<S ID='S-151' AZ='OWN'> This might be a likely state for say a head final language , but an unlikely state for a language such as English . </S>
<S ID='S-152' AZ='OWN'> Note that incremental interpretation will be of no use here , since the semantic representation should be no more or less plausible in the different languages . </S>
<S ID='S-153' AZ='OWN'> In practical terms , a naive interactive parallel Prolog implementation on a current workstation fails to be interactive in a real sense after about 8 words . </S>
</P>
<P>
<S ID='S-154' AZ='OWN'> What seems to be needed is some kind of language tuning . </S>
<S ID='S-155' AZ='OWN'> This could be in the nature of fixed restrictions to the rules e.g. for English we might rule out uses of prediction when a noun phrase is encountered , and two already exist on the left list . </S>
<S ID='S-156' AZ='OWN'> A more appealing alternative is to base the tuning on statistical methods . </S>
<S ID='S-157' AZ='OWN'> This could be achieved by running the parser over corpora to provide probabilities of particular transitions given particular words . </S>
<S ID='S-158' AZ='OWN'> These transitions would capture the likelihood of a word having a particular part of speech , and the probability of a particular transition being performed with that part of speech . </S>
</P>
<P>
<S ID='S-159' AZ='OWN'> There has already been some early work done on providing statistically based parsing using transitions between recursively structured syntactic categories <REF TYPE='P'>Tugwell 1995</REF> . </S>
<S ID='S-160' AZ='OWN'> Unlike a simple Markov process , there are a potentially infinite number of states , so there is inevitably a problem of sparse data . </S>
<S ID='S-161' AZ='OWN'> It is therefore necessary to make various generalisations over the states , for example by ignoring the R <EQN/> lists . </S>
</P>
<P>
<S ID='S-162' AZ='OWN'> The full processing model can then be either serial , exploring the most highly ranked transitions first ( but allowing backtracking if the semantic plausibility of the current interpretation drops too low ) , or ranked parallel , exploring just the n paths ranked highest according to the transition probabilities and semantic plausibility . </S>
</P>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-5'> Conclusion </HEADER>
<P>
<S ID='S-163' ABSTRACTC='A-0' AZ='AIM'> The paper has presented a method for providing interpretations word by word for basic Categorial Grammar . </S>
<S ID='S-164' AZ='OWN'> The final section contrasted parsing with lexicalised and rule based grammars , and argued that statistical language tuning is particularly suitable for incremental , lexicalised parsing strategies . </S>
</P>
</DIV>
</BODY>
<REFERENCELIST>
<REFERENCE>
<SURNAME>Ades</SURNAME>, A. amp; Steedman, M.: <DATE>1972</DATE>, `On the Order of Words', Linguistics
amp; Philosophy 4, 517-558.</REFERENCE>
<REFERENCE>
<SURNAME>Bar-Hillel</SURNAME>, Y.: <DATE>1953</DATE>, `A Quasi-Arithmetical Notation for Syntactic
Description', Language 29, 47-58.</REFERENCE>
<REFERENCE>
<SURNAME>Bar-Hillel</SURNAME>, Y.: <DATE>1964</DATE>, Language amp; Information: Selected Essays
on Their Theory amp; Application, Addison-Wesley.</REFERENCE>
<REFERENCE>
<SURNAME>Bouma</SURNAME>, G.: <DATE>1987</DATE>, `A Unification-Based Analysis of Unbounded Dependencies',
in Proceedings of the 6th Amsterdam Colloquium, ITLI, University
of Amsterdam. </REFERENCE>
<REFERENCE>
<SURNAME>Bouma</SURNAME>, G. amp; van Noord, G.: <DATE>1994</DATE>, `Constraint-Based Categorial Grammar',
in Proceedings of the 32nd ACL, Las Cruces, U.S.A. </REFERENCE>
<REFERENCE>
<SURNAME>Earley</SURNAME>, J.: <DATE>1970</DATE>, `An Efficient Context-free Parsing Algorithm', 
ACM Communications 13(2), 94-102.</REFERENCE>
<REFERENCE>
<SURNAME>Gaifman</SURNAME>, H.: <DATE>1965</DATE>, `Dependency Systems amp; Phrase Structure Systems',
Information amp; Control 8: 304-337.</REFERENCE>
<REFERENCE>
<SURNAME>Gazdar</SURNAME>, G., <SURNAME>Klein</SURNAME>, E., <SURNAME>Pullum</SURNAME>, G.K., amp; Sag, I.A.: <DATE>1985</DATE>, 
Generalized Phrase Structure Grammar, Blackwell, Oxford.</REFERENCE>
<REFERENCE>
<SURNAME>Hays</SURNAME>, D.G.: <DATE>1964</DATE>, `Dependency Theory: A Formalism amp; Some Observations',
Language 40, 511-525. </REFERENCE>
<REFERENCE>
<SURNAME>Joshi</SURNAME>, A.K.: <DATE>1987</DATE>, `An Introduction to Tree Adjoining Grammars', in
Manaster-Ramer (ed.), Mathematics of Language, John Benjamins,
Amsterdam.</REFERENCE>
<REFERENCE>
<SURNAME>Lambek</SURNAME>, J.: <DATE>1958</DATE>, `The Mathematics of Sentence Structure', American
Mathematical Monthly 65, 154-169.</REFERENCE>
<REFERENCE>
<SURNAME>Marcus</SURNAME>, M., <SURNAME>Hindle</SURNAME>, D., amp; Fleck, M.: <DATE>1983</DATE>, `D-Theory: Talking about
Talking about Trees', in Proceedings of the 21st ACL, Cambridge, Mass.</REFERENCE>
<REFERENCE>
<SURNAME>Marslen-Wilson</SURNAME>, W.: <DATE>1973</DATE>, `Linguistic Structure amp; Speech Shadowing
at Very Short Latencies', Nature 244, 522-523.</REFERENCE>
<REFERENCE>
<SURNAME>Milward</SURNAME>, D.: <DATE>1992</DATE>, `Dynamics, Dependency Grammar amp; Incremental
Interpretation',
in Proceedings of COLING 92, Nantes, vol 4, <DATE>1095</DATE>-<DATE>1099</DATE>.</REFERENCE>
<REFERENCE>
<SURNAME>Milward</SURNAME>, D. amp; Cooper, R.: <DATE>1994</DATE>, `Incremental Interpretation: Applications,
Theory amp; Relationship to Dynamic Semantics', in Proceedings of
COLING 94, Kyoto, Japan, 748-754. </REFERENCE>
<REFERENCE>
<SURNAME>Milward</SURNAME>, D.: <DATE>1994</DATE>, `Dynamic Dependency Grammar', <DATE>to appear</DATE> in 
Linguistics amp; Philosophy 17, 561-605.</REFERENCE>
<REFERENCE>
<SURNAME>Mitchell</SURNAME>, D.C., <SURNAME>Cuetos</SURNAME>, F., amp; Corley, M.M.B.: <DATE>1992</DATE>, `Statistical versus
linguistic determinants of parsing bias: cross-linguistic evidence'.
Paper presented at the 5th Annual CUNY Conference on Human Sentence
Processing, New York. </REFERENCE>
<REFERENCE>
<SURNAME>Moore</SURNAME>, R.C.: <DATE>1989</DATE>, `Unification-Based Semantic Interpretation', in
Proceedings of the 27th ACL, Vancouver. </REFERENCE>
<REFERENCE>
<SURNAME>Moortgat</SURNAME>, M.: <DATE>1988</DATE>, Categorial Investigations: Logical amp; Linguistic
Aspects of the Lambek Calculus, Foris, Dordrecht. </REFERENCE>
<REFERENCE>
<SURNAME>Morrill</SURNAME>, G., <SURNAME>Leslie</SURNAME>, N., <SURNAME>Hepple</SURNAME>, M. amp; Barry,G.: <DATE>1990</DATE>, `Categorial
Deductions amp; Structural Operations', in Barry, G. amp;</REFERENCE>
<REFERENCE>
<SURNAME>Morrill</SURNAME>, G. (eds.), Studies in Categorial
Grammar, Edinburgh Working Papers in Cognitive Science, 5. </REFERENCE>
<REFERENCE>
<SURNAME>Polanyi</SURNAME>, L. amp; Scha, R.: <DATE>1984</DATE>, `A Syntactic Approach to Discourse Semantics',
in Proceedings of COLING 84, Stanford, 413-419. </REFERENCE>
<REFERENCE>
<SURNAME>Pollard</SURNAME>, C. amp; Sag, I.A.: <DATE>1994</DATE>, Head-Driven Phrase Structure Grammar,
University of Chicago Press amp; CSLI Publications, Chicago.</REFERENCE>
<REFERENCE>
<SURNAME>Pulman</SURNAME>, S.G.: <DATE>1986</DATE>, `Grammars, Parsers, amp; Memory Limitations', 
Language
amp; Cognitive Processes 1(3), 197-225.</REFERENCE>
<REFERENCE>
<SURNAME>Spivey-Knowlton</SURNAME>, M., <SURNAME>Sedivy</SURNAME>, J., <SURNAME>Eberhard</SURNAME>, K., amp; Tanenhaus, M.:
<DATE>1994</DATE>, `Psycholinguistic Study of the Interaction Between Language amp;
Vision', in Proceedings of the 12th National Conference on AI, AAAI-94.</REFERENCE>
<REFERENCE>
<SURNAME>Stabler</SURNAME>, E.P.: <DATE>1991</DATE>, `Avoid the Pedestrian's Paradox', in Berwick,
R.C. et al. (eds.), Principle-Based
Parsing: Computation amp; Psycholinguistics, Kluwer, Netherlands,
199-237.</REFERENCE>
<REFERENCE>
<SURNAME>Steedman</SURNAME>, M.J.: <DATE>1991</DATE>, `Type-Raising amp; Directionality in Combinatory Grammar',
in Proceedings of the 29th ACL, Berkeley, U.S.A. </REFERENCE>
<REFERENCE>
<SURNAME>Tanenhaus</SURNAME>, M.K., <SURNAME>Garnsey</SURNAME>, S., amp; Boland, J.: <DATE>1990</DATE>, `Combinatory
Lexical
Information amp; Language Comprehension', in Altmann, G.T.M. 
  Cognitive
Models of Speech Processing, MIT Press, Cambridge Ma.</REFERENCE>
<REFERENCE>
<SURNAME>Tugwell</SURNAME>, D.: <DATE>1995</DATE>, `A State-Transition Grammar for Data-Oriented
Parsing', in Proceedings of the 7th Conference of the European
  ACL, EACL-95, Dublin, this volume. </REFERENCE>
<REFERENCE>
<SURNAME>Vijay-Shanker</SURNAME>, K.: <DATE>1992</DATE>, `Using Descriptions of Trees in a Tree Adjoining
Grammar', Computational Linguistics 18(4), 481-517.
</REFERENCE>
</REFERENCELIST>
</PAPER>
