CTR	Words unknown to the lexicon present a substantial problem to part-of-speech tagging .	A-0
AIM	In this paper we present a technique for fully unsupervised statistical acquisition of rules which guess possible parts-of-speech for unknown words .	A-1
OWN	Three complementary sets of word-guessing rules are induced from the lexicon and a raw corpus : prefix morphological rules , suffix morphological rules and ending-guessing rules .	A-2
CTR	The learning was performed on the Brown Corpus data and rule-sets , with a highly competitive performance , were produced and compared with the state-of-the-art .	A-3
BKG	Words unknown to the lexicon present a substantial problem to part-of-speech ( POS ) tagging of real-world texts .	S-0
BKG	Taggers assign a single POS - tag to a word-token , provided that it is known what parts-of-speech this word can take on in principle .	S-1
BKG	So , first words are looked up in the lexicon .	S-2
BKG	However , 3 to 5 % of word tokens are usually missing in the lexicon when tagging real-world texts .	S-3
BKG	This is where word-POS guessers take their place -- they employ the analysis of word features , e.g. word leading and trailing characters , to figure out its possible POS categories .	S-4
OTH	A set of rules which on the basis of ending characters of unknown words , assign them with sets of possible POS - tags is supplied with the Xerox tagger.	S-5
OTH	A similar approach was taken inwhere an unknown word was guessed given the probabilities for an unknown word to be of a particular POS , its capitalisation feature and its ending .	S-6
OTH	Ina system of rules which uses both ending-guessing and more morphologically motivated rules is described .	S-7
OTH	The best of these methods are reported to achieve 82 - 85 % of tagging accuracy on unknown words,.	S-8
BKG	The major topic in the development of word-POS guessers is the strategy which is to be used for the acquisition of the guessing rules .	S-9
OTH	A rule-based tagger described inis equipped with a set of guessing rules which has been hand-crafted using knowledge of English morphology and intuition .	S-10
CTR	A more appealing approach is an empirical automatic acquisition of such rules using available lexical resources .	S-11
OTH	Ina system for the automated learning of morphological word-formation rules is described .	S-12
OTH	This system divides a string into three regions and from training examples infers their correspondence to underlying morphological features .	S-13
OTH	outlines a transformation-based learner which learns guessing rules from a pre-tagged training corpus .	S-14
OTH	A statistical-based suffix learner is presented in.	S-15
OTH	From a pre-tagged training corpus it constructs the suffix tree where every suffix is associated with its information measure .	S-16
CTR	Although the learning process in these and some other systems is fully unsupervised and the accuracy of obtained rules reaches current state-of-the-art , they require specially prepared training data -- a pre-tagged training corpus , training examples , etc .	S-17
AIM	In this paper we describe a new fully automatic technique for learning part-of-speech guessing rules .	S-18
OWN	This technique does not require specially prepared training data and employs fully unsupervised statistical learning using the lexicon supplied with the tagger and word-frequencies obtained from a raw corpus .	S-19
OWN	The learning is implemented as a two-staged process with feedback .	S-20
OWN	First , setting certain parameters a set of guessing rules is acquired , then it is evaluated and the results of evaluation are used for re-acquisition of a better tuned rule-set .	S-21
BKG	As was pointed out above , one of the requirements in many techniques for automatic learning of part-of-speech guessing rules is specially prepared training data -- a pre-tagged training corpus , training examples , etc .	S-22
OWN	In our approach we decided to reuse the data which come naturally with a tagger , viz. the lexicon .	S-23
OWN	Another source of information which is used and which is not prepared specially for the task is a text corpus .	S-24
CTR	Unlike other approaches we don't require the corpus to be pre-annotated but use it in its raw form .	S-25
OWN	In our experiments we used the lexicon and word-frequencies derived from the Brown Corpus.	S-26
OWN	There are a number of reasons for choosing the Brown Corpus data for training .	S-27
OWN	The most important ones are that the Brown Corpus provides a model of general multi-domain language use , so general language regularities can be induced from it , and second , many taggers come with data trained on the Brown Corpus which is useful for comparison and evaluation .	S-28
OWN	This , however , by no means restricts the described technique to that or any other tag-set , lexicon or corpus .	S-29
OWN	Moreover , despite the fact that the training is performed on a particular lexicon and a particular corpus , the obtained guessing rules suppose to be domain and corpus independent and the only training-dependent feature is the tag-set in use .	S-30
OWN	The acquisition of word-POS guessing rules is a three-step procedure which includes the rule extraction , rule scoring and rule merging phases .	S-31
OWN	At the rule extraction phase , three sets of word-guessing rules ( morphological prefix guessing rules , morphological suffix guessing rules and ending-guessing rules ) are extracted from the lexicon and cleaned from coincidental cases .	S-32
OWN	At the scoring phase , each rule is scored in accordance with its accuracy of guessing and the best scored rules are included into the final rule-sets .	S-33
OWN	At the merging phase , rules which have not scored high enough to be included into the final rule-sets are merged into more general rules , then re-scored and depending on their score added to the final rule-sets .	S-34
OWN	Morphological word-guessing rules describe how one word can be guessed given that another word is known .	S-35
OWN	For example , the rule :says that prefixing the string `` un '' to a word , which can act as past form of verb ( VBD ) and participle ( VBN ) , produces an adjective ( JJ ) .	S-36
OWN	For instance , by applying this rule to the word `` undeveloped '' , we first segment the prefix `` un '' and if the remaining part `` developed '' is found in the lexicon as ( VBD VBN ) , we conclude that the word `` undeveloped '' is an adjective ( JJ ) .	S-37
OWN	The first POS - set in a guessing rule is called the initial class ( I-class ) and the POS - set of the guessed word is called the resulting class ( R-class ) .	S-38
OWN	In the example above ( VBD VBN ) is the I-class of the rule and ( JJ ) is the R-class .	S-39
BKG	In English , as in many other languages , morphological word formation is realised by affixation : prefixation and suffixation .	S-40
BKG	Although sometimes the affixation is not just a straightforward concatenation of the affix with the stem , the majority of cases clearly obey simple concatenative regularities .	S-41
OWN	So , we decided first to concentrate only on simple concatenative cases .	S-42
OWN	There are two kinds of morphological rules to be learned : suffix rules () -- rules which are applied to the tail of a word , and prefix rules () -- rules which are applied to the beginning of a word .	S-43
OWN	For example :	S-44
OWN	says that if by stripping the suffix `` ed '' from an unknown word we produce a word with the POS - class ( NN VB ) , the unknown word is of the class - ( JJ VBD VBN ) .	S-45
OWN	This rule works , for instance , for [ bookbooked ] , [ waterwatered ] , etc .	S-46
OWN	To extract such rules a special operatoris applied to every pair of words from the lexicon .	S-47
OWN	It tries to segment an affix by leftmost string subtraction for suffixes and rightmost string subtraction for prefixes .	S-48
OWN	If the subtraction results in an non-empty string it creates a morphological rule by storing the POS - class of the shorter word as the I-class and the POS - class of the longer word as the R-class .	S-49
OWN	For example :	S-50
OWN	Theoperator is applied to all possible lexicon-entry pairs and if a rule produced by such an application has already been extracted from another pair , its frequency count ( f ) is incremented .	S-51
OWN	Thus two different sets of guessing rules -- prefix and suffix morphological rules together with their frequencies -- are produced .	S-52
OWN	Next , from these sets of guessing rules we need to cut out infrequent rules which might bias the further learning process .	S-53
OWN	To do that we eliminate all the rules with the frequency f less than a certain threshold.	S-54
OWN	Such filtering reduces the rule-sets more than tenfold and does not leave clearly coincidental cases among the rules .	S-55
OWN	Unlike morphological guessing rules , ending-guessing rules do not require the main form of an unknown word to be listed in the lexicon .	S-56
OWN	These rules guess a POS - class for a word just on the basis of its ending characters and without looking up its stem in the lexicon .	S-57
OWN	Such rules are able to cover more unknown words than morphological guessing rules but their accuracy will not be as high .	S-58
OWN	For example , an ending-guessing rule	S-59
OWN	says that if a word ends with `` ing '' it can be an adjective , a noun or a gerund .	S-60
OWN	Unlike a morphological rule , this rule does not ask to check whether the substring preceeding the `` ing '' - ending is a word with a particular POS - tag .	S-61
OWN	Thus an ending-guessing rule looks exactly like a morphological rule apart from the I-class which is always void .	S-62
OWN	To collect such rules we set the upper limit on the ending length equal to five characters and thus collect from the lexicon all possible word-endings of length 1 , 2 , 3 , 4 and 5 , together with the POS - classes of the words where these endings were detected to appear .	S-63
OWN	This is done by the operator.	S-64
OWN	For example , from the word [ different ( JJ ) ] theoperator will produce five ending-guessing rules :.	S-65
OWN	Theoperator is applied to each entry in the lexicon in the way described for theoperator of the morphological rules and then infrequent rules withare filtered out .	S-66
OWN	Of course , not all acquired rules are equally good as plausible guesses about word-classes : some rules are more accurate in their guessings and some rules are more frequent in their application .	S-67
OWN	So , for every acquired rule we need to estimate whether it is an effective rule which is worth retaining in the final rule-set .	S-68
OWN	For such estimation we perform a statistical experiment as follows : for every rule we calculate the number of times this rule was applied to a word token from a raw corpus and the number of times it gave the right answer .	S-69
OWN	Note that the task of the rule is not to disambiguate a word 's POS but to provide all and only possible POS s it can take on .	S-70
OWN	If the rule is correct in the majority of times it was applied it is obviously a good rule .	S-71
OWN	If the rule is wrong most of the times it is a bad rule which should not be included into the final rule-set .	S-72
OWN	To perform this experiment we take one-by-one each rule from the rule-sets produced at the rule extraction phase , take each word token from the corpus and guess its POS - set using the rule if the rule is applicable to the word .	S-73
OWN	For example , if a guessing rule strips a particular suffix and a current word from the corpus does not have this suffix , we classify these word and rule as incompatible and the rule as not applicable to that word .	S-74
OWN	If the rule is applicable to the word we perform look-up in the lexicon for this word and then compare the result of the guess with the information listed in the lexicon .	S-75
OWN	If the guessed POS - set is the same as the POS - set stated in the lexicon , we count it as success , otherwise it is failure .	S-76
OWN	The value of a guessing rule , thus , closely correlates with its estimated proportion of success () which is the proportion of all positive outcomes ( x ) of the rule application to the total number of the trials ( n ) , which are , in fact , attempts to apply the rule to all the compatible words in the corpus .	S-77
OWN	We also smoothso as not to have zeros in positive or negative outcome probabilities :.	S-78
OWN	estimate is a good indicator of rule accuracy .	S-79
OWN	However , it frequently suffers from large estimation error due to insufficient training data .	S-80
OWN	For example , if a rule was detected to work just twice and the total number of observations was also two , its estimateis very high ( 1 , or 0.83 for the smoothed version ) but clearly this is not a very reliable estimate because of the tiny size of the sample .	S-81
OWN	Several smoothing methods have been proposed to reduce the estimation error .	S-82
OWN	For different reasons all these smoothing methods are not very suitable in our case .	S-83
OWN	In our approach we tackle this problem by calculating the lower confidence limitfor the rule estimate .	S-84
OWN	This can be seen as the minimal expected value offor the rule if we were to draw a large number of samples .	S-85
OWN	Thus with certain confidencewe can assume that if we used more training data , the rule estimatewould be no worse than thelimit .	S-86
OWN	The lower confidence limitis calculated as :	S-87
OWN	This function favours the rules with higher estimates obtained over larger samples .	S-88
OWN	Even if one rule has a high estimate but that estimate was obtained over a small sample , another rule with a lower estimate but over a large sample might be valued higher .	S-89
OWN	Note also that sinceitself is smoothed we will not have zeros in positive () or negative () outcome probabilities .	S-90
BAS	This estimation of the rule value in fact resembles that used byfor scoring POS - disambiguation rules for the French tagger .	S-91
CTR	The main difference between the two functions is that there the z value was implicitly assumed to be 1 which corresponds to the confidence of 68 % .	S-92
OWN	A more standard approach is to adopt a rather high confidence value in the range of 90 - 95 % .	S-93
OWN	We adopted 90 % confidence for which.	S-94
OWN	Thus we can calculate the score for the ith rule as :	S-95
OWN	Another important consideration for scoring a word-guessing rule is that the longer the affix or ending of the rule the more confident we are that it is not a coincidental one , even on small samples .	S-96
OWN	For example , if the estimate for the word-ending `` o '' was obtained over a sample of 5 words and the estimate for the word-ending `` fulness '' was also obtained over a sample of 5 words , the later case is more representative even though the sample size is the same .	S-97
OWN	Thus we need to adjust the estimation error in accordance with the length of the affix or ending .	S-98
OWN	A good way to do that is to divide it by a value which increases along with the increase of the length .	S-99
OWN	After several experiments we obtained :	S-100
OWN	When the length of the affix or ending is 1 the estimation error is not changed sinceis 0 .	S-101
OWN	For the rules with the affix or ending length of 2 the estimation error is reduced by, for the length 3 this will be, etc .	S-102
OWN	The longer the length the smaller the sample which will be considered representative enough for a confident rule estimation .	S-103
OWN	Setting the thresholdat a certain level lets only the rules whose score is higher than the threshold to be included into the final rule-sets .	S-104
TXT	The method for setting up this threshold is based on empirical evaluations of the rule-sets and is described in Section.	S-105
OWN	Rules which have scored lower than the thresholdcan be merged into more general rules which if scored above the threshold are also included into the final rule-sets .	S-106
OWN	We can merge two rules which have scored below the threshold and have the same affix ( or ending ) and the initial class ( I ) .	S-107
OWN	The score of the resulting rule will be higher than the scores of the merged rules since the number of positive observations increases and the number of the trials remains the same .	S-108
OWN	After a successful application of the merging , the resulting rule substitutes the two merged ones .	S-109
OWN	To perform such rule-merging over a rule-set , first , the rules which have not been included into the final set are sorted by their score and best-scored rules are merged first .	S-110
OWN	This is done recursively until the score of the resulting rule does not exceed the threshold in which case it is added to the final rule-set .	S-111
OWN	This process is applied until no merges can be done to the rules which have scored below the threshold .	S-112
OWN	There are two important questions which arise at the rule acquisition stage - how to choose the scoring thresholdand what is the performance of the rule-sets produced with different thresholds .	S-113
OWN	The task of assigning a set of POS - tags to a word is actually quite similar to the task of document categorisation where a document should be assigned with a set of descriptors which represent its contents .	S-114
OWN	The performance of such assignment can be measured in :	S-115
OWN	recall - the percentage of POS s which were assigned correctly by the guesser to a word ; .	S-116
OWN	precision - the percentage of POS s the guesser assigned correctly over the total number of POS s it assigned to the word ; .	S-117
OWN	coverage - the proportion of words which the guesser was able to classify , but not necessarily correctly ; .	S-118
OWN	In our experiments we measured word precision and word recall ( micro-average ) .	S-119
OWN	There were two types of data in use at this stage .	S-120
OWN	First , we evaluated the guessing rules against the actual lexicon : every word from the lexicon , except for closed-class words and words shorter than five characters , was guessed by the different guessing strategies and the results were compared with the information the word had in the lexicon .	S-121
OWN	In the other evaluation experiment we measured the performance of the guessing rules against the training corpus .	S-122
OWN	For every word we computed its metrics exactly as in the previous experiment .	S-123
OWN	Then we multiplied these results by the corpus frequency of this particular word and averaged them .	S-124
OWN	Thus the most frequent words had the greatest influence on the aggreagte measures .	S-125
OWN	First , we concentrated on finding the best thresholdsfor the rule-sets .	S-126
OWN	To do that for each rule-set produced using different thresholds we recorded the three metrics and chose the set with the best aggregate .	S-127
OWN	In Tablesome results of that experiment are shown .	S-128
OWN	The best thresholds were detected : for ending rules - 75 points , for suffix rules - 60 , and for prefix rules - 80 .	S-129
OWN	One can notice a slight difference in the results obtained over the lexicon and the corpus .	S-130
OWN	The corpus results are better because the training technique explicitly targeted the rule-sets to the most frequent cases of the corpus rather than the lexicon .	S-131
OWN	In average ending-guessing rules were detected to cover over 96 % of the unknown words .	S-132
OWN	The precision of 74 % roughly can be interpreted as that for words which take on three different POS s in their POS - class , the ending-guessing rules will assign four , but in 95 % of the times ( recall ) the three required POS s will be among the four assigned by the guess .	S-133
CTR	In comparison with the Xerox word-ending guesser taken as the base-line model we detect a substantial increase in the precision by about 22 % and a cheerful increase in coverage by about 6 % .	S-134
CTR	This means that the Xerox guesser creates more ambiguity for the disambiguator , assigning five instead of three POS s in the example above .	S-135
CTR	It can also handle 6 % less unknown words which , in fact , might decrease its performance even lower .	S-136
OWN	In comparison with the ending-guessing rules , the morphological rules have much better precision and hence better accuracy of guessing .	S-137
OWN	Virtually almost every word which can be guessed by the morphological rules is guessed exactly correct ( 97 % recall and 97 % precision ) .	S-138
OWN	Not surprisingly , the coverage of morphological rules is much lower than that of the ending-guessing ones - for the suffix rules it is less than 40 % and for the prefix rules about 5 - 6 .	S-139
OWN	After obtaining the optimal rule-sets we performed the same experiment on a word-sample which was not included into the training lexicon and corpus .	S-140
OWN	We gathered about three thousand words from the lexicon developed for the Wall Street Journal corpus and collected frequencies of these words in this corpus .	S-141
OWN	At this experiment we obtained similar metrics apart from the coverage which dropped about 0.5 % for Ending 75 and Xerox rule-sets and 7 % for the Suffix 60 rule-set .	S-142
OWN	This , actually , did not come as a surprise , since many main forms required by the suffix rules were missing in the lexicon .	S-143
OWN	In the next experiment we evaluated whether the morphological rules add any improvement if they are used in conjunction with the ending-guessing rules .	S-144
OWN	We also evaluated in detail whether a conjunctive application with the Xerox guesser would boost the performance .	S-145
OWN	As in the previous experiment we measured the precision , recall and coverage both on the lexicon and on the corpus .	S-146
OWN	Tabledemonstrates some results of this experiment .	S-147
OWN	The first part of the table shows that when the Xerox guesser is applied before the Eguesser we measure a drop in the performance .	S-148
OWN	When the Xerox guesser is applied after the Eguesser no sufficient changes to the performance are noticed .	S-149
OWN	This actually proves that the Erule-set fully supercedes the Xerox rule-set .	S-150
OWN	The second part of the table shows that the cascading application of the morphological rule-sets together with the ending-guessing rules increases the overall precision of the guessing by a further 5 % .	S-151
OWN	This makes the improvements against the base-line Xerox guesser 28 % in precision and 7 % in coverage .	S-152
OWN	The direct evaluation of the rule-sets gave us the grounds for the comparison and selection of the best performing guessing rule-sets .	S-153
OWN	The task of unknown word guessing is , however , a subtask of the overall part-of-speech tagging process .	S-154
OWN	Thus we are mostly interested in how the advantage of one rule-set over another will affect the tagging performance .	S-155
OWN	So , we performed an independent evaluation of the impact of the word guessers on tagging accuracy .	S-156
CTR	In this evaluation we tried two different taggers .	S-157
OTH	First , we used a tagger which was a C++ re-implementation of the LISP implemented HMM Xerox tagger described in.	S-158
OTH	The other tagger was the rule-based tagger of.	S-159
OTH	Both of the taggers come with data and word-guessing components pre-trained on the Brown Corpus .	S-160
OWN	This , actually gave us the search-space of four combinations : the Xerox tagger equipped with the original Xerox guesser ,'s tagger with its original guesser , the Xerox tagger with our cascading P+ S+ Eguesser and's tagger with the cascading guesser .	S-161
OWN	For words which failed to be guessed by the guessing rules we applied the standard method of classifying them as common nouns ( NN ) if they are not capitalised inside a sentence and proper nouns ( NP ) otherwise .	S-162
OWN	As the base-line result we measured the performance of the taggers with all known words on the same word sample .	S-163
OWN	In the evaluation of tagging accuracy on unknown words we pay attention to two metrics .	S-164
OWN	First we measure the accuracy of tagging solely on unknown words :	S-165
OWN	This metric gives us the exact measure of how the tagger has done on unknown words .	S-166
OWN	In this case , however , we do not account for the known words which were mis-tagged because of the guessers .	S-167
OWN	To put a perspective on that aspect we measure the overall tagging performance :	S-168
OWN	Since the Brown Corpus model is a general language model , it , in principle , does not put restrictions on the type of text it can be used for , although its performance might be slightly lower than that of a model specialised for this particular sublanguage .	S-169
OWN	Here we want to stress that our primary task was not to evaluate the taggers themselves but rather their performance with the word-guessing modules .	S-170
OWN	So we did not worry too much about tuning the taggers for the texts and used the Brown Corpus model instead .	S-171
OWN	We tagged several texts of different origins , except from the Brown Corpus .	S-172
OWN	These texts were not seen at the training phase which means that neither the taggers nor the guessers had been trained on these texts and they naturally had words unknown to the lexicon .	S-173
OWN	For each text we performed two tagging experiments .	S-174
OWN	In the first experiment we tagged the text with the Brown Corpus lexicon supplied with the taggers and hence had only those unknown words which naturally occur in this text .	S-175
OWN	In the second experiment we tagged the same text with the lexicon which contained only closed-class and short words .	S-176
OWN	This small lexicon contained only 5,456 entries out of 53,015 entries of the original Brown Corpus lexicon .	S-177
OWN	All other words were considered as unknown and had to be guessed by the guessers .	S-178
OWN	We obtained quite stable results in these experiments .	S-179
OWN	Here is a typical example of tagging a text of 5970 words .	S-180
OWN	This text was detected to have 347 unknown words .	S-181
OWN	First , we tagged the text by the four different combinations of the taggers with the word-guessers using the full-fledged lexicon .	S-182
OWN	The results of this tagging are summarised in Table.	S-183
CTR	When using the Xerox tagger with its original guesser , 63 unknown words were incorrectly tagged and the accuracy on the unknown words was measured at 81.8 % .	S-184
CTR	When the Xerox tagger was equipped with our cascading guesser its accuracy on unknown words increased by almost 9 % upto 90.5 % .	S-185
OWN	The same situation was detected with's tagger which in general was slightly more accurate than the Xerox one .	S-186
CTR	The cascading guesser performed better than's original guesser by about 8 % boosting the performance on the unknown words from 84.5 % to 92.2 % .	S-187
OWN	The accuracy of the taggers on the set of 347 unknown words when they were made known to the lexicon was detected at 98.5 % for both taggers .	S-188
OWN	In the second experiment we tagged the same text in the same way but with the small lexicon .	S-189
OWN	Out of 5,970 words of the text , 2,215 were unknown to the small lexicon .	S-190
OWN	The results of this tagging are summarised in Table.	S-191
OWN	The accuracy of the taggers on the 2,215 unknown words when they were made known to the lexicon was much lower than in the previous experiment -- 90.3 % for the Xerox tagger and 91.5 % for's tagger .	S-192
OWN	Naturally , the performance of the guessers was also lower than in the previous experiment plus the fact that many `` semi-closed '' class adverbs like `` however '' , `` instead '' , etc .	S-193
OWN	, were missing in the small lexicon .	S-194
OWN	The accuracy of the tagging on unknown words dropped by about 5 % in general .	S-195
CTR	The best results on unknown words were again obtained on the cascading guesser ( 86 % - 87.45 % ) and's tagger again did better then the Xerox one by 1.5 .	S-196
OWN	Two types of mis-taggings caused by the guessers occured .	S-197
OWN	The first type is when guessers provided broader POS - classes for unknown words and the tagger had difficulties with the disambiguation of such broader classes .	S-198
OWN	This is especially the case with the `` ing '' words which , in general , can act as nouns , adjectives and gerunds and only direct lexicalization can restrict the search space , as in the case with the word `` going '' which cannot be an adjective but only a noun and a gerund .	S-199
OWN	The second type of mis-tagging was caused by wrong assignments of POS s by the guesser .	S-200
OWN	Usually this is the case with irregular words like , for example , `` cattle '' which was wrongly guessed as a singular noun ( NN ) but in fact is a plural noun ( NNS ) .	S-201
AIM	We presented a technique for fully unsupervised statistical acquisition of rules which guess possible parts-of-speech for words unknown to the lexicon .	S-202
OWN	This technique does not require specially prepared training data and uses for training the lexicon and word frequencies collected from a raw corpus .	S-203
OWN	Using these training data three types of guessing rules are learned : prefix morphological rules , suffix morphological rules and ending-guessing rules .	S-204
OWN	To select best performing guessing rule-sets we suggested an evaluation methodology , which is solely dedicated to the performance of part-of-speech guessers .	S-205
CTR	Evaluation of tagging accuracy on unknown words using texts unseen by the guessers and the taggers at the training phase showed that tagging with the automatically induced cascading guesser was consistently more accurate than previously quoted results known to the author ( 85 % ) .	S-206
CTR	The cascading guesser outperformed the guesser supplied with the Xerox tagger by about 8 - 9 % and the guesser supplied with's tagger by about 6 - 7 % .	S-207
OWN	Tagging accuracy on unknown words using the cascading guesser was detected at 90 - 92 % when tagging with the full-fledged lexicon and 86 - 88 % when tagging with the closed-class and short word lexicon .	S-208
OWN	When the unknown words were made known to the lexicon the accuracy of tagging was detected at 96 - 98 % and 90 - 92 % respectively .	S-209
OWN	This makes the accuracy drop caused by the cascading guesser to be less than 6 % in general .	S-210
OWN	Another important conclusion from the evaluation experiments is that the morphological guessing rules do improve the guessing performance .	S-211
OWN	Since they are more accurate than ending-guessing rules they are applied before ending-guessing rules and improve the precision of the guessings by about 5 % .	S-212
OWN	This , actually , results in about 2 % higher accuracy of tagging on unknown words .	S-213
OWN	The acquired guessing rules employed in our cascading guesser are , in fact , of a standard nature and in that form or another are used in other POS - guessers .	S-214
OWN	There are , however , a few points which make the rule-sets acquired by the presented here technique more accurate :	S-215
OWN	the learning of such rules is done from the lexicon rather than tagged corpus , because the guesser 's task is akin to the lexicon lookup ;	S-216
OWN	there is a well-tuned statistical scoring procedure which accounts for rule features and frequency distribution ;	S-217
OWN	there is an empirical way to determine an optimum collection of rules , since acquired rules are subject to rigorous direct evaluation in terms of precision , recall and coverage ;	S-218
OWN	rules are applied cascadingly using the most accurate rules first .	S-219
OWN	One of the most important issues in the induction of guessing rule-sets is the choice right data for training .	S-220
OWN	In our approach , guessing rules are extracted from the lexicon and the actual corpus frequencies of word-usage then allow for discrimination between rules which are no longer productive ( but have left their imprint on the basic lexicon ) and rules that are productive in real-life texts .	S-221
OWN	Thus the major factor in the learning process is the lexicon .	S-222
OWN	Since guessing rules are meant to capture general language regularities the lexicon should be as general as possible ( list all possible POS s for a word ) and as large as possible .	S-223
OWN	The corresponding corpus should include most of the words from the lexicon and be large enough to obtain reliable estimates of word-frequency distribution .	S-224
OWN	Our experiments with the lexicon and word frequencies derived from the Brown Corpus , which can be considered as a general model of English , resulted in guessing rule-sets which proved to be domain and corpus independent , producing similar results on test texts of different origin .	S-225
OWN	Although in general the performance of the cascading guesser is only 6 % worse than the lookup of a general language lexicon there is room for improvement .	S-226
OWN	First , in the extraction of the morphological rules we did not attempt to model non-concatenative cases .	S-227
OWN	In English , however , since most of letter mutations occur in the last letter of the main word it is possible to account for it .	S-228
OWN	So our next goal is to extract morphological rules with one letter mutations at the end .	S-229
OWN	This would account for cases like `` try - tries '' , `` reduce - reducing '' , `` advise - advisable '' .	S-230
OWN	We expect it to increase the coverage of thesuffix morphological rules and hence contribute to the overall guessing accuracy .	S-231
OWN	Another avenue for improvement is to provide the guessing rules with the probabilities of emission of POS s from their resulting POS - classes .	S-232
OWN	This information can be compiled automatically and also might improve the accuracy of tagging unknown words .	S-233
OWN	The described rule acquisition and evaluation methods are implemented as a modular set of C ++ and AWK tools , and the guesser is easily extendable to sub-language specific regularities and retrainable to new tag-sets and other languages , provided that these languages have affixational morphology .	S-234
OWN	Both the software and the produced guessing rule-sets are available by contacting the author .	S-235
