AIM	This paper presents a grammar formalism designed for use in data-oriented approaches to language processing .	A-0
OWN	It goes on to investigate ways in which a corpus pre-parsed with this formalism may be processed to provide a probabilistic language model for use in the parsing of fresh texts .	A-1
BKG	Recent years have seen a resurgence of interest in probabilistic techniques for automatic language analysis .	S-0
OTH	In particular , there has arisen a distinct paradigm of processing on the basis of pre-analyzed data which has taken the name Data-Oriented Parsing .	S-1
OTH	`` Data Oriented Parsing ( DOP ) is a model where no abstract rules , but language experiences in the form of an analyzed corpus , constitute the basis for language processing . ''	S-2
OTH	There is not space here to present full justification for adopting such an approach or to detail the advantages that it offers .	S-3
OTH	The main claim it makes is that effective language processing requires a consideration of both the structural and statistical aspects of language , whereas traditional competence grammars rely only on the former , and standard statistical techniques such as n-gram models only on the latter .	S-4
OTH	DOP attempts to combine these two traditions and produce `` performance grammars '' , which :	S-5
OTH	`` ... should not only contain information on the structural possibilities of the general language system , but also on details of actual language use in a language community . ''	S-6
OTH	This approach entails however that a corpus has first to be pre-analyzed ( ie. hand-parsed ) , and the question immediately arises as to the formalism to be used for this .	S-7
CTR	There is no lack of competing competence grammars available , but also no reason to expect that such grammars should be suited to a DOP approach , designed as they were to characterize the nature of linguistic competence rather than performance .	S-8
AIM	The next section sets out some of the properties that we might require from such a `` performance grammar '' and offers a formalism which attempts to satisfy these requirements .	S-9
OWN	Given that we are attempting to construct a formalism that will do justice to both the statistical and structural aspects of language , the features that we would wish to maximize will include the following :	S-10
OWN	The formalism should be easy to use with probabilistic processing techniques , ideally having a close correspondence to a simple probabilistic model such as a Markov process .	S-11
OWN	The formalism should be fine-grained , ie. responsive to the behaviour of individual words ( as n-gram models are ) .	S-12
OWN	This suggests a radically lexicalist approachin which all rules are encoded in the lexicon , there being no phrase structure rules which do not introduce lexical items .	S-13
OWN	It should be capable of capturing fully the linguistic intuitions of language users .	S-14
OWN	In other words , using the formalism one should be able to characterize the structural regularities of language with at least the sophistication of modern competence grammars .	S-15
OWN	As it is to be used with real data , the formalism should be able to characterize the wide range of syntactic structures found in actual language use , including those normally excluded by competence grammars as belonging to the `` periphery '' of the language or as being `` ungrammatical '' .	S-16
OWN	Ideally every interpretable utterance should have one and only one analysis for any interpretation of it .	S-17
OWN	Considering the first of these points , namely a close relation to a simple probabilistic model , a good place to start the search might be with a right-branching finite-state grammar .	S-18
OWN	In this class of grammars every rule has the form Aa B ( A , B{ non-terminals } , a{ terminals } ) and all trees have the simple structure :	S-19
OWN	Or : [TABLE]	S-20
OWN	( with an equivalent vertical alignment , henceforth to be used in this paper , on the right ) .	S-21
OWN	In probabilistic terms , a finite-state grammar corresponds to a first-order Markov process , where given a sequence of states,, ... drawn from a finite set of possible states {, ... ,} the probability of a particular state occurring depends solely on the identity of the previous state .	S-22
OWN	In the finite-state grammar each word is associated with a transition between two categories , in the tree above ` a ' with the transition AB and so on .	S-23
OWN	To calculate the probability that a string of words,,, ...has the parse represented by the string of category-states S, S, S, ... S, we simply take the product of the probability of each transition : ie..	S-24
OWN	In addition to satisfying our first criterion , a finite-state grammar also fulfills the requirement that the formalism be radically lexicalist , as by definition every rule introduces a lexical item .	S-25
OWN	If a finite-state grammar is chosen however , the third criterion , that of linguistic adequacy , seems to present an insurmountable stumbling block .	S-26
OWN	How can such a simple formalism , in which syntax is reduced to a string of category-states , hope to capture even the basic hierarchical structure , the familiar `` tree structure '' , of linguistic expressions ?	S-27
OWN	Indeed , if the non-terminals are viewed as atomic categories then there is no way this can be done .	S-28
OWN	If however , in line with most current theories , categories are taken to be bundles of features and crucially if one of these features has the value of a stack of categories , then this hierarchical structure can indeed be represented .	S-29
OWN	Using the notationto represent a state of basic category A carrying a category B on its stack , the hierarchical structure of the sentence :	S-30
OWN	can be represented as :	S-31
OWN	Intuitively , syntactic links between non-adjacent words , impossible in a standard finite-state grammar , are here established by passing categories along on the stack `` through '' the state of intervening words .	S-32
OWN	That such a formalism can fully capture basic linguistic structures is confirmed by the proof inthat an indexed grammar ( ie. one where categories are supplemented with a stack of unbounded length , as above ) , if restricted to right linear trees ( also as above ) , is equivalent to a context-free grammar .	S-33
OWN	A perusal of the state transitions associated with individual words inreveals an obvious relationship to the `` types '' of categorial grammar .	S-34
OWN	Usingto represent a list of categories ( possibly null ) , we arrive at the following transitions ( with their corresponding categorial types alongside ) .	S-35
OWN	The ditransitive verb ` gave ' is	S-36
OWN	Determiners in complement position are both :	S-37
OWN	Determiner in subject position is ` type-raised ' to :	S-38
OWN	The common nouns are all :	S-39
OWN	In fact as no intermediate constituents are formed in the analysis , an even closer parallel is to a dependency syntax where only rightward pointing arrows are allowed , of which the formalism as presented above is a notational variant .	S-40
OWN	This lack of intermediate constituents has the added benefit that no `` spurious ambiguities '' can arise .	S-41
OWN	Knowing now that the addition of a stack-valued feature suffices to capture the basic hierarchical structure of language , additional features can be used to deal with other syntactic relations .	S-42
OWN	For example , following the example of GPSG , unbounded dependencies can be captured using `` slashed '' categories .	S-43
OWN	If we represent a `` slashed '' category X with the lower case x , and use the notation A(b) for a category A carrying a feature b , then the topicalized sentence :	S-44
OWN	will have the analysis :	S-45
OWN	Although there is no space in this paper to go into greater detail , further constructions involving unbounded dependency and complement control phenomena can be captured in similar ways .	S-46
OWN	The criterion that remains to be satisfied is that of width of coverage : can the formalism cope with the many `` peripheral '' structures found in real written and spoken texts ?	S-47
OWN	As it stands the formalism is weakly equivalent to a context-free grammar and as such will have problems dealing with phenomena like discontinuous constituents , non-constituent coordination and gapping .	S-48
OWN	Fortunately if extensions are made to the formalism , necessarily taking it outside weak equivalence to a context-free grammar , natural and general analyses present themselves for such constructions .	S-49
OWN	Two of these will now be sketched .	S-50
OWN	Consider the pair of sentencesand, identical in interpretation , but the latter containing a discontinuous noun phrase and the former not :	S-51
OWN	which have the respective analyses :	S-52
OWN	The only transition inthat differs from that of the corresponding word in the ` core ' variantis that of ` dog ' which has the respective transitions :	S-53
OWN	Both nouns introduce a relative clause modifier, the difference being that in the discontinuous variant a category has been taken off the stack at the same time as the modifier has been placed on the stack .	S-54
OWN	It has been assumed so far that we are using a right-linear indexed grammar , but such a rule is expressly disallowed in an indexed grammar and so allowing transitions of this kind ends the formalism ` s weak equivalence to the context-free grammars .	S-55
OWN	Of course , having allowed such crossed dependencies , there is nothing in the formalism itself that will disallow a similar analysis for a discontinuity unacceptable in English such as :	S-56
OWN	This does not present a problem , however , as in DOP it is information in the parsed corpus which determines the structures that are possible .	S-57
OWN	There is no need to explicitly rule out, as the transition NP [][ N ] will be vanishingly rare in any corpus of even the most garbled speech , while the transition N [][ S ( rel ) ] is commonly met with in both written and spoken English .	S-58
OWN	The analysis of standard coordination is shown in:	S-59
OWN	Instead of a typical transition for ` gnawed ' of VPNP , we have a transition introducing a coordinated VP :.	S-60
OWN	In general for any transition XY , where X is a category and Y a list of categories ( possibly empty ) , there will be a transition introducing coordination :.	S-61
OWN	Non-constituent coordinations such aspresent serious problems for phrase-structure approaches :	S-62
OWN	However if we generalize the schema already obtained for standard coordination by allowing X to be not only a single category , but a list of categories , it is found to suffice for non-constituent coordination as well .	S-63
OWN	In this analysis instead of a regular transition for ` bone ' of :.	S-64
OWN	there is instead a transition introducing coordination :.	S-65
OWN	Allowing categories on the stack to themselves have non-empty stacks moves the formalism one step further from being an indexed grammar .	S-66
OWN	This is the final incarnation of the formalism , being the State-Transition Grammar of the title .	S-67
OWN	Similar schemas are being investigated to characterize gapping constructions .	S-68
OWN	It should be noted that an indefinite amount of centre-embedding can be described , but only at the expense of unlimited growth in the length of states :	S-69
OWN	This contrasts with unlimited right-recursion where there is no growth in state length :	S-70
OWN	As the model is to be trained from real data , transitions involving long states as inwill have an ever smaller and eventually effectively nil probability .	S-71
OWN	Therefore , when tuned to any particular language corpus the resulting grammar will be effectively finite-state .	S-72
OWN	Assuming that we now have a corpus parsed with the state-transition grammar , how can this information be used to parse fresh text .	S-73
OWN	Firstly , for each word type in the corpus we can collect the transitions with which it occurs and calculate its probability distribution over all possible transitions ( an infinite number of which will be zero ) .	S-74
OWN	To make this concrete , there are five tokens of the word ` dog ' in the examples thus far , and so ` dog ' will have the transition probability distribution :	S-75
OWN	To find the most probable parse for a sentence , we simply find the path from word to word which maximizes the product of the state transitions ( as we have a first order Markov process ) .	S-76
OWN	However this simple-minded approach , although easy to implement , in other ways leaves much to be desired .	S-77
OWN	The probability distributions are far too `` gappy '' and even if a huge amount of data were collected , the chances that they would provide the desired path for a sentence of any reasonable length are slim .	S-78
OWN	The process of generalizing or smoothing the transition probabilities is therefore seen to be indispensable .	S-79
OWN	Although far from exhausting the possible methods for smoothing , the following three are those used in the implementation described at the end of the paper .	S-80
OWN	Factor out elements on the stack which are merely carried over from state to state ( which was done earlier in looking at the correspondence of state transitions to categorial types ) .	S-81
OWN	The previous transitions for ` dog ' then become :	S-82
OWN	Factor out other features which are merely passed from state to state .	S-83
OWN	For instance in the example sentences , ` the ' has the generalized transitions :	S-84
OWN	which can be further generalized to the single transition :	S-85
OWN	Establish word paradigms , ie. classes of words which occur with similar transitions .	S-86
OWN	The probability distribution for individual words can then be smoothed by suitably blending in the paradigmatic distribution .	S-87
OWN	These paradigms will correspond to a great extent to the word classes of rule-based grammars .	S-88
OWN	The advantage would be retained however that the system is still fine-grained enough to reflect the idiosyncratic patterns of individual words and could override this paradigmatic information if sufficient data were available .	S-89
OWN	Words hitherto unknown to the system can be treated as being extreme examples of words lacking sufficient transition data and they might then be given a transition distribution blended from the open class word paradigms .	S-90
OWN	Although essential for effective processing , the smoothing operations may give rise to new problems .	S-91
OWN	For example , factoring out items on the stack , as in, removes from the model the disinclination for long states inherent in the original corpus .	S-92
OWN	To recapture this discarded aspect of the language , it would be sufficient to introduce into the model a probabilistic penalty based on state length .	S-93
OWN	This penalty may easily be calculated according to the lengths of states in the parsed corpus .	S-94
OWN	Not only would this allow the modelling of the restriction on centre-embedding , but it would also allow many other `` processing '' phenomena to be accurately characterized .	S-95
OWN	Taking as an example `` heavy-NP shift '' , suppose that the corpus contained two distinct transitions for the word ` threw ' , with the particle ` out ' both before and after the object .	S-96
OWN	Even ifwere considerably greater than, the cumulative negative effect of the longer states inwould eventually lead to the model giving the sentence with the shifted NPa higher probability .	S-97
OTH	One strength of n-gram models is that they can capture a certain amount of lexical preference information .	S-98
OTH	For example , in a bigram model trained on sufficient data the probability of the bigram ` dog barked ' could be expected to be significantly higher than ` cat barked ' , and this slice of `` world knowledge '' is something our model lacks .	S-99
OWN	It would not be difficult to make a small extension to the present model to capture such information , namely by introducing an additional feature containing the '' lexical value '' of the head of a phrase .	S-100
OWN	Abandoning the shorthand ` VP ' and representing a subject explicitly as a `` slashed '' NP , a sentence with added lexical head features would appear as :	S-101
OWN	In contrast to n-grams , where this sentence would cloud somewhat the `` world knowledge '' , containing as it does the bigram ` cat barked ' , the added structure of our model allows the lexical preference to be captured no matter how far the head noun is from the head verb .	S-102
OWN	Fromthe world knowledge of the system would be reinforced by the two stereotypical transitions :	S-103
OWN	16,000 + running words from section N of the Brown corpus ( texts N01 - N08 ) were hand-parsed using the state-transition grammar .	S-104
OWN	The actual formalism used was much fuller than the rather schematic one given above , including many additional features such as case , tense , person and number .	S-105
OWN	Transition probabilities were generalized in the ways discussed in the previous section .	S-106
OWN	sentences of less than 15 words were chosen randomly from other texts in section N of the Brown corpus ( N09 - N14 ) and fed to the parser without alteration .	S-107
OWN	Unknown words in the input , of which there were obviously many , were assigned to one of seven orthographic classes and given appropriate transitions calculated from the corpus .	S-108
OWN	27 were parsed correctly , ie. exactly the same as the hand parse or differing in only relatively insignificant ways which the model could not hope to know .	S-109
OWN	23 were parsed wrongly , ie. the analysis differed from the hand parse in some non-trivial way .	S-110
OWN	50 were not parsed at all , ie. one or more of the transitions necessary to find a parse path was lacking , even after generalizing the transitions .	S-111
OWN	Although the results at present are extremely modest , it should be borne in mind both that the amount of data the system has to work on is very small and that the smoothing of transition probabilities is still far from optimal .	S-112
OWN	The present target is to achieve such a level of performance that the corpus can be extended by hand-correction of the parser output , rather than hand-parsing from scratch .	S-113
OWN	Not only will this hopefully save a certain amount of drudgery , it should also help to minimize errors and maintain consistency .	S-114
OWN	A more distant goal is to ascertain whether the performance of the model can improve after parsing new texts and processing the data therein even without hand-correction of the parses , and if so what the limits are to such `` self-improvement '' .	S-115
