AIM	This paper proposes a method for measuring semantic similarity between words as a new tool for text analysis .	A-0
OWN	The similarity is measured on a semantic network constructed systematically from a subset of the English dictionary , LDOCE ( Longman Dictionary of Contemporary English ) .	A-1
OWN	Spreading activation on the network can directly compute the similarity between any two words in the Longman Defining Vocabulary , and indirectly the similarity of all the other words in LDOCE .	A-2
OWN	The similarity represents the strength of lexical cohesion or semantic relation , and also provides valuable information about similarity and coherence of texts .	A-3
BKG	A text is not just a sequence of words , but it also has coherent structure .	S-0
BKG	The meaning of each word in a text depends on the structure of the text .	S-1
BKG	Recognizing the structure of text is an essential task in text understanding.	S-2
BKG	One of the valuable indicators of the structure of text is lexical cohesion.	S-3
BKG	Lexical cohesion is the relationship between words , classified as follows :	S-4
BKG	Reiteration :	S-5
BKG	Semantic relation :	S-6
BKG	Reiteration of words is easy to capture by morphological analysis .	S-7
BKG	Semantic relation between words , which is the focus of this paper , is hard to recognize by computers .	S-8
OWN	We consider lexical cohesion as semantic similarity between words .	S-9
OWN	Similarity is computed by spreading activation ( or association )on a semantic network constructed systematically from an English dictionary .	S-10
OWN	Whereas it is edited by some lexicographers , a dictionary is a set of associative relation shared by the people in a linguistic community .	S-11
OWN	The similarity between words is a mapping:, where L is a set of words ( or lexicon ) .	S-12
OWN	The following examples suggest the feature of the similarity :	S-13
OWN	The value ofincreases with strength of semantic relation between w and w ' .	S-14
TXT	The following section examines related work in order to clarify the nature of the semantic similarity .	S-15
TXT	Sectiondescribes how the semantic network is systematically constructed from the English dictionary .	S-16
TXT	Sectionexplains how to measure the similarity by spreading activation on the semantic network .	S-17
TXT	Sectionshows applications of the similarity measure -- computing similarity between texts , and measuring coherence of a text .	S-18
TXT	Sectiondiscusses the theoretical aspects of the similarity .	S-19
BKG	Words in a language are organized by two kinds of relationship .	S-20
BKG	One is a syntagmatic relation : how the words are arranged in sequential texts .	S-21
BKG	The other is a paradigmatic relation : how the words are associated with each other .	S-22
BKG	Similarity between words can be defined by either a syntagmatic or a paradigmatic relation .	S-23
OTH	Syntagmatic similarity is based on co-occurrence data extracted from corpora, definitions in dictionaries, and so on .	S-24
OTH	Paradigmatic similarity is based on association data extracted from thesauri, psychological experiments, and so on .	S-25
OWN	This paper concentrates on paradigmatic similarity , because a paradigmatic relation can be established both inside a sentence and across sentence boundaries , while syntagmatic relations can be seen mainly inside a sentence -- like syntax deals with sentence structure .	S-26
TXT	The rest of this section focuses on two related works on measuring paradigmatic similarity -- a psycholinguistic approach and a thesaurus-based approach .	S-27
OTH	Psycholinguists have been proposed methods for measuring similarity .	S-28
OTH	One of the pioneering works is ` semantic differential 'which analyses meaning of words into a range of different dimensions with the opposed adjectives at both ends ( see Figure) , and locates the words in the semantic space .	S-29
OTH	Recent works on knowledge representation are somewhat related to's semantic differential .	S-30
OTH	Most of them describe meaning of words using special symbols like microfeatures,that correspond to the semantic dimensions .	S-31
CTR	However , the following problems arise from the semantic differential procedure as measurement of meaning .	S-32
CTR	The procedure is not based on the denotative meaning of a word , but only on the connotative emotions attached to the word ; it is difficult to choose the relevant dimensions , i.e. the dimensions required for the sufficient semantic space .	S-33
OTH	used's thesaurus as knowledge base for determining whether or not two words are semantically related .	S-34
OTH	For example , the semantic relation of truck / car and drive / car are captured in the following way :	S-35
OTH	This method can capture almost all types of semantic relations ( except emotional and situational relation ) , such as paraphrasing by superordinate ( ex . cat / pet ) , systematic relation ( ex .  north / east ) , and non-systematic relation ( ex .  theatre / film ) .	S-36
CTR	However , thesauri provide neither information about semantic difference between words juxtaposed in a category , nor about strength of the semantic relation between words -- both are to be dealt in this paper .	S-37
BKG	The reason is that thesauri are designed to help writers find relevant words , not to provide the meaning of words .	S-38
OWN	We analyse word meaning in terms of the semantic space defined by a semantic network , called Paradigme .	S-39
OWN	Paradigme is systematically constructed from Glossme , a subset of an English dictionary .	S-40
BKG	A dictionary is a closed paraphrasing system of natural language .	S-41
BKG	Each of its headwords is defined by a phrase which is composed of the headwords and their derivations .	S-42
BKG	A dictionary , viewed as a whole , looks like a tangled network of words .	S-43
BAS	We adopted Longman Dictionary of Contemporary Englishas such a closed system of English .	S-44
OTH	LDOCE has a unique feature that each of its 56,000 headwords is defined by using the words in Longman Defining Vocabulary ( hereafter , LDV ) and their derivations .	S-45
OTH	LDV consists of 2,851 words ( as the headwords in LDOCE ) based on the survey of restricted vocabulary.	S-46
OWN	We made a reduced version of LDOCE , called Glossme .	S-47
OWN	Glossme has every entry of LDOCE whose headword is included in LDV .	S-48
OWN	Thus , LDV is defined by Glossme , and Glossme is composed of LDV .	S-49
OWN	Glossme is a closed subsystem of English .	S-50
OWN	Glossme has 2,851 entries that consist of 101,861 words ( 35.73 words / entry on the average ) .	S-51
OWN	An item of Glossme has a headword , a word-class , and one or more units corresponding to numbered definitions in the entry of LDOCE .	S-52
OWN	Each unit has one head-part and several det-parts .	S-53
OWN	The head-part is the first phrase in the definition , which describes the broader meaning of the headword .	S-54
OWN	The det-parts restrict the meaning of the head-part .	S-55
OWN	( See Figure. )	S-56
OWN	We then translated Glossme into a semantic network Paradigme .	S-57
OWN	Each entry in Glossme is mapped onto a node in Paradigme .	S-58
OWN	Paradigme has 2,851 nodes and 295,914 unnamed links between the nodes ( 103.79 links / node on the average ) .	S-59
OWN	Figureshows a sample node red_1 .	S-60
OWN	Each node consists of a headword , a word-class , an activity-value , and two sets of links : a rfrant and a rfr .	S-61
OWN	A rfrant of a node consists of several subrfrants correspond to the units of Glossme .	S-62
OWN	As shown in Figureand, a morphological analysis maps the word brownish in the second unit onto a link to the node brown_1 , and the word colour onto two links to colour_1 ( adjective ) and colour_2 ( noun ) .	S-63
OWN	A rfr of a node p records the nodes referring to p .	S-64
OWN	For example , the rfr of red_1 is a set of links to nodes ( ex. apple_1 ) that have a link to red_1 in their rfrants .	S-65
OWN	The rfr provides information about the extension of red_1 , not the intension shown in the rfrant .	S-66
OWN	Each link has thickness, which is computed from the frequency of the wordin Glossme and other information , and normalized asin each subrfrant or rfr .	S-67
OWN	Each subrfrant also has thickness ( for example , 0.333333 in the first subrfrant of red_1 ) , which is computed by the order of the units which represents significance of the definitions .	S-68
OWN	Appendix A describes the structure of Paradigme in detail .	S-69
OWN	Similarity between words is computed by spreading activation on Paradigme .	S-70
OWN	Each of its nodes can hold activity , and it moves through the links .	S-71
OWN	Each node computes its activity valueat timeas follows :	S-72
OWN	whereandare the sum of weighted activity ( at time T ) of the nodes referred in the rfrant and rfr respectively .	S-73
OWN	And ,is activity given from outside ( at time T ) ; to ` activate a node ' is to let.	S-74
OWN	The output functionsums up three activity values in appropriate proportion and limits the output value to [ 0,1 ] .	S-75
OWN	Appendix B gives the details of the spreading activation .	S-76
OWN	Activating a node for a certain period of time causes the activity to spread over Paradigme and produce an activated pattern on it .	S-77
OWN	The activated pattern approximately gets equilibrium after 10 steps , whereas it will never reach the actual equilibrium .	S-78
OWN	The pattern thus produced represents the meaning of the node or of the words related to the node by morphological analysis .	S-79
OWN	The activated pattern , produced from a word w , suggests similarity between w and any headword in LDV .	S-80
OWN	The similarityis computed in the following way .	S-81
OWN	( See also Figure)	S-82
OWN	Reset activity of all nodes in Paradigme .	S-83
OWN	Activate w with strength s(w) for 10 steps , where s(w) is significance of the word w.	S-84
OWN	Then , an activated pattern P(w) is produced on Paradigme .	S-85
OWN	Observe-- an activity value of the node w ' in P(w) .	S-86
OWN	Then ,is.	S-87
BAS	The word significanceis defined as the normalized information of the word w in the corpus.	S-88
OWN	For example , the word red appears 2,308 times in the 5,487,056 - word corpus , and the word and appears 106,064 times .	S-89
OWN	So ,andare computed as follows :	S-90
BAS	We estimated the significance of the words excluded from the word listat the average significance of their word classes .	S-91
BAS	This interpolation virtually enlarged's 5,000,000 - word corpus .	S-92
OWN	For example , let us consider the similarity between red and orange .	S-93
OWN	First , we produce an activated patternon Paradigme .	S-94
OWN	( See Figure. ) In this case , both of the nodes red_1 ( adjective ) and red_2 ( noun ) are activated with strength.	S-95
OWN	Next , we compute, and observe.	S-96
OWN	Then , the similarity between red and orange is obtained as follows :	S-97
OWN	The procedure described above can compute the similaritybetween any two words w , w ' in LDV and their derivations .	S-98
OWN	Computer programs of this procedure -- spreading activation ( in C ) , morphological analysis and others ( in Common Lisp ) -- can computewithin 2.5 seconds on a workstation ( SPARCstation 2 ) .	S-99
OWN	The similaritybetween words works as an indicator of the lexical cohesion .	S-100
OWN	The following examples illustrate thatincreases with the strength of semantic relation :	S-101
OWN	The similarityalso increases with the co-occurrence tendency of words , for example :	S-102
OWN	Note thathas direction ( from w to w ' ) , so thatmay not be equal to:	S-103
OWN	Meaningful words should have higher similarity ; meaningless words ( especially , function words ) should have lower similarity .	S-104
OWN	The similarityincreases with the significance s(w) and s ( w ' ) that represent meaningfulness of w and w ' :	S-105
OWN	Note that the reflective similarityalso depends on the significance s(w) , so that:	S-106
OWN	The similarity of words in LDV and their derivations is measured directly on Paradigme ; the similarity of extra words is measured indirectly on Paradigme by treating an extra word as a word listof its definition in LDOCE .	S-107
OWN	( Note that eachis included in LDV or their derivations . )	S-108
OWN	The similarity between the word lists W , W ' is defined as follows .	S-109
OWN	( See also Figure. )	S-110
OWN	where P(w) is the activated pattern produced from W by activating eachwith strengthfor 10 steps .	S-111
OWN	And ,is an output function which limits the value to [ 0,1 ] .	S-112
OWN	As shown in Figure, bottle_1 and wine_1 have high activity in the pattern produced from the phrase `` red alcoholic drink '' .	S-113
OWN	So , we may say that the overlapped pattern implies `` a bottle of wine '' .	S-114
OWN	For example , the similarity between linguistics and stylistics , both are the extra words , is computed as follows :	S-115
OWN	Obviously , bothand, where W is an extra word and w is not , are also computable .	S-116
OWN	Therefore , we can compute the similarity between any two headwords in LDOCE and their derivations .	S-117
TXT	This section shows the application of the similarity between words to text analysis -- measuring similarity between texts , and measuring text coherence .	S-118
OWN	Suppose a text is a word list without syntactic structure .	S-119
OWN	Then , the similaritybetween two texts X , X ' can be computed as the similarity of extra words described above .	S-120
OWN	The following examples suggest that the similarity between texts indicates the strength of coherence relation between them :	S-121
OWN	It is worth noting that meaningless iteration of words ( especially , of function words ) has less influence on the text similarity :	S-122
OWN	The text similarity provides a semantic space for text retrieval -- to recall the most similar text into the given text X .	S-123
OWN	Once the activated pattern P ( X ) of the text X is produced on Paradigme , we can compute and compare the similarityimmediately .	S-124
OWN	( See Figure. ) .	S-125
OWN	Let us consider the reflective similarityof a text X , and use the notation c ( X ) for.	S-126
OWN	Then , c ( X ) can be computed as follows :	S-127
OWN	The activated pattern P ( X ) , as shown in Figure, represents the average meaning of.	S-128
OWN	So , c ( X ) represents cohesiveness of X -- or semantic closeness of, or semantic compactness of X .	S-129
OWN	( It is also closely related to distortion in clustering . )	S-130
OWN	The following examples suggest that c ( X ) indicates the strength of coherence of X :	S-131
OWN	= 0.502510 ( coherent ) ,	S-132
OWN	= 0.250840 ( incoherent ) .	S-133
OWN	However , a cohesive text can be incoherent ; the following example shows cohesiveness of the incoherent text -- three sentences randomly selected from LDOCE :	S-134
OWN	= 0.560172 ( incoherent , but cohesive ) .	S-135
OWN	Thus , c ( X ) can not capture all the aspects of text coherence .	S-136
OWN	This is because c ( X ) is based only on the lexical cohesion of the words in X .	S-137
OWN	The structure of Paradigme represents the knowledge system of English , and an activated state produced on it represents word meaning .	S-138
TXT	This section discusses the nature of the structure and states of Paradigme , and also the nature of the similarity computed on it .	S-139
OWN	The set of all the possible activated patterns produced on Paradigme can be considered as a semantic space where each state is represented as a point .	S-140
OWN	The semantic space is a 2,851 - dimensional hypercube ; each of its edges corresponds to a word in LDV .	S-141
OWN	LDV is selected according to the following information : the word frequency in written English , and the range of contexts in which each word appears .	S-142
OWN	So , LDV has a potential for covering all the concepts commonly found in the world .	S-143
OWN	This implies the completeness of LDV as dimensions of the semantic space .	S-144
CTR	's semantic differential procedure used 50 adjective dimensions ; our semantic measurement uses 2,851 dimensions with completeness and objectivity .	S-145
OWN	Our method can be applied to construct a semantic network from an ordinary dictionary whose defining vocabulary is not restricted .	S-146
OWN	Such a network , however , is too large to spread activity over it .	S-147
OWN	Paradigme is the small and complete network for measuring the similarity .	S-148
OWN	The proposed similarity is based only on the denotational and intensional definitions in the dictionary LDOCE .	S-149
OWN	Lack of the connotational and extensional knowledge causes some unexpected results of measuring the similarity .	S-150
OWN	For example , consider the following similarity :	S-151
OWN	This is due to the nature of the dictionary definitions -- they only indicate sufficient conditions of the headword .	S-152
OWN	For example , the definition of tree in LDOCE tells nothing about leaves : tree n 1 a tall plant with a wooden trunk and branches , that lives for many years 2 a bush or other plant with a treelike form 3 a drawing with a branching form , esp .	S-153
OWN	as used for showing family relationships However , the definition is followed by pictures of leafy trees providing readers with connotational and extensional stereotypes of trees .	S-154
OWN	In the proposed method , the definitions in LDOCE are treated as word lists , though they are phrases with syntactic structures .	S-155
OWN	Let us consider the following definition of lift :	S-156
OWN	Anyone can imagine that something is moving upward .	S-157
OWN	But , such a movement can not be represented in the activated pattern produced from the phrase .	S-158
OWN	The meaning of a phrase , sentence , or text should be represented as pattern changing in time , though what we need is static and paradigmatic relation .	S-159
OWN	This paradox also arises in measuring the similarity between texts and the text coherence .	S-160
OWN	As we have seen in Section, there is a difference between the similarity of texts and the similarity of word lists , and also between the coherence of a text and cohesiveness of a word list .	S-161
OWN	However , so far as the similarity between words is concerned , we assume that activated patterns on Paradigme will approximate the meaning of words , like a still picture can express a story .	S-162
AIM	We described measurement of semantic similarity between words .	S-163
OWN	The similarity between words is computed by spreading activation on the semantic network Paradigme which is systematically constructed from a subset of the English dictionary LDOCE .	S-164
OWN	Paradigme can directly compute the similarity between any two words in LDV , and indirectly the similarity of all the other words in LDOCE .	S-165
OWN	The similarity between words provides a new method for analysing the structure of text .	S-166
OWN	It can be applied to computing the similarity between texts , and measuring the cohesiveness of a text which suggests coherence of the text , as we have seen in Section.	S-167
OWN	And , we are now applying it to text segmentation,, i.e. to capture the shifts of coherent scenes in a story .	S-168
OWN	In future research , we intend to deal with syntagmatic relations between words .	S-169
OWN	Meaning of a text lies in the texture of paradigmatic and syntagmatic relations between words.	S-170
OWN	Paradigme provides the former dimension -- an associative system of words -- as a screen onto which the meaning of a word is projected like a still picture .	S-171
OWN	The latter dimension -- syntactic process -- will be treated as a film projected dynamically onto Paradigme .	S-172
OWN	This enables us to measure the similarity between texts as a syntactic process , not as word lists .	S-173
OWN	We regard Paradigme as a field for the interaction between text and episodes in memory -- the interaction between what one is hearing or reading and what one knows.	S-174
OWN	The meaning of words , sentences , or even texts can be projected in a uniform way on Paradigme , as we have seen in Sectionand.	S-175
OWN	Similarly , we can project text and episodes , and recall the most relevant episode for interpretation of the text .	S-176
OWN	The semantic network Paradigme is systematically constructed from the small and closed English dictionary Glossme .	S-177
OWN	Each entry of Glossme is mapped onto a node of Paradigme in the following way .	S-178
OWN	( See also Figureand. )	S-179
OWN	For each entryin Glossme , map each unitinonto a subrfrantof the corresponding nodein Paradigme .	S-180
OWN	Each wordis mapped onto a link or links in, in the following way :	S-181
OWN	Letbe the reciprocal of the number of appearance of( as its root form ) in Glossme .	S-182
OWN	Ifis in a head-part , letbe doubled .	S-183
OWN	Find nodescorresponds to( ex. red{ red_1 , red_2 } ) .	S-184
OWN	Then , divideintoin proportion to their frequency .	S-185
OWN	Add linksto, whereis a link to the nodewith thickness.	S-186
OWN	Thus ,becomes a set of links :, whereis a link with thickness.	S-187
OWN	Then , normalize thickness of the links as, in each.	S-188
OWN	Step 2 .	S-189
OWN	For each node, compute thicknessof each subrfrantin the following way :	S-190
OWN	Letbe the number of subrfrants of.	S-191
OWN	Letbe.	S-192
OWN	( Note that= 2:1 . )	S-193
OWN	Normalize thicknessas, in each.	S-194
OWN	Step 3 .	S-195
OWN	Generate rfr of each node in Paradigme , in the following way :	S-196
OWN	For each nodein Paradigme , let its rfrbe an empty set .	S-197
OWN	For each, for each subrfrantof, for each linkin:	S-198
OWN	Letbe the node referred by, and letbe thickness of.	S-199
OWN	Add a new link l ' to rfr of, where l ' is a link towith thickness.	S-200
OWN	Thus , eachbecomes a set of links :, whereis a link with thickness.	S-201
OWN	Then , normalize thickness of the links as, in each.	S-202
OWN	Each nodeof the semantic network Paradigme computes its activity valueat timeas follows :	S-203
OWN	whereandare activity ( at time T ) collected from the nodes referred in the rfrant and rfr respectively ;is activity given from outside ( at time T ) ; the output functionlimits the value to [ 0,1 ] .	S-204
OWN	is activity of the most plausible subrfrant in, defined as follows :	S-205
OWN	whereis thickness of the j-th subrfrant of.	S-206
OWN	is the sum of weighted activity of the nodes referred in the j-th subrfrant of, defined as follows :	S-207
OWN	whereis thickness of the k-th link of, andis activity ( at time T ) of the node referred by the k-th link of.	S-208
OWN	is weighted activity of the nodes referred in the rfrof:	S-209
OWN	whereis thickness of the k-th link of, andis activity ( at time T ) of the node referred by the k-th link of.	S-210
