AIM	In this paper we provide a probabilistic interpretation for typed feature structures very similar to those used by.	A-0
OWN	We begin with a version of the interpretation which lacks a treatment of re-entrant feature structures , then provide an extended interpretation which allows them .	A-1
OWN	We sketch algorithms allowing the numerical parameters of our probabilistic interpretations of HPSG to be estimated from corpora .	A-2
AIM	The purpose of our paper is to develop a principled technique for attaching a probabilistic interpretation to feature structures .	S-0
BAS	Our techniques apply to the feature structures described by.	S-1
BAS	Since these structures are the ones which are used in bytheir relevance to computational grammars is apparent .	S-2
BKG	On the basis of the usefulness of probabilistic context-free grammars, it is plausible to assume that that the extension of probabilistic techniques to such structures will allow the application of known and new techniques of parse ranking and grammar induction to more interesting grammars than has hitherto been the case .	S-3
TXT	The paper is structured as follows .	S-4
TXT	We start by reviewing the training and use of probabilistic context-free grammars ( PCFGs ) .	S-5
TXT	We then develop a technique to allow analogous probabilistic annotations on type hierarchies .	S-6
OWN	This gives us a clear account of the relationship between a large class of feature structures and their probabilities , but does not treat re-entrancy .	S-7
TXT	We conclude by sketching a technique which does treat such structures .	S-8
CTR	While we know of previous work which associates scores with feature structuresare not aware of any previous treatment which makes explicit the link to classical probability theory .	S-9
BAS	We take a slightly unconventional perspective on feature structures , because it is easier to cast our theory within the more general framework of incremental description refinementthan to exploit the usual metaphors of constraint-based grammar .	S-10
OWN	In fact we can afford to remain entirely agnostic about the means by which the HPSG grammar associates signs with linguistic strings , because all that we need in order to train our stochastic procedures is a corpus of signs which are known to be valid descriptions of strings .	S-11
TXT	We review the standard probabilistic interpretation of PCFGs .	S-12
OTH	A PCFG is a four-tuple, where W is a set of terminal symbols, N is a set of non-terminal symbols,is the starting symbol and R is a set of rules of the form, whereis a string of terminals and non-terminals .	S-13
OTH	Each rule has a probabilityand the probabilities for all the rules that expand a given non-terminal must sum to one .	S-14
OTH	We associate probabilities with partial phrase markers , which are sets of terminal and non-terminal nodes generated by beginning from the starting node successively expanding non-terminal leaves of the partial tree .	S-15
OTH	Phrase markers are those partial phrase markers which have no non-terminal leaves .	S-16
OTH	Probabilities are assigned by the following inductive definition :	S-17
OTH	If T is a partial phrase marker , and T ' is a partial phrase marker which differs from it only in that a single non-terminal nodein T has been expanded toin T ' , then.	S-19
OTH	In this definition R acts as a specification of the accessibility relationships which can hold between nodes of the trees admitted by the grammar .	S-20
OTH	The rule probabilities specify the cost of making particular choices about the way in which the rules develop .	S-21
OWN	It is going to turn out that an exactly analogous system of accessibility relations is present in the probabilistic type hierarchies which we define later .	S-22
OTH	The definition of PCFGs implies that the probability of a phrase marker depends only on the choice of rules used in expanding non-terminal nodes .	S-23
OTH	In particular , the probability does not depend on the order in which the rules are applied .	S-24
CTR	This has the arguably unwelcome consequence that PCFGs are unable to make certain discriminations between trees which differ only in their configuration .	S-25
OWN	The models developed in this paper build in similar independence assumptions .	S-26
BKG	A large part of the art of probabilistic language modelling resides in the management of the trade-off between descriptive power ( which has the merit of allowing us to make the discriminations which we want ) and independence assumptions ( which have the merit of making training practical by allowing us to treat similar situations as equivalent ) .	S-27
OTH	The crucial advantage of PCFGs over CFGs is that they can be trained and / or learned from corpora .	S-28
OTH	Readers for whom this fact is unfamiliar are referred to's textbook, Chapter 7 .	S-29
TXT	We do not have space to recapitulate the discussion of training which can be found there .	S-30
TXT	We do however illustrate the outcome of training .	S-31
OTH	Consider the simple grammar in figureand its training against the corpus in figure.	S-32
OTH	Since there are 3 plural sentences and only 2 singular sentences , the optimal set of parameters will reflect the distribution found in the corpus , as shown in figure.	S-33
OTH	One might have hoped that the ratiowould be 2/3 , but it is instead.	S-34
OTH	This is a consequence of the assumption of independence .	S-35
OTH	Effectively the algorithm is ascribing the difference in distribution of singular and plural sentences to the joint effect of two independent decisions .	S-36
OWN	What we would really like it to do is to recognize that the two apparently independent decisions are ( in effect ) one and the same .	S-37
CTR	Also , because the grammar has no means of enforcing number agreement , the system systematically prefers plurals to singulars , even when doing this will lead to agreement clashes .	S-38
CTR	Thus `` buses stop '' has estimated, `` bus stop '' and `` buses stops '' both have probabilityand `` bus stops '' has probability.	S-39
CTR	This behaviour is clearly unmotivated by the corpus , and arises purely because of the inadequacy of the probabilistic model .	S-40
OTH	's ALEallows the user to define the type hierarchy of a grammar by writing a collection of clauses which together denote an inheritance hierarchy , a set of features and a set of appropriateness conditions .	S-41
OTH	An example of such a hierarchy is given in ALE syntax in figure.	S-42
OTH	The inheritance information tells us that a sign is a forced choice between a sentence and a phrase , that a phrase is a forced choice between a noun-phrase ( np ) and a verb-phrase ( vp ) and that number values ( num ) are partitioned into singular ( sing ) and plural ( pl ) .	S-43
OTH	The features which are defined are left , right , and num , and the appropriateness information says that the feature num introduces a new instance of the type num on all phrases , and that left and right introduce np and vp respectively on sentences .	S-44
OWN	The parallel which makes it possible to apply the PCFG training scheme almost unchanged is that the sub-types of a given super-type partition the feature structures of that type in just the same way that the different rules which expand a given non-terminal N of the PCFG partition the space of trees whose topmost node is N .	S-45
OWN	Equally , the features defined in the hierarchy act as an accessibility relation between nodes in a way which is for our purposes entirely equivalent to the way in which the right hand sides of the rules introduce new nodes into partial phrase markers .	S-46
OWN	The hierarchy in figureis related to but not isomorphic with the grammar in figure.	S-47
OWN	One difference is that num is explicitly introduced as a feature in the hierarchy , where at is only implicitly present in the original grammar .	S-48
OWN	The other difference is the use of left and right as models of the dominance relationships between nodes .	S-49
OWN	For our purposes , a probabilistic type hierarchy ( PTH ) is a four-tuple	S-50
OWN	where MT is a set of maximal types, NT is a set of non-maximal types,is the starting symbol and I is a set of introduction relationships of the form, whereis a multiset of maximal and non-maximal types .	S-51
OWN	Each introduction relationship has a probabilityand the probabilities for all the introduction relationships that apply to a given non-maximal type must sum to one .	S-52
OWN	As things stand this definition is nearly isomorphic to that given for PCFGs , with the major differences being two changes which move us from rules to introduction relationships .	S-53
OWN	Firstly , we relax the stipulation that the items on the right hand side of the rules are strings , allowing them instead to be multisets .	S-54
OWN	Secondly , we introduce an additional term in the head of introduction rules to signal the fact that when we apply a particular introduction relationship to a node we also specialize the type of the node by picking exactly one of the direct subtypes of its current type .	S-55
OWN	Finally , we need to deal with the case whereis non-maximal .	S-56
OWN	This is simply achieved by defining the iterated introduction relationships fromas being those corresponding to the chains of introduction relationships fromwhich refine the type to a maximal type .	S-57
OWN	In the probabilistic type hierarchy , it is the iterated introduction relationships which correspond to the context-free rewrite rules of a PCFG .	S-58
OWN	A useful side-effect of this is that we can preserve the invariant that all types except those at the fringe of the structure are maximal .	S-59
OWN	The hierarchy whose ALE syntax is given in figureis captured in the new notation by figure.	S-60
OWN	We associate probabilities with feature structures , which are sets of maximal and non-maximal nodes generated by beginning from the starting node and successively expanding non-maximal leaves of the partial tree .	S-61
OWN	Maximally specified feature structures are those feature structures which have only maximal leaves .	S-62
OWN	Probabilities are assigned by the following inductive definition :	S-63
OWN	If F is a feature structure , and F ' is a partial feature structure which differs from it only in that a single non-maximal nodeof typein F has been refined to typeexpanded toin F ' , then.	S-65
OWN	Modulo notation , this definition is identical to the one given earlier for PCFGs .	S-66
OWN	Given the correspondence between the definitions of a PTH and a PCFG it should be apparent that the training methods which apply to one can equally be used with the other .	S-67
OWN	We will shortly provide an example .	S-68
OWN	Because we have not yet treated the crucial matter of re-entrancy , it would be inappropriate to call what we so far have stochastic HPSG , so we refer to it as stochastic.	S-69
OWN	Using the hierarchy in figurethe analyses of the five sentences from figureare as in figure.	S-70
OWN	Training is a matter of counting the transitions which are found the observed results , then using counts to refine initial estimates of the probabilities of particular transitions .	S-71
OWN	This is entirely analogous to what went on with PCFGs .	S-72
OWN	The results of training are essentially identical to those given earlier , with the optimal assignment being as shown in figure.	S-73
OWN	At this point we have provided a system which allows us to use feature structures instead of PCFGs , but we have not yet dealt with the question of re-entrancy , which forms a crucial part of the expressive power of typed feature structures .	S-74
OWN	We will return to this shortly , but first we consider the detailed implications of what we have done so far .	S-75
OWN	The similarities between these results and those in figure	S-76
OWN	We still model the distribution observed in the corpus by assuming two independent decisions .	S-77
OWN	We still get a strange ranking of the parses , which favours number disagreement , in spite of the fact that the grammar which generated the corpus enforces number agreement .	S-78
OWN	The differences between these results and the earlier ones are :	S-79
OWN	The hierarchy uses bot rather than s as its start symbol .	S-80
OWN	The probabilities tell us that the corpus contains no free-standing structures of type num .	S-81
OWN	The zero probability of	S-82
OWN	codifies a similar observation that there are no free-standing structures with type phrase .	S-83
OWN	Since items of type phrase are never introduced at that type , but only in the form of sub-types , there are no transitions from phrase in the corpus .	S-84
OWN	Therefore the initial estimates of the probabilities of such transitions are unaffected by training .	S-85
OWN	In the PCFG the symmetry between the expansions of np and vp to singular and plural variants is implicit , whereas in the PTH the distribution of singular and plural variants is encoded at a single location , namely that at which num is refined .	S-86
OWN	The independence assumption which is built into the training algorithm is that types are to be refined according to the same probability distribution irrespective of the context in which they are expanded .	S-87
OWN	We have already seen a consequence of this : the PTH lumps together all occasions where num is expanded , irrespective of whether the enclosing context is np or vp .	S-88
OWN	For the moment we are prepared to tolerate this because :	S-89
OWN	Clarity :	S-90
OWN	The decisions which we have made lead to a system with a clear probabilistic semantics .	S-91
OWN	Trainability :	S-92
OWN	the number of parameters which must be estimated for a grammar is a linear function of the size of the type hierarchy	S-93
OWN	Easy extensibility :	S-94
OWN	There is a clear route to a more finely grained account if we allow the expansion probabilities to be conditioned on surrounding context .	S-95
OWN	This would increase the number of parameters to be estimated , which may or may not prove to be a problem .	S-96
OWN	We now turn to an extension of the system which takes proper account of re-entrancies in the structure .	S-97
OWN	The essence of our approach is to define a stochastic procedure which simultaneously expands the nodes of the tree in the way outlined above and guesses the pattern of re-entrancies which relate them .	S-98
OWN	It pays to stipulate that the structures which we build are fully inequated in the sense defined by, p120 .	S-99
OWN	The essential insight is that the choice of a fully inequated feature structure involving a set of nodes is the same thing as the choice of an arbitrary equivalence relation over these nodes , and this is in turn equivalent to the choice of a partition of the set of nodes into a set of non-empty sets .	S-100
OWN	These sets of nodes are equivalence classes .	S-101
OWN	The standard recursive procedure for generating partitions of k + 1 elements is to non-deterministically add the k+1thq node to each of the equivalence classes of each of the partitions of k nodes , and also to nondeterministically consider the new node as a singleton set .	S-102
OWN	The basis of the stochastic procedure for generating fully-inequated feature structures is to interleave the generation of equivalence classes with the expansion from the initial node as described above .	S-103
OWN	For the purposes of the expansion algorithm , a fully inequated feature structure consists of a feature tree ( as before ) and an equivalence relation over all the maximal nodes in that tree .	S-104
OWN	The task of the algorithm is to generate all such structures and to equip them with probabilities .	S-105
OWN	We proceed as in the case without re-entrancy , except that we only ever expand sub-trees in the case where the new node begins a new equivalence class .	S-106
OWN	This avoids the double counting which was a problem earlier .	S-107
OWN	The remaining task is that of assigning scores to equivalence relations .	S-108
OWN	We do not have a fully satisfactory solution to this problem .	S-109
OWN	The reason for this is that we would ideally like to assign probabilities to intermediate structures in such a way that the probabilities of fully expanded structures are independent of the route by which they were arrived at .	S-110
OWN	This can be done , and the method which we adopt has the merit of simplicity .	S-111
OWN	We associate a single probabilistic parameterwith each type T , and derive the probability of the structure in which a particular pairwise equation of nodes in type T have been equated by multiplying the probability of the structure in which no decision has been made by.	S-112
OWN	We derive the probability of the corresponding inequated structure by multiplying byin an entirely analogous way .	S-113
OWN	This ensures that the probabilities of the equated and inequated extensions of the original structure sum to the original probability .	S-114
OWN	The cost is a deficiency in modelling , since this takes no account of the fact that token identity of nodes is transitive .	S-115
OWN	which are generated .	S-116
OWN	As things stand the stochastic procedure is free to generate structures where,but, which are not in fact legal feature structures .	S-117
OWN	This leads to distortions of the probability estimates since the training algorithm spends part of its probability mass on impossible structures .	S-118
OWN	Even a crude account of re-entrancy is better than completely ignoring the issue , and the one proposed gets the right result for cases of double counting such as those discussed above , but it should be obvious that there is room for improvement in the treatment which we provide .	S-119
OWN	Intuitively what is required is a parametrisable means of distributing probability mass among the distinct equivalence relations which extend the current structure .	S-120
OWN	One attractive possibility would be to enumerate the relations which can be obtained by adding the current node to the various different equivalence classes which are available , apply some scoring function to each class , and then normalize such that the total score over all alternatives is one .	S-121
OWN	But this might introduce unpleasant dependencies of the probabilities of feature structures on the order in which the stochastic procedure chooses to expand nodes , because the normalisation is carried out before we have full knowledge of the equivalence classes with which the current node might become associated .	S-122
OWN	It may be that an appropriate choice of scoring function will circumvent this difficulty , but this is left as a matter for further research .	S-123
AIM	We have presented two proposals for the association of probabilities with typed feature-structures of the form used in HPSG .	S-124
OWN	As far as we know these are the most detailed of their type , and the ones which are most likely to be able to exploit standard training and parsing algorithms .	S-125
OWN	For typed feature structures lacking re-entrancy we believe our proposal to be the simplest and most natural which is available .	S-126
OWN	The proposal for dealing with re-entrancy is less satisfactory but offers a basis for empirical exploration , and has definite advantages over the straightforward use of PCFGs .	S-127
OWN	We plan to follow up the current work by training and testing a suitable instantiation of our framework against manually annotated corpora .	S-128
