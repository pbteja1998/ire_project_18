<?xml version='1.0' encoding='ISO-8859-1'?>
<!DOCTYPE PAPER SYSTEM "paper-structure.dtd">
<PAPER>
<METADATA>
<FILENO>9409004</FILENO>
<REFLABEL>Ribas 1994b</REFLABEL>
<APPEARED><CONFERENCE>COLING</CONFERENCE><YEAR>1994</YEAR></APPEARED>
<CLASSIFICATION> Lg.Pr.Sm Lg.Pr.Cl </CLASSIFICATION>
</METADATA>
<TITLE> An Experiment on Learning Appropriate Selectional Restrictions from a Parsed Corpus </TITLE>
<AUTHORLIST>
<AUTHOR>Francesc Ribas Framis</AUTHOR>
</AUTHORLIST>
<ABSTRACT>
<A-S ID='A-0' DOCUMENTC='S-15' AZ='AIM'> We present a methodology to extract Selectional Restrictions at a variable level of abstraction from phrasally analyzed corpora . </A-S>
<A-S ID='A-1' AZ='OWN'> The method relays in the use of a wide-coverage noun taxonomy and a statistical measure of the co-occurrence of linguistic items . </A-S>
<A-S ID='A-2' AZ='OWN'> Some experimental results about the performance of the method are provided . </A-S>
</ABSTRACT>
<BODY>
<DIV DEPTH='1'>
<HEADER ID='H-0'> Introduction </HEADER>
<P>
<S ID='S-0' AZ='BKG'> These last years there has been a common agreement in the natural language processing research community on the importance of having an extensive coverage of the surface lexical semantics of the domain to work with , ( specially , typical contexts of use ) . </S>
<S ID='S-1' AZ='BKG'> This knowledge may be expressed at different levels of abstraction depending on the phenomena involved : selectional restrictions ( SRs ) , lexical preferences , col-locations , etc . </S>
<S ID='S-2' AZ='BKG'> We are specially interested on SRs , which can be expressed as semantic type constraints that a word sense imposes on the words with which it combines in the process of semantic interpretation . </S>
<S ID='S-3' AZ='BKG'> SRs must include information on the syntactic position of the words that are being restricted semantically . </S>
<S ID='S-4' AZ='BKG'> For instance , one of the senses of the verb drink restricts its subject to be an animal and its object to be a liquid . </S>
</P>
<P>
<S ID='S-5' AZ='BKG'> SRs may help a parser to prefer some parses among several grammatical ones <REF TYPE='P'>Whittemore et al. 1990</REF> . </S>
<S ID='S-6' AZ='BKG'> Furthermore , SRs may help the parser when deciding the semantic role played by a syntactic complement . </S>
<S ID='S-7' AZ='BKG'> Lexicography is also interested in the acquisition of SRs . </S>
<S ID='S-8' AZ='BKG'> On the one hand , SRs are an interesting information to be included in dictionaries ( defining in context approach ) . </S>
<S ID='S-9' AZ='BKG'> On the other hand , as <REF TYPE='A'>Church and Hanks 1990</REF> remark , the effort involved in analyzing and classifying all the linguistic material provided by concordances of use of a word can be extremely labor-intensive . </S>
<S ID='S-10' AZ='BKG'> If it was possible to represent roughly the SRs of the word being studied , it could be possible to classify roughly the concordances automatically in the different word uses before the lexicographer analysis . </S>
</P>
<P>
<S ID='S-11' AZ='BKG'> The possible sources of SRs are : introspection by lexicographers , machine-readable dictionaries , and on-line corpora . </S>
<S ID='S-12' AZ='BKG'> The main advantage of the latter is that they provide experimental evidence of words uses . </S>
<S ID='S-13' AZ='OTH'> Recently , several approaches on acquiring different kinds of lexical information from corpora have been developed <REF TYPE='P'>Basili et al. 1992</REF> , <REF TYPE='P'>Church et al. 1991</REF> , <REF  TYPE='P'>Church and Hanks 1990</REF>, <REF  TYPE='P'>Resnik 1992</REF> . </S>
<S ID='S-14' AZ='AIM'> This paper is interested in exploring the amenability of using a method for extracting SRs from textual data , in the line of these works . </S>
<S ID='S-15' ABSTRACTC='A-0' AZ='AIM'> The aim of the proposed technique is to learn the SRs that a word is imposing , from the analysis of the examples of use of that word contained in the corpus . </S>
<S ID='S-16' AZ='BKG'> An illustration of such a learning is shown in Figure <CREF/> , where the system , departing from the three examples of use , and knowing that prosecutor , buyer and lawmaker are nouns belonging to the semantic class <EQN/> , and that indictment , assurance and legislation are members of <EQN/> , should induce that the verb seek imposes SRs that constraint the subject to be a member of the semantic type <EQN/> , and the object to be a kind of <EQN/> . </S>
<S ID='S-17' AZ='BKG'> Concluding , the system should extract for each word ( with complements ) having enough number occurrences of use in the corpus and for each of its syntactic complements , a list of the alternative SRs that this word is imposing . </S>
</P>
<IMAGE ID='I-0'/>
<P>
<S ID='S-18' AZ='OTH'> In order to detect the SRs that a word imposes in its context by means of statistical techniques two distinct approaches have been proposed : word-based <REF TYPE='P'>Church et al. 1991</REF> , and class-based <REF  TYPE='P'>Basili et al. 1992</REF>, <REF  TYPE='P'>Resnik 1992</REF> . </S>
<S ID='S-19' AZ='OTH'> Word-based approach infers SRs as the collection of words that co-occur significantly in the syntactic context of the studied word . </S>
<S ID='S-20' AZ='OTH'> The class-based techniques gather the different nouns by means of semantic classes . </S>
<S ID='S-21' AZ='OTH'> The advantages of the latter are clear . </S>
<S ID='S-22' AZ='OTH'> On the one hand , statistically meaningful data can be gathered from ( relatively ) small corpora , and not only for the most frequent words . </S>
<S ID='S-23' AZ='OTH'> On the other hand , SRs are generalized to new examples not present in the training set . </S>
<S ID='S-24' AZ='OTH'> Finally , the acquired SRs are more independent of the lexical choices made in the training corpus . </S>
</P>
<P>
<S ID='S-25' AZ='AIM'> We have developed and implemented a method for automatically extracting class-based SRs from on-line corpora . </S>
<S ID='S-26' AZ='TXT'> In section <CREF/> we describe it while discussing other approaches . </S>
<S ID='S-27' AZ='TXT'> In section <CREF/> we analyze some data about the performance of an experiment run in a Unix machine , on a corpus of 800,000 words . </S>
<S ID='S-28' AZ='TXT'> Finally , in section <CREF/> we discuss the performance achieved , and suggest further refinements of the technique in order to solve some remaining problems . </S>
</P>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-1'> The Method of Acquisition </HEADER>
<P>
<S ID='S-29' AZ='BKG'> SRs have been used to express semantic constraints holding in different syntactic and functional configurations . </S>
<S ID='S-30' AZ='AIM'> However , in this paper we focus only in selectional restrictions holding between verbs and their complements . </S>
<S ID='S-31' AZ='OTH'> The method can be easily exported to other configurations . </S>
<S ID='S-32' AZ='OTH'> We won't distinguish the SRs imposed by verbs on arguments and adjuncts . </S>
<S ID='S-33' AZ='OTH'> We believe that few adjuncts are going to provide enough evidence in the corpus for creating SRs . </S>
<S ID='S-34' AZ='TXT'> In the following paragraphs we describe the functional specification of the system . </S>
</P>
<P>
<S ID='S-35' AZ='OWN'> Training set  </S>
<S ID='S-36' AZ='OWN'> The input to the learning process is a list of co-occurrence triples codifying the co-occurrence of verbs and complement heads in the corpus : ( verb , syntactic relationship , noun ) . </S>
<S ID='S-37' AZ='OWN'> Verb and noun are the lemmas of the inflected forms appearing in text . </S>
<S ID='S-38' AZ='OWN'> Syntactic relationship codes the kind of complement : 0 subject , 1 object , or preposition in case it is a PP . </S>
<S ID='S-39' AZ='TXT'> A method to draw the co-occurrence triples from corpus is proposed in subsection <CREF/> . </S>
</P>
<P>
<S ID='S-40' AZ='OWN'> Output </S>
<S ID='S-41' AZ='OWN'> The result of the learning process is a set of syntactic SRs , ( verb , syntactic relationship , semantic class ) . </S>
<S ID='S-42' AZ='OWN'> Semantic classes are represented extensionally as sets of nouns . </S>
<S ID='S-43' AZ='OWN'> SRs are only acquired if there are enough cases in the corpus as to gather statistical evidence . </S>
<S ID='S-44' AZ='OWN'> As long as distinct uses of the same verb can have different SRs , we permit to extract more than one class for the same syntactic position . </S>
<S ID='S-45' AZ='OWN'> Nevertheless , they must be mutually disjoint , i.e. not related by hyperonymy .</S>
</P>
<P>
<S ID='S-46' AZ='OWN'> Previous knowledge used </S>
<S ID='S-47' AZ='OWN'> In the process of learning SRs , the system needs to know how words are clustered in semantic classes , and how semantic classes are hierarchically organized . </S>
<S ID='S-48' AZ='OWN'> Ambiguous words must be represented as having different hyperonym classes . </S>
<S ID='S-49' AZ='TXT'> In subsection <CREF/> we defend the use of a broad-coverage taxonomy . </S>
</P>
<P>
<S ID='S-50' AZ='OWN'> Learning process . </S>
<S ID='S-51' AZ='OWN'> The computational process is divided in three stages : </S>
<S ID='S-52' TYPE='ITEM' AZ='OWN'> Guessing the possible semantic classes , i.e. creation of the space of candidates . </S>
<S ID='S-53' AZ='OWN'> In principle , all the hyperonyms ( at all levels ) of the nouns appearing in the training set are candidates . </S>
</P>
<P>
<S ID='S-54' AZ='OWN'> Evaluation of the appropriateness of the candidates . </S>
<S ID='S-55' AZ='OWN'> In order to compare the different candidates , an statistical measure summarizing the relevance of the occurrence of each of the candidate classes is used . </S>
<S ID='S-56' TYPE='ITEM' AZ='OWN'> Selection of the most appropriate subset of the candidate space to convey the SRs , taking into account that the final classes must be mutually disjoint . </S>
<S ID='S-57' AZ='TXT'> While in subsection <CREF/> an statistical measure to fulfill stage <CREF/> is presented , stages <CREF/> and <CREF/> are discussed in <CREF/> thoroughly . </S>
</P>
<DIV DEPTH='2'>
<HEADER ID='H-2'> Extracting Co-occurrence Triples </HEADER>
<P>
<S ID='S-58' AZ='BKG'> In any process of learning from examples the accuracy of the training set is the base for the system to make correct predictions . </S>
<S ID='S-59' AZ='OWN'> In our case , where the semantic classes are hypothesized not univoquely from the examples , accuracy becomes fundamental . </S>
</P>
<P>
<S ID='S-60' AZ='OTH'> Different approaches to obtain lexical co-occurrences have been proposed in the literature <REF TYPE='P'>Basili et al. 1992</REF> , <REF  TYPE='P'>Church et al. 1991</REF>, <REF  TYPE='P'>Church 1990</REF> . </S>
<S ID='S-61' AZ='CTR'> These approaches seem inappropriate for tackling our needs , either because they detect only local co-occurrences <REF TYPE='P'>Church et al. 1991</REF> , <REF TYPE='P'>Church and Hanks 1990</REF> , or because they extract many spurious co-occurrence triples <REF  TYPE='P'>Basili et al. 1992</REF>, <REF  TYPE='P'>Church and Hanks 1990</REF> . </S>
<S ID='S-62' AZ='OWN'> On the one hand , our system intends to learn SRs on any kind of verb 's complements . </S>
<S ID='S-63' AZ='CTR'> On the other hand , the fact that these approaches extract co-occurrences without reliability on being verb-complements violates accuracy requirements . </S>
</P>
<P>
<S ID='S-64' AZ='OWN'> However , if the co-occurrences were extracted from a corpus annotated with structural syntactic information ( i.e. , part of speech and `` skeletal '' trees ) , the results would have considerably higher degrees of accuracy and representativity . </S>
<S ID='S-65' AZ='OWN'> In this way , it would be easy to detect all the relationships between verb and complements , and few non-related co-occurrences would be extracted . </S>
<S ID='S-66' AZ='OWN'> The most serious objection to this approach is that the task of producing syntactic analyzed corpora is very expensive . </S>
<S ID='S-67' AZ='OWN'> Nevertheless , lately there has been a growing interest to produce skeletally analyzed corpora . </S>
</P>
<P>
<S ID='S-68' AZ='OWN'> A parser , with some simple heuristics , would be enough to meet the requirements of representativeness and accuracy introduced above . </S>
<S ID='S-69' AZ='OWN'> On the other hand , it could be useful to represent the co-occurrence triples as holding between lemmas , in order to gather as much evidence as possible . </S>
<S ID='S-70' AZ='OWN'> A simple morphological analyzer that could get the lemma for a big percentage of the words appearing in the corpus would suffice . </S>
</P>
</DIV>
<DIV DEPTH='2'>
<HEADER ID='H-3'> Semantic Knowledge Used </HEADER>
<P>
<S ID='S-71' AZ='OTH'> Of the two class-based approaches presented in section <CREF/> , <REF TYPE='A'>Resnik 1992</REF> 's technique uses a wide-coverage semantic taxonomy , whereas <REF TYPE='A'>Basili et al. 1992</REF> consists in hand-tagging with a fixed set of semantic labels . </S>
<S ID='S-72' AZ='CTR'> The advantages and drawbacks of both approaches are diverse . </S>
<S ID='S-73' AZ='OTH'> On the one hand , in <REF TYPE='A'>Basili et al. 1992</REF> approach , semantic classes relevant to the domain are chosen , and consequently , the adjustment of the classes to the corpus is quite nice . </S>
<S ID='S-74' AZ='OTH'> Nevertheless , <REFAUTHOR>Resnik</REFAUTHOR> 's system is less constrained and is able to induce a most appropriate level for the SRs . </S>
<S ID='S-75' AZ='CTR'> On the other hand , while <REF TYPE='A'>Basili et al. 1992</REF> implies hand-coding all the relevant words with semantic tags , <REF TYPE='A'>Resnik 1992</REF> needs a broad semantic taxonomy . </S>
<S ID='S-76' AZ='BAS'> However , there is already an available taxonomy , WordNet . </S>
<S ID='S-77' AZ='BAS'> We take <REF TYPE='A'>Resnik 1992</REF> approach because of the better results obtained , and the lower cost involved . </S>
</P>
</DIV>
<DIV DEPTH='2'>
<HEADER ID='H-4'> Class appropriateness : the Association Score </HEADER>
<P>
<S ID='S-78' AZ='OWN' TYPE='ITEM'> When trying to choose a measure of the appropriateness of a semantic class , we have to consider the features of the problem : </S>
<S ID='S-79' TYPE='ITEM' AZ='OWN'> robustness in front of noise , and </S>
<S ID='S-80' TYPE='ITEM' AZ='OWN'> conservatism in order to be able to generalize only from positive examples , without having the tendency to over-generalize . </S>
<S ID='S-81' AZ='OTH'> Several statistical measures that accomplish these requirements have been proposed in the literature <REF TYPE='P'>Basili et al. 1992</REF> , <REF  TYPE='P'>Church et al. 1991</REF>, <REF  TYPE='P'>Resnik 1992</REF> . </S>
<S ID='S-82' AZ='BAS'> We adopt <REF TYPE='A'>Resnik 1992</REF> 's approach , which quantifies the statistical association between verbs and classes of nouns from their co-occurrence . </S>
<S ID='S-83' AZ='OWN'> However we adapt it taking into account the syntactic position of the relationship . </S>
<S ID='S-84' AZ='OWN'> Let </S>
<IMAGE ID='I-1'/>
<IMAGE ID='I-2'/>
</P>
<P>
<S ID='S-85' AZ='OWN'> be the sets of all verbs , nouns , syntactic positions , and possible noun classes , respectively . </S>
<S ID='S-86' AZ='OWN'> Given <EQN/> , Association Score , Assoc , between v and c in a syntactic position s is defined to be </S>
<IMAGE ID='I-3'/>
<IMAGE ID='I-4'/>
</P>
<P>
<S ID='S-87' AZ='OWN'> Where conditional probabilities are estimated by counting the number of observations of the joint event and dividing by the frequency of the given event , e.g. </S>
<IMAGE ID='I-5'/>
</P>
<P>
<S ID='S-88' AZ='OWN'> The two terms of Assoc try to capture different properties of the SR expressed by the candidate class . </S>
<S ID='S-89' AZ='OWN'> Mutual information , <EQN/> , measures the strength of the statistical association between the given verb v and the candidate class c in the given syntactic position s . </S>
<S ID='S-90' AZ='OWN'> If there is a real relationship , then hopefully <EQN/> . </S>
<S ID='S-91' AZ='OWN'> On the other hand , the conditional probability , <EQN/> , favors those classes that have more occurrences of nouns . </S>
</P>
</DIV>
<DIV DEPTH='2'>
<HEADER ID='H-5'> Selecting the best classes </HEADER>
<P>
<S ID='S-92' AZ='OWN'> The existence of noise in the training set introduces classes in the candidate space that can't be considered as expressing SRs . </S>
<S ID='S-93' AZ='OWN'> A common technique used for ignoring as far as possible this noise is to consider only those events that have a higher number of occurrences than a certain threshold . </S>
<S ID='S-94' AZ='OWN'> However , some erroneous classes may persist because they exceed the threshold . </S>
<S ID='S-95' AZ='OWN'> However , if candidate classes were ordered by the significance of their Assoc with the verb , it is likely that less appropriate classes ( introduced by noise ) would be ranked in the last positions of the candidate list . </S>
</P>
<P>
<S ID='S-96' AZ='OWN'> The algorithm to learn SRs is based in a search through all the classes with more instances in the training set than the given threshold . </S>
<S ID='S-97' AZ='OWN'> In different iterations over these candidate classes , two operations are performed : first , the class , c , having the best Assoc ( best class ) , is extracted for the final result ; and second , the remaining candidate classes are filtered from classes being hyper / hyponyms to the best class . </S>
<S ID='S-98' AZ='OWN'> This last step is made because the definitive classes must be mutually disjoint . </S>
<S ID='S-99' AZ='OWN'> The iterations are repeated until the candidate space has been run out . </S>
</P>
<P>
<S ID='S-100' AZ='CTR'> <REF TYPE='A'>Resnik 1992</REF> performed a similar learning process , but while he was only looking for the preferred class of object nouns , we are interested in all the possible classes ( SRs ) . </S>
<S ID='S-101' AZ='OTH'> He performed a best-first search on the candidate space . </S>
<S ID='S-102' AZ='CTR'> However , if the function to maximize doesn't have a monotone behavior ( as it is the case of Assoc ) the best-first search doesn't guarantee global optimals , but only local ones . </S>
<S ID='S-103' AZ='OWN'> This fact made us to decide for a global search , specially because the candidate space is not so big . </S>
</P>
</DIV>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-6'> Experimental Results </HEADER>
<P>
<S ID='S-104' AZ='OWN'> In order to experiment the methodology presented , we implemented a system in a Unix machine . </S>
<S ID='S-105' AZ='OWN'> The corpus used for extracting co-occurrence triples is a fragment of parsed material from the Penn Treebank Corpus ( about 880,000 words and 35,000 sentences ) , consisting of articles of the Wall Street Journal , that has been tagged and parsed . </S>
<S ID='S-106' AZ='OWN'> We used Wordnet as the verb and noun lexicons for the lemmatizer , and also as the semantic taxonomy for clustering nouns in semantic classes . </S>
<S ID='S-107' AZ='TXT' TYPE='ITEM'> In this section we evaluate the performance of the methodology implemented : </S>
<S ID='S-108' TYPE='ITEM' AZ='TXT' > looking at the performance of the techniques used for extracting triples , </S>
<S ID='S-109' TYPE='ITEM' AZ='TXT' > considering the coverage of the WordNet taxonomy regarding the noun senses appearing in Treebank , and</S>
<S ID='S-110' TYPE='ITEM' AZ='TXT' > analyzing the performance of the learning process . </S>
<IMAGE ID='I-6'/>
</P>
<P>
<S ID='S-111' AZ='OWN'> The total number of co-occurrence triples extracted amounts to 190,766 . </S>
<S ID='S-112' AZ='OWN'> Many of these triples ( 68,800 , 36.1 % ) were discarded before the lemmatizing process because the surface NP head wasn't a noun . </S>
<S ID='S-113' AZ='OWN'> The remaining 121,966 triples were processed through the lemmatizer . </S>
<S ID='S-114' AZ='OWN'> 113,583 ( 93.1 % ) could be correctly mapped into their corresponding lemma form . </S>
</P>
<P>
<S ID='S-115' AZ='OWN'> In addition , we analyzed manually the results obtained for a subset of the extracted triples , looking at the sentences in the corpus where they occurred . </S>
<S ID='S-116' AZ='OWN'> The subset contains 2,658 examples of four average common verbs in the Treebank : rise , report , seek and present ( from now on , the testing sample ) . </S>
<S ID='S-117' AZ='OWN'> On the one hand , 235 ( 8.8 % ) of these triples were considered to be extracted erroneously because of the parser , and 51 ( 1.9 % ) because of the lemmatizer . </S>
<S ID='S-118' AZ='OWN'> Summarizing , 2,372 ( 89.2 % ) of the triples in the testing set were considered to be correctly extracted and lemmatized . </S>
</P>
<P>
<S ID='S-119' AZ='OWN'> When analyzing the coverage of WordNet taxonomy we considered two different ratios . </S>
<S ID='S-120' AZ='OWN'> On the one hand , how many of the noun occurrences have one or more senses included in the taxonomy : 113,583 of the 117,215 extracted triples ( 96.9 % ) . </S>
<S ID='S-121' AZ='OWN'> On the other hand , how many of the noun occurrences in the testing sample have the correct sense introduced in the taxonomy : 2,165 of the 2,372 well-extracted triples ( 92.3 % ) . </S>
<S ID='S-122' AZ='OWN'> These figures give a positive evaluation of the coverage of WordNet . </S>
</P>
<P>
<S ID='S-123' AZ='OWN'> In order to evaluate the performance of the learning process we inspected manually the SRs acquired on the testing-sample , assessing if they corresponded to the actual SRs imposed . </S>
<S ID='S-124' AZ='OWN'> A first way of evaluation is by means of measuring precision and recall ratios in the testing sample . </S>
<S ID='S-125' AZ='OWN'> In our case , we define precision as the proportion of triples appearing in syntactic positions with acquired SRs , which effectively fulfill one of those SRs . </S>
<S ID='S-126' AZ='OWN'> Precision amounts to 79.2 % . </S>
<S ID='S-127' AZ='OWN'> The remaining 20.8 % triples didn't belong to any of the classes induced for their syntactic positions . </S>
<S ID='S-128' AZ='OWN'> Some of them because they didn't have the correct sense included in the WordNet taxonomy , and others because the correct class had not been induced because there wasn't enough evidence . </S>
<S ID='S-129' AZ='OWN'> On the other hand , we define recall as the proportion of triples which fulfill one of the SRs acquired for their corresponding syntactic positions . </S>
<S ID='S-130' AZ='OWN'> Recall amounts to 75.7 . </S>
</P>
<P>
<S ID='S-131' AZ='OWN'> A second way of evaluating the performance of the abstraction process is to manually diagnose the reasons that have made the system to deduce the SRs obtained . </S>
<S ID='S-132' AZ='OWN'> Table <CREF/> shows the SRs corresponding to the subject position of the verb seek . </S>
<S ID='S-133' AZ='OWN'> Type indicates the diagnostic about the class appropriateness . </S>
<S ID='S-134' AZ='OWN'> Assoc , the value of the association score . </S>
<S ID='S-135' AZ='OWN'> `` # n '' , the number of nouns appearing in the corpus that are contained in the class . </S>
<S ID='S-136' AZ='OWN'> Finally , `` # s '' indicates the number of actual noun senses used in the corpus which are contained in the class . </S>
<S ID='S-137' AZ='OWN'> In this table we can see some examples of the five types of manual diagnostic : </S>
</P>
<P>
<S ID='S-138' AZ='OWN'> Ok </S>
<S ID='S-139' AZ='OWN'> The acquired SR is correct according to the noun senses contained in the corpus . </S>
</P>
<P>
<S ID='S-140' AZ='OWN'> <EQN/> Abs </S>
<S ID='S-141' AZ='OWN'> The best level for stating the SR is not the one induced , but a lower one . </S>
<S ID='S-142' AZ='OWN'> It happens because erroneous senses , metonymies , ... , accumulate evidence for the higher class . </S>
</P>
<P>
<S ID='S-143' AZ='OWN'> <EQN/> Abs </S>
<S ID='S-144' AZ='OWN'> Some of the SRs could be best gathered in a unique class . </S>
<S ID='S-145' AZ='OWN'> We didn't find any such case . </S>
</P>
<P>
<S ID='S-146' AZ='OWN'> Senses </S>
<S ID='S-147' AZ='OWN'> The class has cropped up because it accumulates enough evidence , provided by erroneous senses . </S>
</P>
<P>
<S ID='S-148' AZ='OWN'> Noise </S>
<S ID='S-149' AZ='OWN'> The class accumulates enough evidence provided by erroneously extracted triples . </S>
</P>
<P>
<S ID='S-150' AZ='OWN'> Table <CREF/> shows the incidence of the diagnostic types in the testing sample . </S>
<S ID='S-151' AZ='OWN'> Each row shows : the type of diagnostic , the number and percentage of classes that accomplish it , and the number and percentage of noun occurrences contained by these classes in the testing sample . </S>
<S ID='S-152' AZ='OWN'> Analyzing the results obtained from the testing sample ( some of which are shown in tables <CREF/> and <CREF/> ) we draw some positive ( <CREF/> , <CREF/> ) and some negative conclusions ( <CREF/> , <CREF/> , <CREF/> and <CREF/> ) : </S>
</P>
<P>
<S ID='S-153' AZ='OWN'> Almost one correct semantic class for each syntactic position in the sample is acquired . </S>
<S ID='S-154' AZ='OWN'> The technique achieves a good coverage , even with few co-occurrence triples . </S>
</P>
<P>
<S ID='S-155' AZ='OWN'> Although many of the classes acquired result from the accumulation of incorrect senses ( 73.3 % ) , it seems that their size tends to be smaller than classes in other categories , as they only contain a 51.4 % of the senses . </S>
</P>
<P>
<S ID='S-156' AZ='OWN'> There doesn't seem to be a clear co-relation between Assoc and the manual diagnostic . </S>
<S ID='S-157' AZ='OWN'> Specifically , the classes considered to be correct sometimes aren't ranked in the higher positions of the Assoc ( e.g. , Table <CREF/> ) . </S>
</P>
<P>
<S ID='S-158' AZ='OWN'> The over-generalization seems to be produced because of little difference in the nouns included in the rival classes . </S>
<S ID='S-159' AZ='OWN'> Nevertheless this situation is rare . </S>
</P>
<P>
<S ID='S-160' AZ='OWN'> The impact of noise provided by erroneous extraction of co-occurrence triples , in the acquisition of wrong semantic classes , seems to be very moderate . </S>
</P>
<P>
<S ID='S-161' AZ='OWN'> Since different verb senses occur in the corpus , the SRs acquired appear mixed . </S>
<IMAGE ID='I-7'/>
</P>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-7'> Further Work </HEADER>
<P>
<S ID='S-162' AZ='OWN'> Although performance of the technique presented is pretty good , some of the detected problems could possibly be solved . </S>
<S ID='S-163' AZ='OWN'> Specifically , there are various ways to explore in order to reduce the problems stated in points <CREF/> and <CREF/> above : </S>
</P>
<P>
<S ID='S-164' AZ='OWN'> To measure the Assoc by means of Mutual Information between the pair v-s and c . </S>
<S ID='S-165' AZ='OWN'> In this way , the syntactic position also would provide information ( statistical evidence ) for measuring the most appropriate classes . </S>
</P>
<P>
<S ID='S-166' AZ='OWN'> To modify the Assoc in such a way that it was based in a likelihood ratio test <REF TYPE='P'>Dunning 1993</REF> . </S>
<S ID='S-167' AZ='OWN'> It seems that this kind of tests have a better performance than mutual information when the counts are small , as it is the case . </S>
</P>
<P>
<S ID='S-168' AZ='OWN'> To estimate the probabilities of classes , not directly from the frequencies of their noun members , but correcting this evidence by the number of senses of those nouns , e. g . </S>
<IMAGE ID='I-8'/>
</P>
<P>
<S ID='S-169' AZ='OWN'> In this way , the estimated function would be a probability distribution , and more interesting , nouns would provide evidence on the occurrence of their hyperonyms , inversely proportional to their degree of ambiguity . </S>
</P>
<P>
<S ID='S-170' AZ='OWN'> To collect a bigger number of examples for each verbal complement , projecting the complements in the internal arguments , using diathesis sub-categorization rules . </S>
<S ID='S-171' AZ='OWN'> Hopefully , Assoc would have a better performance if it was estimated on a bigger population . </S>
<S ID='S-172' AZ='OWN'> On the other hand , in this way it would be possible to detect the SRs holding on internal arguments . </S>
</P>
<P>
<S ID='S-173' AZ='OWN' TYPE='ITEM'> In order to solve point d above , we have foreseen two possibilities : </S>
</P>
<P>
<S ID='S-174' TYPE='ITEM' AZ='OWN'> To take into consideration the statistical significance of the alternatives involved , before doing a generalization step , climbing upwards , </S>
<S ID='S-175' TYPE='ITEM' AZ='OWN'> To use the PPs that in the corpus are attached to other complements and not to the main verb as a source of `` implicit negative examples '' , in such a way that they would constrain the over-generalization . </S>
</P>
<P>
<S ID='S-176' AZ='OWN'> Finally , It would be interesting to investigate the solution to point f. One possible way would be to disambiguate the senses of the verbs appearing in the corpus , using the SRs already acquired and gathering evidence of the patterns corresponding to each sense by means of a technique similar to that used by <REF TYPE='A'>Yarowsky 1992</REF> . </S>
<S ID='S-177' AZ='OWN'> Therefore , once disambiguated the verb senses it would be possible to split the set of SRs acquired . </S>
</P>
<P>
<S ID='S-178' AZ='OTH'> Some of the future lines of research outlined above have been already investigated and their results included in <REF SELF="YES" TYPE='A'>Ribas 1994a</REF> . </S>
</P>
</DIV>
</BODY>
<REFERENCELIST>
<REFERENCE>
R. <SURNAME>Basili</SURNAME>, M.T. <SURNAME>Pazienza</SURNAME>, and P. <SURNAME>Velardi</SURNAME>.
Computational lexicons: the neat examples and the odd exemplars.
In Proc. of the 3rd ANLP, <DATE>1992</DATE>.
</REFERENCE>
<REFERENCE>
K.W. <SURNAME>Church</SURNAME>, W. <SURNAME>Gale</SURNAME>, P. <SURNAME>Hanks</SURNAME>, and D. <SURNAME>Hindle</SURNAME>.
Using statistics in lexical analysis.
In U. Zernik, editor, Lexical Acquisition: Exploiting On-Line
  Resources to Build a Lexicon. Lawrence Erlbaum, <DATE>1991</DATE>.
</REFERENCE>
<REFERENCE>
K.W. <SURNAME>Church</SURNAME> and P. <SURNAME>Hanks</SURNAME>.
Word association norms, mutual information and lexicography.
Computational Linguistics, 16(1), <DATE>1990</DATE>.
</REFERENCE>
<REFERENCE>
T. <SURNAME>Dunning</SURNAME>.
Accurate methods for the statistics of surprise and coincidence.
Computational Linguistics, 19(1), <DATE>1993</DATE>.
</REFERENCE>
<REFERENCE>
G. <SURNAME>Miller</SURNAME>, R. <SURNAME>Beckwith</SURNAME>, C. <SURNAME>Fellbaum</SURNAME>, D. <SURNAME>Gross</SURNAME>, and K. <SURNAME>Miller</SURNAME>.
Five papers on wordnet.
Technical report, CSL, Princeton University, <DATE>1990</DATE>.
</REFERENCE>
<REFERENCE>
Mitchell P. <SURNAME>Marcus</SURNAME>, Beatrice <SURNAME>Santorini</SURNAME>, and Mary <SURNAME>Ann</SURNAME> Marcinkiewicz.
Building a large annotated corpus of english: the Penn Treebank.
Computational Linguistics, 19(2), <DATE>1993</DATE>.
</REFERENCE>
<REFERENCE>
P. <SURNAME>Resnik</SURNAME>.
Wordnet and distributional analysis: A class-based approach to
  lexical discovery.
In Proc. of AAAI Workshop on Statistical Methods in NLP, <DATE>1992</DATE>.
</REFERENCE>
<REFERENCE>
F. <SURNAME>Ribas</SURNAME>.
Learning more appropriate selectional restrictions.
Technical report, ESPRIT BRA-7315 ACQUILEX-II Working Paper, August
  <DATE>1994</DATE>.
</REFERENCE>
<REFERENCE>
G. <SURNAME>Whittemore</SURNAME>, K. <SURNAME>Ferrara</SURNAME>, and H. <SURNAME>Brunner</SURNAME>.
Empirical study of predictive powers of simple attachment schemes for
  post-modifier prepositional phrases.
In Proc. of the 28th ACL, <DATE>1990</DATE>.
</REFERENCE>
<REFERENCE>
David <SURNAME>Yarowsky</SURNAME>.
Word-sense disambiguation using statistical models of Roget's
  categories trained on large corpora.
In Proceedings of COLING-92, Nantes, France, <DATE>1992</DATE>.
</REFERENCE>
</REFERENCELIST>
</PAPER>
