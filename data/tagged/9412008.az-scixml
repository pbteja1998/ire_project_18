<?xml version='1.0' encoding='ISO-8859-1'?>
<!DOCTYPE PAPER SYSTEM "paper-structure.dtd">
<PAPER>
<METADATA>
<FILENO>9412008</FILENO>
<APPEARED><CONFERENCE>COLING</CONFERENCE><YEAR>1994</YEAR></APPEARED>
<CLASSIFICATION> Lg.Pr.St Lg.Pr.Ml </CLASSIFICATION>
</METADATA>
<TITLE> Analysis of Japanese Compound Nouns using Collocational Information </TITLE>
<AUTHORLIST>
<AUTHOR>Yosiyuki Kobayasi</AUTHOR>
<AUTHOR>Takenobu Tokunaga</AUTHOR>
<AUTHOR>Hozumi Tanaka</AUTHOR>
</AUTHORLIST>
<ABSTRACT>
<A-S ID='A-0' DOCUMENTC='S-0' AZ='BKG'> Analyzing compound nouns is one of the crucial issues for natural language processing systems , in particular for those systems that aim at a wide coverage of domains . </A-S>
<A-S ID='A-1' DOCUMENTC='S-22' AZ='AIM'> In this paper , we propose a method to analyze structures of Japanese compound nouns by using both word collocations statistics and a thesaurus . </A-S>
<A-S ID='A-2' AZ='OWN'> An experiment is conducted with 160,000 word collocations to analyze compound nouns of with an average length of 4.9 characters . </A-S>
<A-S ID='A-3' DOCUMENTC='S-163' AZ='OWN'> The accuracy of this method is about 80 % . </A-S>
</ABSTRACT>
<BODY>
<DIV DEPTH='1'>
<HEADER ID='H-0'> Introduction </HEADER>
<P>
<S ID='S-0' ABSTRACTC='A-0' AZ='BKG'> Analyzing compound nouns is one of the crucial issues for natural language processing systems , in particular for those systems that aim at a wide coverage of domains . </S>
<S ID='S-1' AZ='BKG'> Registering all compound nouns in a dictionary is an impractical approach , since we can create a new compound noun by combining nouns . </S>
<S ID='S-2' AZ='BKG'> Therefore , a mechanism to analyze the structure of a compound noun from the individual nouns is necessary . </S>
</P>
<P>
<S ID='S-3' AZ='BKG'> In order to identify structures of a compound noun , we must first find a set of words that compose the compound noun . </S>
<S ID='S-4' AZ='BKG'> This task is trivial for languages such as English , where words are separated by spaces . </S>
<S ID='S-5' AZ='BKG'> The situation is worse , however , in Japanese where no spaces are placed between words . </S>
<S ID='S-6' AZ='BKG'> The process to identify word boundaries is usually called segmentation . </S>
<S ID='S-7' AZ='BKG'> In processing languages such as Japanese , ambiguities in segmentation should be resolved at the same time as analyzing structure . </S>
<S ID='S-8' AZ='BKG'> For instance , the Japanese compound noun `` SinGataKansetuZei '' ( new indirect tax ) , produces <EQN/> segementations possibilities for this case ( by consulting a Japanese dictionary , we would filter out some ) . </S>
<S ID='S-9' AZ='BKG'> In this case , we have two remaining possibilities : `` Sin ( new ) / Gata ( type ) / Kansetu ( indirect ) / Zei ( tax ) '' and `` SinGata ( new ) / Kansetu ( indirect ) / Zei ( tax ) '' . </S>
<S ID='S-10' AZ='BKG'> We must choose the correct segmentation , `` SinGata ( new ) / Kansetu ( indirect ) / Zei ( tax ) '' and analyze structure . </S>
</P>
<P>
<S ID='S-11' AZ='BKG'> Segmentation of Japanese is difficult only when using syntactic knowledge . </S>
<S ID='S-12' AZ='OWN'> Therefore , we could not always expect a sequence of correctly segmented words as an input to structure analysis . </S>
<S ID='S-13' AZ='OWN'> The information of structures is also expected to improve segmentation accuracy . </S>
</P>
<P>
<S ID='S-14' AZ='OTH'> There are several researches that are attacking this problem . </S>
<S ID='S-15' AZ='OTH'> <REFAUTHOR>Fuzisaki et al.</REFAUTHOR> applied the HMM model to segmentation and probabilistic CFG to analyzing the structure of compound nouns <REF TYPE='P'>Fijisaki et al. 1991</REF> . </S>
<S ID='S-16' AZ='OTH'> The accuracy of their method is 73 % in identifying correct structures of kanzi character sequences with average length is 4.2 characters . </S>
<S ID='S-17' AZ='OTH'> In their approach , word boundaries are identified through purely statistical information ( the HMM model ) without regarding such linguistic knowledge , as dictionaries . </S>
<S ID='S-18' AZ='CTR'> Therefore , the HMM model may suggest an improper character sequence as a word . </S>
<S ID='S-19' AZ='CTR'> Furthermore , since nonterminal symbols of CFG are derived from a statistical analysis of word collocations , their number tends to be large and so the number of CFG rules are also large . </S>
<S ID='S-20' AZ='CTR'> They assumed compound nouns consist of only one character words and two character words . </S>
<S ID='S-21' AZ='CTR'> It is questionable whether this method can be extended to handle cases that include more than two character words without lowering accuracy . </S>
</P>
<P>
<S ID='S-22' ABSTRACTC='A-1' AZ='AIM'> In this paper , we propose a method to analyze structures of Japanese compound nouns by using word collocational information and a thesaurus . </S>
<S ID='S-23' AZ='OWN'> The collocational information is acquired from a corpus of four kanzi character words . </S>
<S ID='S-24' AZ='OWN' TYPE='ITEM'> The outline of procedures to acquire the collocational information is as follows : </S>
</P>
<P>
<S ID='S-25' TYPE='ITEM' AZ='OWN' > extract collocations of nouns from a corpus of four kanzi character words . </S>
<S ID='S-26' TYPE='ITEM' AZ='OWN' > replace each noun in the collocations with thesaurus categories , to obtain the collocations of thesaurus categories . </S>
<S ID='S-27' TYPE='ITEM' AZ='OWN' > count occurrence frequencies for each collocational pattern of thesaurus categories . </S>
</P>
<P>
<S ID='S-28' AZ='OWN'> For each possible structure of a compound noun , the preference is calculated based on this collocational information and the structure with the highest score wins . </S>
</P>
<P>
<S ID='S-29' AZ='OTH'> <REFAUTHOR>Hindle and Rooth</REFAUTHOR> also used collocational information to solve ambiguities of pp-attachment in English <REF TYPE='P'>Hindle and Rooth 1991</REF> . </S>
<S ID='S-30' AZ='OTH'> Ambiguities are resolved by comparing the strength of associativity between a preposition and a verb and the preposition and a nominal head . </S>
<S ID='S-31' AZ='OTH'> The strength of associativity is calculated on the basis of occurrence frequencies of word collocations in a corpus . </S>
<S ID='S-32' AZ='OWN'> Besides the word collocations information , we also use semantic knowledge , namely , a thesaurus . </S>
</P>
<P>
<S ID='S-33' AZ='TXT'> The structure of this paper is as follows : Section <CREF/> explains the knowledge for structure analysis of compound nouns and the procedures to acquire it from a corpus , Section <CREF/> describes the analysis algorithm , and Section <CREF/> describes the experiments that are conducted to evaluate the performance of our method , and Section <CREF/> summarizes the paper and discusses future research directions . </S>
</P>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-1'> Collocational Information for Analyzing Compound Nouns </HEADER>
<P>
<S ID='S-34' AZ='TXT'> This section describes procedures to acquire collocational information for analyzing compound nouns from a corpus of four kanzi character words . </S>
<S ID='S-35' AZ='OWN'> What we need is occurrence frequencies of all word collocations . </S>
<S ID='S-36' AZ='OWN'> It is not realistic , however , to collect all word collocations . </S>
<S ID='S-37' AZ='OWN'> We use collocations from thesaurus categories that are word abstractions . </S>
</P>
<P>
<S ID='S-38' AZ='OWN' TYPE='ITEM'> The procedures consist of the following four steps : </S>
</P>
<P>
<S ID='S-39' TYPE='ITEM' AZ='OWN' > collect four kanzi character words ( section <CREF/> ) </S>
<S ID='S-40' TYPE='ITEM' AZ='OWN' > divide the above words in the middle to produce pairs of two kanzi character words ; if one is not in the thesaurus , this four kanzi character word is discarded ( section <CREF/> )  </S>
<S ID='S-41' TYPE='ITEM' AZ='OWN' > assign thesaurus categories to both two kanzi character word ( section <CREF/> )  </S>
<S ID='S-42' TYPE='ITEM' AZ='OWN' > count occurrence frequencies of category collocations ( section <CREF/> )  </S>
</P>
<DIV DEPTH='2'>
<HEADER ID='H-2'> Collecting Word Collocations </HEADER>
<P>
<S ID='S-43' AZ='OWN'> We use a corpus of four kanzi character words as the knowledge source of collocational information . </S>
<S ID='S-44' AZ='OWN'> The reasons are as follows : </S>
</P>
<P>
<S ID='S-45' AZ='OWN'> In Japanese , kanzi character sequences longer than three are usually compound nouns , This tendency is confirmed by comparing the occurrence frequencies of kanzi character words in texts and those headwords in dictionaries . </S>
<S ID='S-46' AZ='OWN'> We investigated the tendency by using sample texts from newspaper articles and encyclopedias , and Bunrui Goi Hyou ( BGH for short ) , which is a standard Japanese thesaurus . </S>
<S ID='S-47' AZ='OWN'> The sample texts include about 220,000 sentences . </S>
<S ID='S-48' AZ='OWN'> We found that three character words and longer represent 4 % in the thesaurus , but 71 % in the sample texts . </S>
<S ID='S-49' AZ='OWN'> Therefore a collection of four kanzi character words would be used as a corpus of compound nouns . </S>
</P>
<P>
<S ID='S-50' AZ='OWN'> Four kanzi character sequences are useful to extract binary relations of nouns , because dividing a four kanzi character sequence in the middle often gives correct segmentation . </S>
<S ID='S-51' AZ='OWN'> Our preliminary investigation shows that the accuracy of the above heuristics is 96 % ( 961 / 1000 ) . </S>
</P>
<P>
<S ID='S-52' AZ='OTH'> There is a fairly large corpus of four kanzi character words created by <REFAUTHOR>Prof. Tanaka Yasuhito</REFAUTHOR> at Aiti Syukutoku college <REF TYPE='P'>Tanaka 1992</REF> . </S>
<S ID='S-53' AZ='OTH'> The corpus was manually created from newspaper articles and includes about 160,000 words .</S>
</P>
</DIV>
<DIV DEPTH='2'>
<HEADER ID='H-3'> Assigning Thesaurus Categories </HEADER>
<P>
<S ID='S-54' AZ='OWN'> After collecting word collocations , we must assign a thesaurus category to each word . </S>
<S ID='S-55' AZ='OWN'> This is a difficult task because some words are assigned multiple categories . </S>
<S ID='S-56' AZ='OWN'> In such cases , we have several category collocations from a single word collocation , some of which are incorrect . </S>
<S ID='S-57' AZ='OWN' TYPE='ITEM'> The choices are as follows ; </S>
</P>
<P>
<S ID='S-58' TYPE='ITEM' AZ='OWN' > use word collocations with all words is assigned a single category . </S>
<S ID='S-59' TYPE='ITEM' AZ='OWN' > equally distribute frequency of word collcations to all possible category collocations <REF TYPE='P'>Grishman and Sterling 1992</REF> . </S>
<S ID='S-60' TYPE='ITEM' AZ='OWN' > calculate the probability of each category collocation and distribute frequency based on these probabilities ; the probability of collocations are calculated by using method <CREF/> <REF TYPE='P'>Grishman and Sterling 1992</REF> . </S>
<S ID='S-61' TYPE='ITEM' AZ='OWN' > determine the correct category collocation by using statistical methods other than word collocations <REF TYPE='P'>Cowie et al. 1992</REF> , <REF TYPE='P'>Yarowsky 1992</REF> , <REF  TYPE='P'>Veronis 1990</REF>, <REF  TYPE='P'>Lesk 1986</REF> . </S>
</P>
<P>
<S ID='S-62' AZ='OWN'> Fortunately , there are few words that are assigned multiple categories in BGH. Therefore , we use method <CREF/> . </S>
<S ID='S-63' AZ='OWN'> Word collocations containing words with multiple categories represent about 1/3 of the corpus . </S>
<S ID='S-64' AZ='OWN'> If we used other thesauruses , which assign multiple categories to more words , we would need to use method <CREF/> , <CREF/> , or <CREF/> . </S>
</P>
</DIV>
<DIV DEPTH='2'>
<HEADER ID='H-4'> Counting Occurrence of Category Collocations </HEADER>
<P>
<S ID='S-65' AZ='OWN' TYPE='ITEM'> After assigning the thesaurus categories to words , we count occurrence frequencies of category collocations as follows : </S>
</P>
<P>
<S ID='S-66' TYPE='ITEM' AZ='OWN' > collect word collocations , at this time we collect only patterns of word collocations , but we do not care about occurrence frequencies of the patterns . </S>
<S ID='S-67' TYPE='ITEM' AZ='OWN' > replace thesaurus categories with words to produce category collocation patterns . </S>
<S ID='S-68' TYPE='ITEM' AZ='OWN' > count the number of category collocation patterns . </S>
</P>
<P>
<S ID='S-69' AZ='OWN'> Note : we do not care about frequencies of word collocations prior to replacing words with thesaurus categories . </S>
</P>
</DIV>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-5'> Algorithm </HEADER>
<P>
<S ID='S-70' AZ='OWN' TYPE='ITEM'> The analysis consists of three steps : </S>
</P>
<P>
<S ID='S-71' TYPE='ITEM' AZ='OWN' > enumerate possible segmentations of an input compound noun by consulting headwords of the thesaurus ( BGH ) </S>
<S ID='S-72' TYPE='ITEM' AZ='OWN' > assign thesaurus categories to all words </S>
<S ID='S-73' TYPE='ITEM' AZ='OWN' > calculate the preferences of every structure of the compound noun according to the frequencies of category collocations </S>
</P>
<P>
<S ID='S-74' AZ='OWN'> We assume that a structure of a compound noun can be expressed by a binary tree . </S>
<S ID='S-75' AZ='OWN'> We also assume that the category of the right branch of a ( sub ) tree represents the category of the ( sub ) tree itself . </S>
<S ID='S-76' AZ='OWN'> This assumption exsists because Japanese is a head-final language , a modifier is on the left of its modifiee . </S>
<S ID='S-77' AZ='OWN'> With these assumptions , a preference value of a structure is calculated by recursive function p as follows : </S>
<IMAGE ID='I-0'/>
</P>
<P>
<S ID='S-78' AZ='OWN'> where function l and r return the left and right subtree of the tree respectively , cat returns thesaurus categories of the argument . </S>
<S ID='S-79' AZ='OWN'> If the argument of cat is a tree , cat returns the category of the rightmost leaf of the tree . </S>
<S ID='S-80' AZ='OWN'> Function cv returns an associativity measure of two categories , which is calculated from the frequency of category collocation described in the previous section . </S>
<S ID='S-81' AZ='OWN'> We would use two measures for cv : <EQN/> returns the relative frequency of collation <EQN/> , which appears on the left side and <EQN/> , which appears on the right . </S>
</P>
<P>
<S ID='S-82' AZ='OWN'> Probability : </S>
<IMAGE ID='I-1'/>
</P>
<P>
<S ID='S-83' AZ='OWN'> Modified mutual information statistics ( MIS ) : </S>
<IMAGE ID='I-2'/>
</P>
<P>
<S ID='S-84' AZ='OWN'> where * means don't care . </S>
</P>
<P>
<S ID='S-85' AZ='BAS'> MIS is similar to mutual infromation used by <REFAUTHOR>Church</REFAUTHOR> to calculate semantic dependencies between words <REF TYPE='P'>Church et al. 1991</REF> . </S>
<S ID='S-86' AZ='OWN'> MIS is different from mutual information because MIS takes account of the position of the word ( left / right ) . </S>
</P>
<P>
<S ID='S-87' AZ='OWN'> Let us consider an example `` SinGataKansetuZei '' . </S>
</P>
<P>
<S ID='S-88' AZ='OWN'> Segmentation : </S>
<S ID='S-89' AZ='OWN'> two possibilities , </S>
</P>
<EXAMPLE ID='E-0'>
<EX-S> `` SinGata ( new ) / Kansetu ( indirect ) / Zei ( tax ) '' and </EX-S>
<EX-S> `` Sin ( new ) / Gata ( type ) / Kansetu ( indirect ) / Zei ( tax ) ''</EX-S>
</EXAMPLE>
<P>
<S ID='S-90' AZ='OWN'> remain as mentioned in section <CREF/> . </S>
</P>
<P>
<S ID='S-91' AZ='OWN'> Category assignment : </S>
<S ID='S-92' AZ='OWN'> assigning thesaurus categories provides : </S>
</P>
<EXAMPLE ID='E-1'>
<EX-S> `` SinGata [ 118 ] / Kansetu [ 311 ] / Zei [ 137 ] '' and </EX-S>
<EX-S> `` Sin [ 316 ] / Gata [ 118:141:111 ] / Kansetu [ 311 ] / Zei [ 137 ] . '' </EX-S>
</EXAMPLE>
<P>
<S ID='S-93' AZ='OWN'> A three-digit number stands for a thesaurus category . </S>
<S ID='S-94' AZ='OWN'> A colon `` : '' separates multiple categories assigned to a word . </S>
</P>
<P>
<S ID='S-95' AZ='OWN'> Preference calculation : </S>
<S ID='S-96' AZ='OWN'> For the case <CREF/> , the possible tructures are [ [ 118 , 311 ] , 137 ] and [ 118 , [ 311 , 137 ] ] . </S>
<S ID='S-97' AZ='OWN'> We represent a tree with a list notation . </S>
<S ID='S-98' AZ='OWN'> For the case <CREF/> , there is an ambiguity with the category `` Sin '' [ 118:141:111 ] . </S>
<S ID='S-99' AZ='OWN'> We expand the ambiguity to 15 possible structures . </S>
<S ID='S-100' AZ='OWN'> Preferences are calculated for 17 cases . </S>
<S ID='S-101' AZ='OWN'> For example , the preference of structure [ [ 118 , 311 ] , 137 ] is calculated as follows : </S>
<IMAGE ID='I-3'/>
</P>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-6'> Experiments </HEADER>
<DIV DEPTH='2'>
<HEADER ID='H-7'> Data and Analysis </HEADER>
<P>
<S ID='S-102' AZ='OWN'> We extract kanzi character sequences from newspaper editorials and columns and encyclopedia text , which has no overlap with the training corpus : 954 compound nouns consisting of four kanzi characters , 710 compound nouns consisting of five kanzi characters , and 786 compound nouns consisting of six kanzi characters are manually extracted from the set of the above kanzi character sequences . </S>
<S ID='S-103' AZ='OWN'> These three collections of compound nouns are used for test data . </S>
</P>
<P>
<S ID='S-104' AZ='OWN'> We use a thesaurus BGH , which is a standard machine readble Japanese thesaurus . </S>
<S ID='S-105' AZ='OWN'> BGH is structured as a tree with six hierarchical levels . </S>
<S ID='S-106' AZ='OWN'> Table <CREF/> shows the number of categories at all levels . </S>
<S ID='S-107' AZ='OWN'> In this experiment , we use the categories at level 3 . </S>
<S ID='S-108' AZ='OWN'> If we have more compound nouns as knowledge , we could use a finer hierarchy level . </S>
<IMAGE ID='I-4'/>
</P>
<P>
<S ID='S-109' AZ='OWN'> As mentioned in Section <CREF/> , we create a set of collocations of thesaurus categories from a corpus of four kanzi character sequences and BGH . </S>
<S ID='S-110' AZ='OWN'> We analyze the test data according to the procedures described in Section <CREF/> . </S>
<S ID='S-111' AZ='OWN'> In segmentation , we use a heuristic `` minimizing the number of content words '' in order to prune the search space . </S>
<S ID='S-112' AZ='OWN'> This heuristics is commonly used in the Japanese morphological analysis . </S>
<S ID='S-113' AZ='OWN'> The correct structures of the test data manually created in advance . </S>
</P>
</DIV>
<DIV DEPTH='2'>
<HEADER ID='H-8'> Results and Discussions </HEADER>
<P>
<S ID='S-114' AZ='OWN'> Table <CREF/> shows the result of the analysis for four , five , and six kanzi character sequences . </S>
<S ID='S-115' AZ='OWN'> `` <EQN/> '' means that the correct answer was not obtained because the heuristics is segmentation filtered out from the correct segmentation . </S>
<S ID='S-116' AZ='OWN'> The first row shows the percentage of cases where the correct answer is uniquely identified , no tie . </S>
<S ID='S-117' AZ='OWN'> The rows , denoted `` <EQN/> '' , shows the percentage of correct answers in the n-th rank . </S>
<S ID='S-118' AZ='OWN'> <EQN/> shows the percentage of correct answers ranked lower or equal to 4th place . </S>
<IMAGE ID='I-5'/>
</P>
<P>
<S ID='S-119' AZ='OWN'> Regardless , more than 90 % of the correct answers are within the second rank . </S>
<S ID='S-120' AZ='OWN'> The probabilistic measure <EQN/> provides better accuracy than the mutual information measure <EQN/> for five kanzi character compound nouns , but the result is reversed for six kanzi character compound nouns . </S>
<S ID='S-121' AZ='OWN'> The results for four kanzi character words are almost equal . </S>
<S ID='S-122' AZ='OWN'> In order to judge which measure is better , we need further experiments with longer words . </S>
</P>
<P>
<S ID='S-123' AZ='OWN'> We could not obtain correct segmentation for 11 out of 954 cases for four kanzi character words , 39 out of 710 cases for five kanzi character words , and 15 out of 787 cases for six kanzi character words . </S>
<S ID='S-124' AZ='OWN'> Therefore , the accuracy of segmentation candidates are 99 % ( 943 / 954 ) , 94.5 % ( 671 / 710 ) and 98.1 % ( 772 / 787 ) respectively . </S>
<S ID='S-125' AZ='OWN'> Segmentation failure is due to words missing from the dictionary and the heuristics we adopted . </S>
</P>
<P>
<S ID='S-126' AZ='OWN'> As mentioned in Section <CREF/> , it is difficult to correct segmentation by using only syntactic knowledge . </S>
<S ID='S-127' AZ='OWN'> We used the heuristics to reduce ambiguities in segmentation , but ambiguities may remain . </S>
<S ID='S-128' AZ='OWN'> In these experiments , there are 75 cases where ambiguities can not be solved by the heuristics . </S>
<S ID='S-129' AZ='OWN'> There are 11 such cases for four kanzi character words , 35 such cases for five kanzi character words , and 29 cases for six kanzi character words . </S>
<S ID='S-130' AZ='OWN'> For such cases , the correct segmentation can be uniquely identified by applying the structure analysis for 7 , 19 , and 17 cases , and the correct structure can be uniquely identified for 7 , 10 , and 8 cases for all collections of test data by using <EQN/> . </S>
<S ID='S-131' AZ='OWN'> On the other hand , 4 , 18 , and 21 cases correctly segmented and 7 , 11 , and 8 cases correctly analyzed their structures for all collections by using <EQN/> . </S>
</P>
<P>
<S ID='S-132' AZ='OWN'> For a sequence of segmented words , there are several possible structures . </S>
<S ID='S-133' AZ='OWN'> Table <CREF/> shows possible structures for four words sequence and their occurrence in all data collections . </S>
<S ID='S-134' AZ='OWN'> Since a compound noun of our test data consists of four , five , and six characters , there could be cases with a compound noun consisting of four , five , or six words . </S>
<S ID='S-135' AZ='OWN'> In the current data collections , however , there are no such cases . </S>
</P>
<P>
<S ID='S-136' AZ='OWN'> In table <CREF/> , we find significant deviation over occurrences of structures . </S>
<S ID='S-137' AZ='OWN'> This deviation has strong correlation with the distance between modifiers and modifees . </S>
<S ID='S-138' AZ='OWN'> The rightmost column ( labeled <EQN/> ) shows sums of distances between modifiers and modifiee contained in the structure . </S>
<S ID='S-139' AZ='OWN'> The distance is measured based on the number of words between a modifier and a modifiee . </S>
<S ID='S-140' AZ='OWN'> For instance , the distance is one , if a modifier and a modifiee are immediately adjacent . </S>
</P>
<P>
<S ID='S-141' AZ='OWN'> The correlation between the distance and the occurrence of structures tells us that a modifier tends to modify a closer modifiee . </S>
<S ID='S-142' AZ='OTH'> This tendency has been experimentally proven by <REF TYPE='A'>Maruyama et al. 1992</REF> . </S>
<S ID='S-143' AZ='OWN'> The tendency is expressed by the formula that follows : </S>
<IMAGE ID='I-6'/>
</P>
<P>
<S ID='S-144' AZ='OWN'> where d is the distance between two words and q ( d ) is the probability when two words of said distance is d and have a modification relation . </S>
</P>
<P>
<S ID='S-145' AZ='OWN'> We redifined cv by taking this tendency as the formula that follows : </S>
<IMAGE ID='I-7'/>
</P>
<P>
<S ID='S-146' AZ='OWN'> where cv ' is redifined cv. </S>
<S ID='S-147' AZ='OWN'> Table <CREF/> shows the result by using new cvs . </S>
<S ID='S-148' AZ='OWN'> We obtained significant improvement in 5 kanzi and 6 kanzi collection . </S>
<IMAGE ID='I-8'/>
<IMAGE ID='I-9'/>
</P>
<P>
<S ID='S-149' AZ='OWN'> We assumed that the thesaurus category of a tree be represented by the category of its right branch subtree because Japanese is a head-final language . </S>
<S ID='S-150' AZ='OWN'> However , when a right subtree is a word such as suffixes , this assumption does not always hold true . </S>
<S ID='S-151' AZ='OWN'> Since our ultimate aim is to analyze semantic structures of compound nouns , then dealing with only the grammatical head is not enough . </S>
<S ID='S-152' AZ='OWN'> We should take semantic heads into consideration . </S>
<S ID='S-153' AZ='OWN'> In order to do so , however , we need knowledge to judge which subtree represents the semantic features of the tree . </S>
<S ID='S-154' AZ='OWN'> This knowledge may be extracted from corpora and machine readable dictionaries . </S>
</P>
<P>
<S ID='S-155' AZ='OWN'> A certain class of Japanese nouns ( called Sahen meisi ) may behave like verbs . </S>
<S ID='S-156' AZ='OWN'> Actually , we can make verbs from these nouns by adding a special verb `` - suru '' . </S>
<S ID='S-157' AZ='OWN'> These nouns have case frames just like ordinary verbs . </S>
<S ID='S-158' AZ='OWN'> With compound nouns including such nouns , we could use case frames and selectional restrictions to analyze structures . </S>
<S ID='S-159' AZ='OWN'> This process would be almost the same as analyzing ordinary sentences . </S>
</P>
</DIV>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-9'> Concluding Remarks </HEADER>
<P>
<S ID='S-160' AZ='AIM'> We propose a method to analyze Japanese compound nouns using collocational information and a thesaurus . </S>
<S ID='S-161' AZ='AIM'> We also describe a method to acquire the collocational information from a corpus of four kanzi character words . </S>
<S ID='S-162' AZ='OWN'> The method to acquire collocational information is dependent on the Japanese character , but the method to calculate preferences of structures si applicable to any language with compound nouns . </S>
</P>
<P>
<S ID='S-163' ABSTRACTC='A-3' AZ='OWN'> The experiments show that when the method analyzes compound nouns with an average length 4.9 , it produces an accuracy rate of about 83 . </S>
</P>
<P>
<S ID='S-164' AZ='OWN' TYPE='ITEM'> We are considering those future works that follow : </S>
</P>
<P>
<S ID='S-165' TYPE='ITEM' AZ='OWN' > incorporate other syntactic information , such as affixes knowledge </S>
<S ID='S-166' TYPE='ITEM' AZ='OWN' > use another semantic information as well as thesauruses , such as selectional restriction</S>
<S ID='S-167' TYPE='ITEM' AZ='OWN' > apply this method to disambiguate other syntactic structures such as dependency relations . </S>
</P>
</DIV>
</BODY>
<REFERENCELIST>
<REFERENCE>
K. W. <SURNAME>Church</SURNAME>, W. <SURNAME>Gale</SURNAME> P. <SURNAME>Hanks</SURNAME>, and D. Hindle.
Using statistics in lexical analysis.
In Lexcal Acquisitin, chapter 6. Lawrence Erlbaum Associates,
  <DATE>1991</DATE>.
</REFERENCE>
<REFERENCE>
J. <SURNAME>Cowie</SURNAME>, J. A. <SURNAME>Guthrie</SURNAME>, and L. <SURNAME>Guthrie</SURNAME>.
Lexical disambiguation using simulated annealing.
In COLING p310, <DATE>1992</DATE>.
</REFERENCE>
<REFERENCE>
T. <SURNAME>Fujisaki</SURNAME>, F. <SURNAME>Jelinek</SURNAME>, J. <SURNAME>Cocke</SURNAME>, and E. Black <SURNAME>T</SURNAME>. Nishino.
A probabilistic parsing method for sentences disambiguation.
In Current Issues in Parsing Thchnology, chapter 10. Kluwer
  Academic Publishers, <DATE>1991</DATE>.
</REFERENCE>
<REFERENCE>
R. <SURNAME>Grishman</SURNAME> and J. <SURNAME>Sterling</SURNAME>.
Acquisition of selectional patterns.
In COLING p658, <DATE>1992</DATE>.
</REFERENCE>
<REFERENCE>
D. <SURNAME>Hindle</SURNAME> and M. <SURNAME>Rooth</SURNAME>.
Structual ambiguity and lexocal relations.
In ACL p229, <DATE>1991</DATE>.
</REFERENCE>
<REFERENCE>
M. E. <SURNAME>Lesk</SURNAME>.
Automatic sense disambiguation using machine readable dictionaries:
  How to tell a pine cone from an ice cream cone.
In ACM SIGDOC, <DATE>1986</DATE>.
</REFERENCE>
<REFERENCE>
H. <SURNAME>Maruyama</SURNAME> and S. <SURNAME>Ogino</SURNAME>.
A statistical property of Japanese phrase-to-phrase modifications.
Mathematical Linguistics, Vol. 18, No. 7, pp. 348-352, <DATE>1992</DATE>.
</REFERENCE>
<REFERENCE>
Y. <SURNAME>Tanaka</SURNAME>.
Acquisition of knowledge for natural language ;the four kanji
  character sequencies (in japanese).
In National Conference of Information Processing Society of
  Japan, <DATE>1992</DATE>.
</REFERENCE>
<REFERENCE>
J. <SURNAME>Veronis</SURNAME> and N. M. <SURNAME>Ide</SURNAME>.
Word sense disambiguation with very large neural networks extracted
  from machine readable dictionaries.
In COLING p389, <DATE>1990</DATE>.
</REFERENCE>
<REFERENCE>
D. <SURNAME>Yarowsky</SURNAME>.
Word-sense disamibiguation using stastistical models of roget's
  categories trained on large corpora.
In COLING p454, <DATE>1992</DATE>.
</REFERENCE>
</REFERENCELIST>
</PAPER>
