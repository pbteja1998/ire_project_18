<?xml version='1.0' encoding='ISO-8859-1'?>
<!DOCTYPE PAPER SYSTEM "paper-structure.dtd">
<PAPER>
<METADATA>
<FILENO>9511006</FILENO>
<REFLABEL>Resnik 1995a</REFLABEL>
<APPEARED><CONFERENCE TYPE='Workshop'>ACL</CONFERENCE><YEAR>1995</YEAR></APPEARED>
<CLASSIFICATION> Lg.Pr.Ml Lg.Pr.Sm </CLASSIFICATION>
</METADATA>
<TITLE> Disambiguating Noun Groupings with Respect to WordNet Senses </TITLE>
<AUTHORLIST>
<AUTHOR>Philip Resnik</AUTHOR>
</AUTHORLIST>
<ABSTRACT>
<A-S ID='A-0' DOCUMENTC='S-0' AZ='BKG'> Word groupings useful for language processing tasks are increasingly available , as thesauri appear on-line , and as distributional word clustering techniques improve . </A-S>
<A-S ID='A-1' DOCUMENTC='S-1' AZ='CTR'> However , for many tasks , one is interested in relationships among word senses , not words . </A-S>
<A-S ID='A-2' AZ='AIM'> This paper presents a method for automatic sense disambiguation of nouns appearing within sets of related nouns -- the kind of data one finds in on-line thesauri , or as the output of distributional clustering algorithms . </A-S>
<A-S ID='A-3' DOCUMENTC='S-89' AZ='OWN'> Disambiguation is performed with respect to WordNet senses , which are fairly fine-grained ; however , the method also permits the assignment of higher-level WordNet categories rather than sense labels . </A-S>
<A-S ID='A-4' AZ='OWN'> The method is illustrated primarily by example , though results of a more rigorous evaluation are also presented . </A-S>
</ABSTRACT>
<BODY>
<DIV DEPTH='1'>
<HEADER ID='H-0'> Introduction </HEADER>
<P>
<S ID='S-0' ABSTRACTC='A-0' AZ='BKG'> Word groupings useful for language processing tasks are increasingly available , as thesauri appear on-line , and as distributional techniques become increasingly widespread <REF TYPE='P'>Bensch and Savitch 1992</REF> , <REF TYPE='P'>Brill 1991</REF> , <REF TYPE='P'>Brown et al. 1992</REF> , <REF TYPE='P'>Grefenstette 1994</REF> , <REF TYPE='P'>McKeown and Hatzivassiloglou 1993</REF> , <REF TYPE='P'>Pereira et al. 1993</REF> , <REF TYPE='P'>Schuetze 1993</REF> . </S>
<S ID='S-1' ABSTRACTC='A-1' AZ='CTR'> However , for many tasks , one is interested in relationships among word senses , not words . </S>
<S ID='S-2' AZ='BKG'> Consider , for example , the cluster containing attorney , counsel , trial , court , and judge , used by <REF TYPE='A'>Brown et al. 1992</REF> to illustrate a `` semantically sticky '' group of words . </S>
<S ID='S-3' AZ='BKG'> As is often the case where sense ambiguity is involved , we as readers impose the most coherent interpretation on the words within the group without being aware that we are doing so . </S>
<S ID='S-4' AZ='CTR'> Yet a computational system has no choice but to consider other , more awkward possibilities -- for example , this cluster might be capturing a distributional relationship between advice ( as one sense of counsel ) and royalty ( as one sense of court ) . </S>
<S ID='S-5' AZ='CTR'> This would be a mistake for many applications , such as query expansion in information retrieval , where a surfeit of false connections can outweigh the benefits obtained by using lexical knowledge . </S>
</P>
<P>
<S ID='S-6' AZ='BKG'> One obvious solution to this problem would be to extend distributional grouping methods to word senses . </S>
<S ID='S-7' AZ='BKG'> For example , one could construct vector representations of senses on the basis of their co-occurrence with words or with other senses . </S>
<S ID='S-8' AZ='CTR'> Unfortunately , there are few corpora annotated with word sense information , and computing reliable statistics on word senses rather than words will require more data , rather than less . </S>
<S ID='S-9' AZ='CTR'> Furthermore , one widely available example of a large , manually sense-tagged corpus -- the WordNet group 's annotated subset of the Brown corpus -- vividly illustrates the difficulty in obtaining suitable data . </S>
<S ID='S-10' AZ='CTR'> It is quite small , by current corpus standards ( on the order of hundreds of thousands of words , rather than millions or tens of millions ) ; the direct annotation methodology used to create it is labor intensive ( <REF TYPE='A'>Marcus et al. 1993</REF> found that direct annotation takes twice as long as automatic tagging plus correction , for part-of-speech annotation ) ; and the output quality reflects the difficulty of the task ( inter-annotator disagreement is on the order of 10 % , as contrasted with the approximately 3 % error rate reported for part-of-speech annotation by <REFAUTHOR>Marcus et al.</REFAUTHOR> ) . </S>
</P>
<P>
<S ID='S-11' AZ='OTH'> There have been some attempts to capture the behavior of semantic categories in a distributional setting , despite the unavailability of sense-annotated corpora . </S>
<S ID='S-12' AZ='OTH'> For example , <REF TYPE='A'>Hearst and Schuetze 1993</REF> take steps toward a distributional treatment of WordNet-based classes , using <REF TYPE='A'>Schuetze 1993</REF> 's approach to constructing vector representations from a large co-occurrence matrix . </S>
<S ID='S-13' AZ='OTH'> <REF TYPE='A'>Yarowsky 1992</REF> 's algorithm for sense disambiguation can be thought of as a way of determining how Roget 's thesaurus categories behave with respect to contextual features . </S>
<S ID='S-14' AZ='OTH'> And my own treatment of selectional constraints <REF TYPE='P' SELF="YES">Resnik 1993</REF> provides a way to describe the plausibility of co-occurrence in terms of WordNet 's semantic categories , using co-occurrence relationships mediated by syntactic structure . </S>
<S ID='S-15' AZ='OTH'> In each case , one begins with known semantic categories ( WordNet synsets , Roget 's numbered classes ) and non-sense-annotated text , and proceeds to a distributional characterization of semantic category behavior using co-occurrence relationships . </S>
</P>
<P>
<S ID='S-16' AZ='CTR'> This paper begins from a rather different starting point . </S>
<S ID='S-17' AZ='OWN'> As in the above-cited work , there is no presupposition that sense-annotated text is available . </S>
<S ID='S-18' AZ='AIM'> Here , however , I make the assumption that word groupings have been obtained through some black box procedure , e.g. from analysis of unannotated text , and the goal is to annotate the words within the groupings post hoc using a knowledge-based catalogue of senses . </S>
<S ID='S-19' AZ='OWN'> If successful , such an approach has obvious benefits : one can use whatever sources of good word groupings are available -- primarily unsupervised word clustering methods , but also on-line thesauri and the like -- without folding in the complexity of dealing with word senses at the same time . </S>
<S ID='S-20' AZ='BKG'> The resulting sense groupings should be useful for a variety of purposes , although ultimately this work is motivated by the goal of sense disambiguation for unrestricted text using unsupervised methods . </S>
</P>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-1'> Disambiguation of Word Groups </HEADER>
<DIV DEPTH='2'>
<HEADER ID='H-2'> Problem Statement </HEADER>
<P>
<S ID='S-21' AZ='OWN'> Let us state the problem as follows . </S>
<S ID='S-22' AZ='OWN'> We are given a set of words <EQN/> , with each word <EQN/> having an associated set <EQN/> of possible senses . </S>
<S ID='S-23' AZ='OWN'> We assume that there exists some set <EQN/> , representing the set of word senses that an ideal human judge would conclude belong to the group of senses corresponding to the word grouping W . </S>
<S ID='S-24' AZ='AIM'> The goal is then to define a membership function <EQN/> that takes <EQN/> , <EQN/> , and W as its arguments and computes a value in [ 0,1 ] , representing the confidence with which one can state that sense <EQN/> belongs in sense grouping W ' . </S>
<S ID='S-25' AZ='OWN'> Note that , in principle , nothing precludes the possibility that multiple senses of a word are included in W ' . </S>
</P>
<P>
<S ID='S-26' AZ='BKG'> Example . </S>
<S ID='S-27' AZ='BKG'> Consider the following word group : </S>
</P>
<EXAMPLE ID='E-0'>
<EX-S> burglars thief rob mugging stray robbing lookout chase crate thieves . </EX-S>
</EXAMPLE>
<P>
<S ID='S-28' AZ='BKG'> Restricting our attention to noun senses in WordNet , only lookout and crate are polysemous . </S>
<S ID='S-29' AZ='BKG'> Treating this word group as W , one would expect <EQN/> to assign a value of 1 to the unique senses of the monosemous words , and to assign a high value to lookout 's sense as </S>
</P>
<EXAMPLE ID='E-1'>
<EX-S> lookout , lookout man , sentinel , sentry , watch , scout : a person employed to watch for something to happen . </EX-S>
</EXAMPLE>
<P>
<S ID='S-30' AZ='BKG'> Low ( or at least lower ) values of <EQN/> would be expected for the senses of lookout that correspond to an observation tower , or to the activity of watching . </S>
<S ID='S-31' AZ='OWN'> Crate 's two WordNet senses correspond to the physical object and the quantity ( i.e. , crateful , as in `` a crateful of oranges '' ) ; my own intuition is that the first of these would more properly be included in W ' than the second , and should therefore receive a higher value of <EQN/> , though of course neither I nor any other individual really constitutes an `` ideal human judge . '' </S>
</P>
</DIV>
<DIV DEPTH='2'>
<HEADER ID='H-3'> Computation of Semantic Similarity </HEADER>
<P>
<S ID='S-32' AZ='BAS'> The core of the disambiguation algorithm is a computation of semantic similarity using the WordNet taxonomy , a topic recently investigated by a number of people <REF TYPE='P'>Leacock and Chodorow 1994</REF> , <REF  SELF="YES" TYPE='P'>Resnik 1995a</REF>, <REF  TYPE='P'>Sussna 1993</REF> . </S>
<S ID='S-33' AZ='OWN'> In this paper , I restrict my attention to WordNet 's taxonomy for nouns , and take an approach in which semantic similarity is evaluated on the basis of the information content shared by the items being compared . </S>
</P>
<P>
<S ID='S-34' AZ='OWN'> The intuition behind the approach is simple : the more similar two words are , the more informative will be the most specific concept that subsumes them both . </S>
<S ID='S-35' AZ='OWN'> ( That is , their least upper bound in the taxonomy ; here a concept corresponds to a WordNet synset . ) </S>
<S ID='S-36' AZ='OTH'> The traditional method of evaluating similarity in a semantic network by measuring the path length between two nodes <REF TYPE='P'>Lee et al. 1993</REF> , <REF TYPE='P'>Rada et al. 1989</REF> also captures this , albeit indirectly , when the semantic network is just an hierarchy : if the minimal path of links between two nodes is long , that means it is necessary to go high in the taxonomy , to more abstract concepts , in order to find their least upper bound . </S>
<S ID='S-37' AZ='CTR'> However , there are problems with the simple path-length definition of semantic similarity , and experiments using WordNet show that other measures of semantic similarity , such as the one employed here , provide a better match to human similarity judgments than simple path length does <REF SELF="YES" TYPE='P'>Resnik 1995a</REF> . </S>
</P>
<P>
<S ID='S-38' AZ='OWN'> Given two words <EQN/> and <EQN/> , their semantic similarity is calculated as </S>
</P>
<IMAGE ID='I-0'/>
<P>
<S ID='S-39' AZ='OWN'> where <EQN/> is the set of WordNet synsets that subsume ( i.e. , are ancestors of ) both <EQN/> and <EQN/> , in any sense of either word . </S>
<S ID='S-40' AZ='OWN'> The concept c that maximizes the expression in <CREF/> will be referred to as the most informative subsumer of <EQN/> and <EQN/> . </S>
<S ID='S-41' AZ='OWN'> Although there are many ways to associate probabilities with taxonomic classes , it is reasonable to require that concept probability be non-decreasing as one moves higher in the taxonomy ; i.e. , that <EQN/> <EQN/> implies <EQN/> . </S>
<S ID='S-42' AZ='OWN'> This guarantees that `` more abstract '' does indeed mean `` less informative , '' defining informativeness in the traditional way in terms of log likelihood . </S>
</P>
<P>
<S ID='S-43' AZ='OWN'> Probability estimates are derived from a corpus by computing </S>
</P>
<IMAGE ID='I-1'/>
<P>
<S ID='S-44' AZ='OWN'> where <EQN/> is the set of nouns having a sense subsumed by concept c. Probabilities are then computed simply as relative frequency : </S>
</P>
<IMAGE ID='I-2'/>
<P>
<S ID='S-45' AZ='OWN'> where N is the total number of noun instances observed . </S>
<S ID='S-46' AZ='OWN'> Singular and plural forms are counted as the same noun , and nouns not covered by WordNet are ignored . </S>
<S ID='S-47' AZ='OWN'> Although the WordNet noun taxonomy has multiple root nodes , a single , `` virtual '' root node is assumed to exist , with the original root nodes as its children . </S>
<S ID='S-48' AZ='OWN'> Note that by equations <CREF/> through <CREF/> , if two senses have the virtual root node as their only upper bound then their similarity value is 0 . </S>
</P>
<P>
<S ID='S-49' AZ='OWN'> Example . </S>
<S ID='S-50' AZ='OWN'> The following table shows the semantic similarity computed for several word pairs , in each case shown with the most informative subsumer . </S>
<S ID='S-51' AZ='OWN'> Probabilities were estimated using the Penn Treebank version of the Brown corpus . </S>
<S ID='S-52' AZ='OWN'> The pairs come from an example given by <REF TYPE='A'>Church and Hanks 1989</REF> , illustrating the words that human subjects most frequently judged as being associated with the word doctor . </S>
<S ID='S-53' AZ='OWN'> ( The word sick also appeared on the list , but is excluded here because it is not a noun . )</S>
</P>
<IMAGE ID='I-3'/>
<P>
<S ID='S-54' AZ='OWN'> Doctors are minimally similar to medicine and hospitals , since these things are all instances of `` something having concrete existence , living or nonliving '' ( WordNet class entity ) , but they are much more similar to lawyers , since both are kinds of professional people , and even more similar to nurses , since both are professional people working specifically within the health professions . </S>
<S ID='S-55' AZ='OWN'> Notice that similarity is a more specialized notion than association or relatedness : doctors and sickness may be highly associated , but one would not judge them to be particularly similar . </S>
</P>
</DIV>
<DIV DEPTH='2'>
<HEADER ID='H-4'> Disambiguation Algorithm </HEADER>
<P>
<S ID='S-56' AZ='OWN'> The disambiguation algorithm for noun groups is inspired by the observation that when two polysemous words are similar , their most informative subsumer provides information about which sense of each word is the relevant one . </S>
<S ID='S-57' AZ='OWN'> In the above table , for example , both doctor and nurse are polysemous : WordNet records doctor not only as a kind of health professional , but also as someone who holds a Ph. D. , and nurse can mean not only a health professional but also a nanny . </S>
<S ID='S-58' AZ='OWN'> When the two words are considered together , however , the shared element of meaning for the two relevant senses emerges in the form of the most informative subsumer . </S>
<S ID='S-59' AZ='OWN'> It may be that other pairings of possible senses also share elements of meaning ( for example , doctor / Ph. D. and nurse / nanny are both descendants of person , individual ) . </S>
<S ID='S-60' AZ='OWN'> However , in cases like those illustrated above , the more specific or informative the shared ancestor is , the more strongly it suggests which senses come to mind when the words are considered together . </S>
<S ID='S-61' AZ='OWN'> The working hypothesis in this paper is that this holds true in general . </S>
</P>
<P>
<S ID='S-62' AZ='OWN'> Turning that observation into an algorithm requires two things : a way to assign credit to word senses based on similarity with co-occurring words , and a tractable way to generalize to the case where more than two polysemous words are involved . </S>
<S ID='S-63' AZ='OWN'> The algorithm given in Figure <CREF/> does both quite straightforwardly . </S>
</P>
<IMAGE ID='I-4'/>
<P>
<S ID='S-64' AZ='OWN'> This algorithm considers the words in W pairwise , avoiding the tractability problems in considering all possible combinations of senses for the group ( <EQN/> if each word had m senses ) . </S>
<S ID='S-65' AZ='OWN'> For each pair considered , the most informative subsumer is identified , and this pair is only considered as supporting evidence for those senses that are descendants of that concept . </S>
<S ID='S-66' AZ='OWN'> Notice that by equation <CREF/> , support [ i , k ] is a sum of log probabilities , and therefore preferring senses with high support is equivalent to optimizing a product of probabilities . </S>
<S ID='S-67' AZ='OWN'> Thus considering words pairwise in the algorithm reflects a probabilistic independence assumption . </S>
</P>
<P>
<S ID='S-68' AZ='OWN'> Example . </S>
<S ID='S-69' AZ='OWN'> The most informative subsumer for doctor and nurse is health professional , and therefore that pairing contributes support to the sense of doctor as an M.D. , but not a Ph. D. </S>
<S ID='S-70' AZ='OWN'> Similarly , it contributes support to the sense of nurse as a health professional , but not a nanny . </S>
<S ID='S-71' AZ='OWN'> The amount of support contributed by a pairwise comparison is proportional to how informative the most informative subsumer is . </S>
<S ID='S-72' AZ='OWN'> Therefore the evidence for the senses of a word will be influenced more by more similar words and less by less similar words . </S>
<S ID='S-73' AZ='OWN'> By the time this process is completed over all pairs , each sense of each word in the group has had the potential of receiving supporting evidence from a pairing with every other word in the group . </S>
<S ID='S-74' AZ='OWN'> The value assigned to that sense is then the proportion of support it did receive , out of the support possible . </S>
<S ID='S-75' AZ='OWN'> ( The latter is kept track of by array normalization in the pseudocode . )</S>
</P>
<DIV DEPTH='3'>
<HEADER ID='H-5'> Discussion </HEADER>
<P>
<S ID='S-76' AZ='BAS'> The intuition behind this algorithm is essentially the same intuition exploited by <REF TYPE='A'>Lesk 1986</REF> , <REF TYPE='A'>Sussna 1993</REF> , and others : the most plausible assignment of senses to multiple co-occurring words is the one that maximizes relatedness of meaning among the senses chosen . </S>
<S ID='S-77' AZ='CTR'> Here I make an explicit comparison with <REFAUTHOR>Sussna</REFAUTHOR> 's approach , since it is the most similar of previous work . </S>
</P>
<P>
<S ID='S-78' AZ='OTH'> <REFAUTHOR>Sussna</REFAUTHOR> gives as an example of the problem he is solving the following paragraph from the corpus of 1963 Time magazine articles used in information retrieval research ( uppercase in the Time corpus , lowercase here for readability ; punctuation is as it appears in the original corpus ) :</S>
</P>
<EXAMPLE ID='E-2'>
<EX-S> the allies after nassau in december 1960 , the u. s . first proposed to help nato develop its own nuclear strike force . but europe made no attempt to devise a plan . last week , as they studied the nassau accord between president kennedy and prime minister macmillan , europeans saw emerging the first outlines of the nuclear nato that the u. s . wants and will support . it all sprang from the anglo-u . s . crisis over cancellation of the bug-ridden skybolt missile , and the u. s . offer to supply britain and france with the proved polaris ( time , dec . 28 ) </EX-S>
</EXAMPLE>
<P>
<S ID='S-79' AZ='OTH'> From this , <REFAUTHOR>Sussna</REFAUTHOR> extracts the following noun grouping to disambiguate : </S>
</P>
<EXAMPLE ID='E-3'>
<EX-S> allies strike force attempt plan week accord president prime minister outlines support crisis cancellation bug missile france polaris time </EX-S>
</EXAMPLE>
<P>
<S ID='S-80' AZ='OTH'> These are the non-stopword nouns in the paragraph that appear in WordNet ( he used version 1.2 ) . </S>
<S ID='S-81' AZ='BAS'> The description of <REFAUTHOR>Sussna</REFAUTHOR> 's algorithm for disambiguating noun groupings like this one is similar to the one proposed here , in a number of ways : relatedness is characterized in terms of a semantic network ( specifically WordNet ) ; the focus is on nouns only ; and evaluations of semantic similarity ( or , in <REFAUTHOR>Sussna</REFAUTHOR> 's case , semantic distance ) are the basis for sense selection . </S>
<S ID='S-82' AZ='CTR'> However , there are some important differences , as well . </S>
<S ID='S-83' AZ='CTR'> First , unlike <REFAUTHOR>Sussna</REFAUTHOR> 's proposal , this algorithm aims to disambiguate groupings of nouns already established ( e.g. by clustering , or by manual effort ) to be related , as opposed to groupings of nouns that happen to appear near each other in running text ( which may or may not reflect relatedness based on meaning ) . </S>
<S ID='S-84' AZ='OWN'> This provides some justification for restricting attention to similarity ( reflected by the scaffolding of links in the taxonomy ) , as opposed to the more general notion of association . </S>
<S ID='S-85' AZ='CTR'> Second , this difference is reflected algorithmically by the fact that <REFAUTHOR>Sussna</REFAUTHOR> uses not only links but also other WordNet links such as PART-OF . </S>
<S ID='S-86' AZ='CTR'> Third , unlike <REFAUTHOR>Sussna</REFAUTHOR> 's algorithm , the semantic similarity / distance computation here is not based on path length , but on information content , a choice that I have argued for elsewhere <REF  SELF="YES" TYPE='P'>Resnik 1993</REF>, <REF  SELF="YES" TYPE='P'>Resnik 1995a</REF> . </S>
<S ID='S-87' AZ='CTR'> Fourth , the combinatorics are handled differently : <REFAUTHOR>Sussna</REFAUTHOR> explores analyzing all sense combinations ( and living with the exponential complexity ) , as well as the alternative of sequentially `` freezing '' a single sense for each of <EQN/> and using those choices , assumed to be correct , as the basis for disambiguating <EQN/> . </S>
<S ID='S-88' AZ='CTR'> The algorithm presented here falls between those two alternatives . </S>
</P>
<P>
<S ID='S-89' ABSTRACTC='A-3' AZ='CTR'> A final , important difference between this algorithm and previous algorithms for sense disambiguation is that it offers the possibility of assigning higher-level WordNet categories rather than lowest-level sense labels . </S>
<S ID='S-90' AZ='OWN'> It is a simple modification to the algorithm to assign values of <EQN/> not only to synsets directly containing words in W , but to any ancestors of those synsets -- one need only let the list of synsets associated with each word <EQN/> ( i.e. , <EQN/> in the problem statement of Section <CREF/> ) also include any synset that is an ancestor of any synset containing word <EQN/> . </S>
<S ID='S-91' AZ='OWN'> Assuming that num _ senses ( w [ i ] ) and sense [ i , k ] are reinterpreted accordingly , the algorithm will compute <EQN/> not only for the synsets directly including words in W , but also for any higher-level abstractions of them . </S>
</P>
<P>
<S ID='S-92' AZ='OWN'> Example . </S>
<S ID='S-93' AZ='OWN'> Consider the word group doctor , nurse , lawyer . </S>
<S ID='S-94' AZ='OWN'> If one were to include all subsuming concepts for each word , rather than just the synsets of which they are directly members , the concepts with non-zero values of <EQN/> would be as follows : </S>
<S ID='S-95' AZ='OWN'> For doctor : </S>
</P>
<IMAGE ID='I-5'/>
<P>
<S ID='S-96' AZ='OWN'> For nurse : </S>
</P>
<IMAGE ID='I-6'/>
<P>
<S ID='S-97' AZ='OWN'> For lawyer : </S>
</P>
<IMAGE ID='I-7'/>
<P>
<S ID='S-98' AZ='OWN'> Given assignments of <EQN/> at all levels of abstraction , one obvious method of semantic annotation is to assign the highest-level concept for which <EQN/> is at least as large as the sense-specific value of <EQN/> . </S>
<S ID='S-99' AZ='OWN'> For instance , in the previous example , one would assign the annotation health professional to both doctor and nurse ( thus explicitly capturing a generalization about their presence in the word group , at the appropriate level of abstraction ) , and the annotation professional to lawyer . </S>
</P>
</DIV>
</DIV>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-6'> Examples </HEADER>
<P>
<S ID='S-100' AZ='TXT'> In this section I present a number of examples for evaluation by inspection . </S>
<S ID='S-101' AZ='OWN'> In each case , I give the source of the noun grouping , the grouping itself , and for each word a description of word senses together with their values of <EQN/> . </S>
</P>
<DIV DEPTH='2'>
<HEADER ID='H-7'> Distributionally derived groupings </HEADER>
<P>
<S ID='S-102' AZ='OTH'> Distributional cluster <REF TYPE='P'>Brown et al. 1992</REF> : head , body , hands , eye , voice , arm , seat , hair , mouth . </S>
</P>
<IMAGE ID='I-8'/>
<P>
<S ID='S-103' AZ='OTH'> This group was among classes hand-selected by <REFAUTHOR>Brown et al.</REFAUTHOR> as `` particularly interesting . '' </S>
</P>
<P>
<S ID='S-104' AZ='OTH'> Distributional cluster <REF TYPE='P'>Brown et al. 1992</REF> : tie , jacket , suit . </S>
</P>
<IMAGE ID='I-9'/>
<P>
<S ID='S-105' AZ='OTH'> This cluster was derived by <REFAUTHOR>Brown et al.</REFAUTHOR> using a modification of their algorithm , designed to uncover `` semantically sticky '' clusters . </S>
</P>
<P>
<S ID='S-106' AZ='OTH'> Distributional cluster <REF TYPE='P'>Brown et al. 1992</REF> : cost , expense , risk , profitability , deferral , earmarks , capstone , cardinality , mintage , reseller . </S>
</P>
<IMAGE ID='I-10'/>
<P>
<S ID='S-107' AZ='OTH'> This cluster was one presented by <REFAUTHOR>Brown et al.</REFAUTHOR> as a randomly-selected class , rather than one hand-picked for its coherence . </S>
<S ID='S-108' AZ='OTH'> ( I hand-selected it from that group for presentation here , however . ) </S>
</P>
<P>
<S ID='S-109' AZ='OTH'> Distributional neighborhood <REF TYPE='P'>Schuetze 1993</REF> : burglars , thief , rob , mugging , stray , robbing , lookout , chase , crate . </S>
</P>
<IMAGE ID='I-11'/>
<P>
<S ID='S-110' AZ='OTH'> As noted in Section <CREF/> , this group represents a set of words similar to burglar , according to <REFAUTHOR>Schuetze</REFAUTHOR> 's method for deriving vector representation from corpus behavior . </S>
<S ID='S-111' AZ='OTH'> In this case , words rob and robbing were excluded because they were not nouns in WordNet . </S>
<S ID='S-112' AZ='OTH'> The word stray probably should be excluded also , since it most likely appears on this list as an adjective ( as in `` stray bullet '' ) . </S>
</P>
<P>
<S ID='S-113' AZ='OTH'> Machine-generated thesaurus entry <REF TYPE='P'>Grefenstette 1994</REF> : method , test , mean , procedure , technique . </S>
</P>
<IMAGE ID='I-12'/>
<P>
<S ID='S-114' AZ='OTH'> I chose this grouping at random from a thesaurus created automatically by <REFAUTHOR>Grefenstette</REFAUTHOR> 's syntactico-distributional methods , using the MED corpus of medical abstracts as its source . </S>
<S ID='S-115' AZ='OTH'> The group comes from from the thesaurus entry for the word method . </S>
<S ID='S-116' AZ='OTH'> Note that mean probably should be means . </S>
</P>
</DIV>
<DIV DEPTH='2'>
<HEADER ID='H-8'> Thesaurus Classes </HEADER>
<P>
<S ID='S-117' AZ='OTH'> There is a tradition in sense disambiguation of taking particularly ambiguous words and evaluating a system 's performance on those words . </S>
<S ID='S-118' AZ='TXT'> Here I look at one such case , the word line ; the goal is to see what sense the algorithm chooses when considering the word in the contexts of each of the Roget 's Thesaurus classes in which it appears , where a `` class '' includes all the nouns in one of the numbered categories . </S>
<S ID='S-119' AZ='OWN'> The following list provides brief descriptions of the 25 senses of line in WordNet : </S>
</P>
<EXAMPLE ID='E-4'>
<EX-S> wrinkle , furrow , crease , crinkle , seam , line : His face has many wrinkles </EX-S>
<EX-S> line : a length ( straight or curved ) without breadth or thickness </EX-S>
<EX-S> line , dividing line : there is a narrow line between sanity and insanity </EX-S>
<EX-S> agate line , line : space for one line of print used to measure advertising </EX-S>
<EX-S> credit line , line of credit , line : the maximum credit that a customer is allowed </EX-S>
<EX-S> line : in games or sports ; a mark indicating positions or bounds of the playing area </EX-S>
<EX-S> line : a spatial location defined by a real or imaginary unidimensional extent </EX-S>
<EX-S> course , line : a connected series of events or actions or developments </EX-S>
<EX-S> line : a formation of people or things one after ( or beside ) another l</EX-S>
<EX-S> lineage , line , line of descent , descent , bloodline , blood line , blood , pedigree </EX-S>
<EX-S> tune , melody , air , strain , melodic line , line , melodic phrase : a succession of notes </EX-S>
<EX-S> line : a linear string of words expressing some idea </EX-S>
<EX-S> line : a mark that is long relative to its width ; He drew a line on the chart </EX-S>
<EX-S> note , short letter , line : drop me a line when you get there </EX-S>
<EX-S> argumentation , logical argument , line of thought , line of reasoning , line </EX-S>
<EX-S> telephone line , phone line , line : a telephone connection </EX-S>
<EX-S> production line , assembly line , line : a factory system </EX-S>
<EX-S> pipeline , line : a long pipe used to transport liquids or gases </EX-S>
<EX-S> line : a commercial organization serving as a common carrier </EX-S>
<EX-S> line , railway line , rail line : railroad track and roadbed </EX-S>
<EX-S> line : something long and thin and flexible </EX-S>
<EX-S> cable , line , transmission line : electrical conductor connecting telephones or television </EX-S>
<EX-S> line , product line , line of products , line of merchandise , business line , line of business </EX-S>
<EX-S> line : acting in conformity ; in line with or he got out of line or toe the line </EX-S>
<EX-S> occupation , business , line of work , line : the principal activity in your life . </EX-S>
</EXAMPLE>
<P>
<S ID='S-120' AZ='OWN'> Since line appears in 13 of the numbered categories in Roget 's thesaurus , a full description of the values of <EQN/> would be too large for the present paper . </S>
<S ID='S-121' AZ='OWN'> Indeed , showing all the nouns in the numbered categories would take up too much space : they average about 70 nouns apiece . </S>
<S ID='S-122' AZ='OWN'> Instead , I identify the numbered category , and give the three WordNet senses of line for which <EQN/> was greatest . </S>
</P>
<IMAGE ID='I-13'/>
<P>
<S ID='S-123' AZ='OWN'> Qualitatively , the algorithm does a good job in most of the categories . </S>
<S ID='S-124' AZ='OWN'> The reader might find it an interesting exercise to try to decide which of the 25 senses he or she would choose , especially in the cases where the algorithm did less well ( e.g. categories #200 , #203 , #466 ) . </S>
</P>
</DIV>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-9'> Formal Evaluation </HEADER>
<P>
<S ID='S-125' AZ='TXT'> The previous section provided illustrative examples , demonstrating the performance of the algorithm on some interesting cases . </S>
<S ID='S-126' AZ='TXT'> In this section , I present experimental results using a more rigorous evaluation methodology . </S>
</P>
<P>
<S ID='S-127' AZ='OWN'> Input for this evaluation came from the numbered categories of Roget 's . </S>
<S ID='S-128' AZ='OWN'> Test instances consisted of a noun group ( i.e. , all the nouns in a numbered category ) together with a single word in that group to be disambiguated . </S>
<S ID='S-129' AZ='OWN'> To use an example from the previous section , category #590 ( `` Writing '' ) contains the following : </S>
</P>
<EXAMPLE ID='E-5'>
<EX-S> writing , chirography , penmanship , quill driving , typewriting , writing , manuscript , MS , these presents , stroke of the pen , dash of the pen , coupe de plume , line , headline , pen and ink , letter , uncial writing , cuneiform character , arrowhead , Ogham , Runes , hieroglyphic , contraction , Devanagari , Nagari , script , shorthand , stenography , secret writing , writing in cipher , cryptography , stenography , copy , transcript , rescript , rough copy , fair copy , handwriting , signature , sign manual , autograph , monograph , holograph , hand , fist , calligraphy , good hand , running hand , flowing hand , cursive hand , legible hand , bold hand , bad hand , cramped hand , crabbed hand , illegible hand , scribble , ill-formed letters , pothooks and hangers , stationery , pen , quill , goose quill , pencil , style , paper , foolscap , parchment , vellum , papyrus , tablet , slate , marble , pillar , table , blackboard , ink bottle , ink horn , ink pot , ink stand , ink well , typewriter , transcription , inscription , superscription , graphology , composition , authorship , writer , scribe , amanuensis , scrivener , secretary , clerk , penman , copyist , transcriber , quill driver , stenographer , typewriter , typist , writer for the press </EX-S>
</EXAMPLE>
<P>
<S ID='S-130' AZ='OWN'> Any word or phrase in that group that appears in the noun taxonomy for WordNet would be a candidate as a test instance -- for example , line , or secret writing . </S>
</P>
<P>
<S ID='S-131' AZ='OWN'> The test set , chosen at random , contained 125 test cases . </S>
<S ID='S-132' AZ='OWN'> ( Note that because of the random choice , there were some cases where more than one test instance came from the same numbered category . )</S>
<S ID='S-133' AZ='OWN'> Two human judges were independently given the test cases to disambiguate . </S>
<S ID='S-134' AZ='OWN'> For each case , they were given the full set of nouns in the numbered category ( as shown above ) together with descriptions of the WordNet senses for the word to be disambiguated ( as , for example , the list of 25 senses for line given in the previous section , though thankfully few words have that many senses ! ) . </S>
<S ID='S-135' AZ='OWN'> It was a forced-choice task ; that is , the judge was required to choose exactly one sense . </S>
<S ID='S-136' AZ='OWN'> In addition , for each judgment , the judge was required to provide a confidence value for this decision , ranging from 0 ( not at all confident ) to 4 ( highly confident ) . </S>
</P>
<P>
<S ID='S-137' AZ='OWN'> Results are presented here individually by judge . </S>
<S ID='S-138' AZ='OWN'> For purposes of evaluation , test instances for which the judge had low confidence ( i.e. confidence ratings of 0 or 1 ) were excluded . </S>
</P>
<P>
<S ID='S-139' AZ='OWN'> For Judge 1 , there were 99 test instances with sufficiently high confidence to be considered . </S>
<S ID='S-140' AZ='OWN'> As a baseline , ten runs were done selecting senses by random choice , with the average percent correct being 34.8 % , standard deviation 3.58 . </S>
<S ID='S-141' AZ='OWN'> As an upper bound , Judge 2 was correct on 65.7 % of those test instances . </S>
<S ID='S-142' AZ='OWN'> The disambiguation algorithm shows considerable progress toward this upper bound , with 58.6 % correct . </S>
</P>
<P>
<S ID='S-143' AZ='OWN'> For Judge 2 , there were 86 test instances with sufficiently high confidence to be considered . </S>
<S ID='S-144' AZ='OWN'> As a baseline , ten runs were done selecting senses by random choice , with the average percent correct being 33.3 % , standard deviation 3.83 . </S>
<S ID='S-145' AZ='OWN'> As an upper bound , Judge 1 was correct on 68.6 % of those test instances . </S>
<S ID='S-146' AZ='OWN'> Again , the disambiguation algorithm performs well , with 60.5 % correct . </S>
</P>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-10'> Conclusions and Future Work </HEADER>
<P>
<S ID='S-147' AZ='CTR'> The results of the evaluation are extremely encouraging , especially considering that disambiguating word senses to the level of fine-grainedness found in WordNet is quite a bit more difficult than disambiguation to the level of homographs <REF  TYPE='P'>Hearst 1991</REF>, <REF  TYPE='P'>Cowie et al. 1992</REF> . </S>
<S ID='S-148' AZ='OWN'> A note worth adding : it is not clear that the `` exact match '' criterion -- that is , evaluating algorithms by the percentage of exact matches of sense selection against a human-judged baseline -- is the right task . </S>
<S ID='S-149' AZ='OWN'> In particular , in many tasks it is at least as important to avoid inappropriate senses than to select exactly the right one . </S>
<S ID='S-150' AZ='OWN'> This would be the case in query expansion for information retrieval , for example , where indiscriminately adding inappropriate words to a query can degrade performance <REF TYPE='P'>Voorhees 1994</REF> . </S>
<S ID='S-151' AZ='OWN'> The examples presented in Section <CREF/> are encouraging in this regard : in addition to performing well at the task of assigning a high score to the best sense , it does a good job of assigning low scores to senses that are clearly inappropriate . </S>
</P>
<P>
<S ID='S-152' AZ='OWN'> Regardless of the criterion for success , the algorithm does need further evaluation . </S>
<S ID='S-153' AZ='OWN'> Immediate plans include a larger scale version of the experiment presented here , involving thesaurus classes , as well as a similarly designed evaluation of how the algorithm fares when presented with noun groups produced by distributional clustering . </S>
<S ID='S-154' AZ='OWN'> In addition , I plan to explore alternative measures of semantic similarity , for example an improved variant on simple path length that has been proposed by <REF TYPE='A'>Leacock and Chodorow 1994</REF> . </S>
</P>
<P>
<S ID='S-155' AZ='OWN'> Ultimately , this algorithm is intended to be part of a suite of techniques used for disambiguating words in running text with respect to WordNet senses . </S>
<S ID='S-156' AZ='OWN'> I would argue that success at that task will require combining knowledge of the kind that WordNet provides , primarily about relatedness of meaning , with knowledge of the kind best provided by corpora , primarily about usage in context . </S>
<S ID='S-157' AZ='CTR'> The difficulty with the latter kind of knowledge is that , until now , the widespread success in characterizing lexical behavior in terms of distributional relationships has applied at the level of words -- indeed , word forms -- as opposed to senses . </S>
<S ID='S-158' AZ='AIM'> This paper represents a step toward getting as much leverage as possible out of work within that paradigm , and then using it to help determine relationships among word senses , which is really where the action is . </S>
</P>
</DIV>
</BODY>
<REFERENCELIST>
<REFERENCE>
<SURNAME>Basili</SURNAME>, R., <SURNAME>Pazienza</SURNAME>, M. T., and <SURNAME>Velardi</SURNAME>, P. (<DATE>1994</DATE>).
The noisy channel and the braying donkey.
In Klavans, J. and Resnik, P., editors, Proceedings of the ACL
  Workshop on Combining Symbolic and Statistical Approaches to Language (The
  Balancing Act), pages 21-28. Association for Computational Linguistics.
</REFERENCE>
<REFERENCE>
<SURNAME>Bensch</SURNAME>, P. A. and <SURNAME>Savitch</SURNAME>, W. J. (<DATE>1992</DATE>).
An occurrence-based model of word categorization.
Presented at 3rd Meeting on Mathematics of Language (MOL3).
</REFERENCE>
<REFERENCE>
<SURNAME>Brill</SURNAME>, E. (<DATE>1991</DATE>).
Discovering the lexical features of a language.
In Proceedings of the 29th Annual Meeting of the Association for
  Computational Linguistics, Berkeley, CA.
</REFERENCE>
<REFERENCE>
<SURNAME>Brown</SURNAME>, P. F., Della <SURNAME>Pietra</SURNAME>, V. <SURNAME>J</SURNAME>., deSouza, P. V., Lai, J. C., and Mercer,
  R. L. (<DATE>1992</DATE>).
Class-based n-gram models of natural language.
Computational Linguistics, 18(4):467-480.
</REFERENCE>
<REFERENCE>
<SURNAME>Church</SURNAME>, K. and <SURNAME>Hanks</SURNAME>, P. (<DATE>1989</DATE>).
Word association norms, mutual information, and lexicography.
In Proceedings of the 27th Meeting of the Association for
  Computational Linguistics.
Vancouver, B.C.
</REFERENCE>
<REFERENCE>
<SURNAME>Cowie</SURNAME>, J., <SURNAME>Guthrie</SURNAME>, J., and <SURNAME>Guthrie</SURNAME>, L. (<DATE>1992</DATE>).
Lexical disambiguation using simulated annealing.
In Proceedings of COLING-92, pages 359-365, Nantes, France.
</REFERENCE>
<REFERENCE>
<SURNAME>Grefenstette</SURNAME>, G. (<DATE>1994</DATE>).
Explorations in Automatic Thesaurus Discovery.
Kluwer.
</REFERENCE>
<REFERENCE>
<SURNAME>Hearst</SURNAME>, M. (<DATE>1991</DATE>).
Noun homograph disambiguation using local context in large corpora.
In Proceedings of the 7th Annual Conference of the University of
  Waterloo Centre for the New OED and Text Research, Oxford.
</REFERENCE>
<REFERENCE>
<SURNAME>Hearst</SURNAME>, M. and <SURNAME>Schtze</SURNAME>, H. (<DATE>1993</DATE>).
Customizing a lexicon to better suit a computational task.
In Proceedings of the ACL SIGLEX Workshop, Columbus, Ohio.
</REFERENCE>
<REFERENCE>
<SURNAME>Leacock</SURNAME>, C. and <SURNAME>Chodorow</SURNAME>, M. (<DATE>1994</DATE>).
Filling in a sparse training space for word sense identification.
ms.
</REFERENCE>
<REFERENCE>
<SURNAME>Lee</SURNAME>, J. H., <SURNAME>Kim</SURNAME>, M. H., and <SURNAME>Lee</SURNAME>, Y. J. (<DATE>1993</DATE>).
Information retrieval based on conceptual distance in IS-A
  hierarchies.
Journal of Documentation, 49(2):188-207.
</REFERENCE>
<REFERENCE>
<SURNAME>Lesk</SURNAME>, M. (<DATE>1986</DATE>).
Automatic sense disambiguation using machine readable dictionaries:
  how to tell a pine cone from an ice cream cone.
In Proceedings of the <DATE>1986</DATE> SIGDOC Conference, pages 24-26.
</REFERENCE>
<REFERENCE>
<SURNAME>Marcus</SURNAME>, M. P., <SURNAME>Santorini</SURNAME>, B., and <SURNAME>Marcinkiewicz</SURNAME>, M. (<DATE>1993</DATE>).
Building a large annotated corpus of English: the Penn
  Treebank.
Computational Linguistics, 19:313-330.
</REFERENCE>
<REFERENCE>
<SURNAME>McKeown</SURNAME>, K. and <SURNAME>Hatzivassiloglou</SURNAME>, V. (<DATE>1993</DATE>).
Augmenting lexicons automatically: Clustering semantically related
  adjectives.
In Bates, M., editor, ARPA Workshop on Human Language
  Technology.
</REFERENCE>
<REFERENCE>
<SURNAME>Miller</SURNAME>, G. (<DATE>1990</DATE>).
WordNet: An on-line lexical database.
International Journal of Lexicography, 3(4).
(Special Issue).
</REFERENCE>
<REFERENCE>
<SURNAME>Pereira</SURNAME>, F., <SURNAME>Tishby</SURNAME>, N., and <SURNAME>Lee</SURNAME>, L. (<DATE>1993</DATE>).
Distributional clustering of English words.
In Proceedings of ACL-93.
</REFERENCE>
<REFERENCE>
<SURNAME>Rada</SURNAME>, R., <SURNAME>Mili</SURNAME>, H., <SURNAME>Bicknell</SURNAME>, E., and <SURNAME>Blettner</SURNAME>, M. (<DATE>1989</DATE>).
Development and application of a metric on semantic nets.
IEEE Transaction on Systems, Man, and Cybernetics,
  19(1):17-30.
</REFERENCE>
<REFERENCE>
<SURNAME>Resnik</SURNAME>, P. (<DATE>1993</DATE>).
Selection and Information: A Class-Based Approach to Lexical
  Relationships.
PhD thesis, University of Pennsylvania.
</REFERENCE>
<REFERENCE>
<SURNAME>Resnik</SURNAME>, P. (<DATE>1995</DATE>).
Using information content to evaluate semantic similarity in a
  taxonomy.
In IJCAI-95.
</REFERENCE>
<REFERENCE>
<SURNAME>Schtze</SURNAME>, H. (<DATE>1993</DATE>).
Word space.
In Hanson, S. J., Cowan, J. D., and Giles, C. L., editors, 
  Advances in Neural Information Processing Systems 5, pages 895-902. Morgan
  Kaufmann Publishers, San Mateo CA.
</REFERENCE>
<REFERENCE>
<SURNAME>Sussna</SURNAME>, M. (<DATE>1993</DATE>).
Word sense disambiguation for free-text indexing using a massive
  semantic network.
In Proceedings of the Second International Conference on
  Information and Knowledge Management (CIKM-93), Arlington, Virginia.
</REFERENCE>
<REFERENCE>
<SURNAME>Voorhees</SURNAME>, E. M. (<DATE>1994</DATE>).
Query expansion using lexical-semantic relations.
In 17th International Conference on Research and Development in
  Information Retrieval (SIGIR '94), Dublin, Ireland.
</REFERENCE>
<REFERENCE>
<SURNAME>Yarowsky</SURNAME>, D. (<DATE>1992</DATE>).
Word-sense disambiguation using statistical models of Roget's
  categories trained on large corpora.
In Proceedings of COLING-92, pages 454-460, Nantes, France.
</REFERENCE>
</REFERENCELIST>
</PAPER>
