AIM	This paper compares a qualitative reasoning model of translation with a quantitative statistical model .	A-0
OWN	We consider these models within the context of two hypothetical speech translation systems , starting with a logic-based design and pointing out which of its characteristics are best preserved or eliminated in moving to the second , quantitative design .	A-1
OWN	The quantitative language and translation models are based on relations between lexical heads of phrases .	A-2
OWN	Statistical parameters for structural dependency , lexical transfer , and linear order are used to select a set of implicit relations between words in a source utterance , a corresponding set of relations between target language words , and the most likely translation of the original utterance .	A-3
BKG	In recent years there has been a resurgence of interest in statistical approaches to natural language processing .	S-0
BKG	Such approaches are not new , witness the statistical approach to machine translation suggested by, but the current level of interest is largely due to the success of applying hidden Markov models and N-gram language models in speech recognition .	S-1
BKG	This success was directly measurable in terms of word recognition error rates , prompting language processing researchers to seek corresponding improvements in performance and robustness .	S-2
BKG	A speech translation system , which by necessity combines speech and language technology , is a natural place to consider combining the statistical and conventional approaches and much of this paper describes probabilistic models of structural language analysis and translation .	S-3
AIM	Our aim will be to provide an overall model for translation with the best of both worlds .	S-4
OWN	Various factors will lead us to conclude that a lexicalist statistical model with dependency relations is well suited to this goal .	S-5
OWN	As well as this quantitative approach , we will consider a constraint / logic based approach and try to distinguish characteristics that we wish to preserve from those that are best replaced by statistical models .	S-6
OWN	Although perhaps implicit in many conventional approaches to translation , a characterization in logical terms of what is being done is rarely given , so we will attempt to make that explicit here , more or less from first principles .	S-7
TXT	Before proceeding , I will first examine some fashionable distinctions in sectionin order to clarify the issues involved in comparing these approaches .	S-8
TXT	I will attempt to argue that the important distinction is not so much a rational-empirical or symbolic-statistical distinction but rather a qualitative-quantitative one .	S-9
TXT	This is followed by discussion of the logic-based model in section, the overall quantitative model in section, monolingual models in section, translation models in section, and some conclusions in section.	S-10
OWN	We concentrate throughout on what information about language and translation is coded and how it is expressed as logical constraints or statistical parameters .	S-11
OWN	Although important , we will say little about search algorithms , rule acquisition , or parameter estimation .	S-12
BKG	One contrast often taken for granted is the identification of a ` statistical-symbolic ' distinction in language processing as an instance of the empirical vs. rational debate .	S-13
OWN	I believe this contrast has been exaggerated though historically it has had some validity in terms of accepted practice .	S-14
BKG	Rule based approaches have become more empirical in a number of ways :	S-15
BKG	First , a more empirical approach is being adopted to grammar development whereby the rule set is modified according to its performance against corpora of natural text.	S-16
BKG	Second , there is a class of techniques for learning rules from text , a recent example being.	S-17
BKG	Conversely , it is possible to imagine building a language model in which all probabilities are estimated according to intuition without reference to any real data , giving a probabilistic model that is not empirical .	S-18
BKG	Most language processing labeled as statistical involves associating real-number valued parameters to configurations of symbols .	S-19
BKG	This is not surprising given that natural language , at least in written form , is explicitly symbolic .	S-20
BKG	Presumably , classifying a system as symbolic must refer to a different set of ( internal ) symbols , but even this does not rule out many statistical systems modeling events involving nonterminal categories and word senses .	S-21
OWN	Given that the notion of a symbol , let alone an ` internal symbol ' , is itself a slippery one , it may be unwise to build our theories of language , or even the way we classify different theories , on this notion .	S-22
BKG	Instead , it would seem that the real contrast driving the shift towards statistics in language processing is a contrast between qualitative systems dealing exclusively with combinatoric constraints , and quantitative systems that involve computing numerical functions .	S-23
BKG	This bears directly on the problems of brittleness and complexity that discrete approaches to language processing share with , for example , reasoning systems based on traditional logical inference .	S-24
CTR	It relates to the inadequacy of the dominant theories in linguistics to capture ` shades ' of meaning or degrees of acceptability which are often recognized by people outside the field as important inherent properties of natural language .	S-25
OTH	The qualitative-quantitative distinction can also be seen as underlying the difference between classification systems based on feature specifications , as used in unification formalisms, and clustering based on a variable degree of granularity.	S-26
OWN	It seems unlikely that these continuously variable aspects of fluent natural language can be captured by a purely combinatoric model .	S-27
OWN	This naturally leads to the question of how best to introduce quantitative modeling into language processing .	S-28
OWN	It is not , of course , necessary for the quantities of a quantitative model to be probabilities .	S-29
OWN	For example , we may wish to define real-valued functions on parse trees that reflect the extent to which the trees conform to , say , minimal attachment and parallelism between conjuncts .	S-30
OTH	Such functions have been used in tandem with statistical functions in experiments on disambiguation.	S-31
OWN	Another example is connection strengths in neural network approaches to language processing , though it has been shown that certain networks are effectively computing probabilities.	S-32
BKG	Nevertheless , probability theory does offer a coherent and relatively well understood framework for selecting between uncertain alternatives , making it a natural choice for quantitative language processing .	S-33
BKG	The case for probability theory is strengthened by a well developed empirical methodology in the form of statistical parameter estimation .	S-34
BKG	There is also the strong connection between probability theory and the formal theory of information and communication , a connection that has been exploited in speech recognition , for example using the concept of entropy to provide a motivated way of measuring the complexity of a recognition problem.	S-35
BKG	Even if probability theory remains , as it currently is , the method of choice in making language processing quantitative , this still leaves the field wide open in terms of carving up language processing into an appropriate set of events for probability theory to work with .	S-36
OTH	For translation , a very direct approach using parameters based on surface positions of words in source and target sentences was adopted in the Candide system.	S-37
CTR	However , this does not capture important structural properties of natural language .	S-38
CTR	Nor does it take into account generalizations about translation that are independent of the exact word order in source and target sentences .	S-39
BKG	Such generalizations are , of course , central to qualitative structural approaches to translation,.	S-40
AIM	The aim of the quantitative language and translation models presented in sectionsandis to employ probabilistic parameters that reflect linguistic structure without discarding rich lexical information or making the models too complex to train automatically .	S-41
OWN	In terms of a traditional classification , this would be seen as a ` hybrid symbolic-statistical ' system because it deals with linguistic structure .	S-42
OWN	From our perspective , it can be seen as a quantitative version of the logic-based model because both models attempt to capture similar information ( about the organization of words into phrases and relations holding between these phrases or their referents ) , though the tools of modeling are substantially different .	S-43
OWN	We now consider a hypothetical speech translation system in which the language processing components follow a conventional qualitative transfer design .	S-44
BAS	Although hypothetical , this design and its components are similar to those used in existing database queryand translation systems.	S-45
OWN	More recent versions of these systems have been gradually taking on a more quantitative flavor , particularly with respect to choosing between alternative analyses , but our hypothetical system will be more purist in its qualitative approach .	S-46
OWN	The overall design is as follows .	S-47
OWN	We assume that a speech recognition subsystem delivers a list of text strings corresponding to transcriptions of an input utterance .	S-48
OWN	These recognition hypotheses are passed to a parser which applies a logic-based grammar and lexicon to produce a set of logical forms , specifically formulas in first order logic corresponding to possible interpretations of the utterance .	S-49
OWN	The logical forms are filtered by contextual and word-sense constraints , and one of them is passed to the translation component .	S-50
OWN	The translation relation is expressed by a set of first order axioms which are used by a theorem prover to derive a target language logical form that is equivalent ( in some context ) to the source logical form .	S-51
OWN	A grammar for the target language is then applied to the target form , generating a syntax tree whose fringe is passed to a speech synthesizer .	S-52
OWN	Taking the various components in turn , we make a note of undesirable properties that might be improved by quantitative modeling .	S-53
OWN	A grammar , expressed as a set of syntactic rules ( axioms )and a set of semantic rules ( axioms )is used to support a relation form holding between strings s and logical formsexpressed in first order logic :	S-54
OWN	The relation form is many-to-many , associating a string with linguistically possible logical form interpretations .	S-55
OWN	In the analysis direction , we are given s and search for logical forms, while in generation we search for strings s given.	S-56
OWN	For analysis and generation , we are treating strings s and logical formsas object level entities .	S-57
OWN	In interpretation and translation , we will move down from this meta-level reasoning to reasoning with the logical forms as propositions .	S-58
OWN	The list of text strings handed by the recognizer to the parser can be assumed to be ordered in accordance with some acoustic scoring scheme internal to the recognizer .	S-59
OWN	The magnitude of the scores is ignored by our qualitative language processor ; it simply processes the hypotheses one at a time until it finds one for which it can produce a complete logical form interpretation that passes grammatical and interpretation constraints , at which point it discards the remaining hypotheses .	S-60
OWN	Clearly , discarding the acoustic score and taking the first hypothesis that satisfies the constraints may lead to an interpretation that is less plausible than one derivable from a hypothesis further down in the recognition list .	S-61
OWN	But there is no point in processing these later hypotheses since we will be forced to select one interpretation essentially at random .	S-62
OWN	The syntactic rules inrelate ` category ' predicatesholding of a string and two spanning substrings ( we limit the rules here to two daughters for simplicity ) :	S-63
OWN	( Here , and subsequently , variables likeandare implicitly universally quantified . )	S-64
OWN	also includes lexical axioms for particular strings w consisting of single words :	S-65
OWN	, ...	S-66
OWN	For a feature-based grammar , these rules can include conjuncts constraining the values ,, of discrete-valued functions f on the strings :	S-67
OWN	The main problem here is that such grammars have no notion of a degree of grammatical acceptability - a sentence is either grammatical or ungrammatical .	S-68
OWN	For small grammars this means that perfectly acceptable strings are often rejected ; for large grammars we get a vast number of alternative trees so the chance of selecting the correct tree for simple sentences can get worse as the grammar coverage increases .	S-69
OWN	There is also the problem of requiring increasingly complex feature sets to describe idiosyncrasies in the lexicon .	S-70
OWN	Semantic grammar axioms belonging tospecify a ` composition ' function g for deriving a logical form for a phrase from those for its subphrases :	S-71
OWN	The interpretation rules for strings bottom out in a set of lexical semantic rules associating words with predicates () corresponding to ` word senses ' .	S-72
OWN	For a particular word and syntactic category , there will be a ( small , possibly empty ) finite set of such word sense predicates :	S-73
OWN	First order logic was assumed as the semantic representation language because it comes with well understood , if not very practical , inferential machinery for constraint solving .	S-74
OWN	However , applying this machinery requires making logical forms fine grained to a degree often not warranted by the information the speaker of an utterance intended to convey .	S-75
OWN	An example of this is explicit scoping which leads ( again ) to large numbers of alternatives which the qualitative model has difficulty choosing between .	S-76
OWN	Also , many natural language sentences cannot be expressed in first order logic without resort to elaborate formulas requiring complex semantic composition rules .	S-77
OWN	These rules can be simplified by using a higher order logic but at the expense of even less practical inferential machinery .	S-78
OWN	In applying the grammar in generation we are faced with the problem of balancing over and under-generation by tweaking grammatical constraints , there being no way to prefer fully grammatical target sentences over more marginal ones .	S-79
OWN	Qualitative approaches to grammar tend to emphasize the ability to capture generalizations as the main measure of success in linguistic modeling .	S-80
OWN	This might explain why producing appropriate lexical collocations is rarely addressed seriously in these models , even though lexical collocations are important for fluent generation .	S-81
OWN	The study of collocations for generation fits in more naturally with statistical techniques , as illustrated by.	S-82
OWN	In the logic-based model , interpretation is the process of identifying from the possible interpretationsof s for whichhold , ones that are consistent with the context of interpretation .	S-83
OWN	We can state this as follows :	S-84
OWN	Here , we have separated the context into a contingent set of contextual propositions S and a set R of ( monolingual ) ` meaning postulates ' , or selectional restrictions , that constrain the word sense predicates in all contexts .	S-85
OWN	A is a set of assumptions sufficient to support the interpretationgiven S and R .	S-86
OWN	In other words , this is ` interpretation as abduction ', since abduction , not deduction , is needed to arrive at the assumptions A .	S-87
OWN	The most common types of meaning postulates in R are those for restriction , hyponymy , and disjointness , expressed as follows :	S-88
OWN	restriction ;	S-89
OWN	hyponymy ;	S-90
OWN	disjointness .	S-91
CTR	Although there are compilation techniqueswhich allow selectional constraints stated in this fashion to be implemented efficiently , the scheme is problematic in other respects .	S-92
CTR	To start with , the assumption of a small set of senses for a word is at best awkward because it is difficult to arrive at an optimal granularity for sense distinctions .	S-93
CTR	Disambiguation with selectional restrictions expressed as meaning postulates is also problematic because it is virtually impossible to devise a set of postulates that will always filter all but one alternative .	S-94
OWN	We are thus forced to under-filter and make an arbitrary choice between remaining alternatives .	S-95
OWN	In both the quantitative and qualitative models we take a transfer approach to translation .	S-96
OWN	We do not depend on interlingual symbols , but instead map a representation with constants associated with the source language into a corresponding expression with constants from the target language .	S-97
OWN	For the qualitative model , the operable notion of correspondence is based on logical equivalence and the constants are source word sense predicatesand target sense predicates.	S-98
OWN	More specifically , we will say the translation relation between a source logical formand a target logical formholds if we have	S-99
OWN	where B is a set of monolingual and bilingual meaning postulates , and S is a set of formulas characterizing the current context .	S-100
OWN	A' is a set of assumptions that includes the assumptions A which supported.	S-101
OWN	Here bilingual meaning postulates are first order axioms relating source and target sense predicates .	S-102
OWN	A typical bilingual postulate for translating betweenandmight be of the form :	S-103
OWN	The need for the assumptions A' arises when a source language word is vaguer that its possible translations in the target language , so different choices of target words will correspond to translations under different assumptions .	S-104
OWN	For example , the conditionabove might be proved from the input logical form , or it might need to be assumed .	S-105
OWN	In the general case , finding solutions ( i.e.pairs ) for the abductive schema is an undecidable theorem proving problem .	S-106
OWN	This can be alleviated by placing restrictions on the form of meaning postulates and input formulas and using heuristic search methods .	S-107
CTR	Although such an approach was applied with some success in a limited-domain system translating logical forms into database queries, it is likely to be impractical for language translation with tens of thousands of sense predicates and related axioms .	S-108
OWN	Setting aside the intractability issue , this approach does not offer a principled way of choosing between alternative solutions proposed by the prover .	S-109
OWN	One would like to prefer solutions with ` minimal ' sets of assumptions , but it is difficult to find motivated definitions for this minimization in a purely qualitative framework .	S-110
OWN	In moving to a quantitative architecture , we propose to retain many of the basic characteristics of the qualitative model :	S-111
OWN	A transfer organization with analysis , transfer , and generation components .	S-112
OWN	Monolingual models that can be used for both analysis and generation .	S-113
OWN	Translation models that exclusively code contrastive ( cross-linguistic ) information .	S-114
OWN	Hierarchical phrases capturing recursive linguistic structure .	S-115
OWN	Instead of feature based syntax trees and first-order logical forms we will adopt a simpler , monostratal representation that is more closely related to those found in dependency grammars.	S-116
OWN	Dependency representations have been used in large scale qualitative machine translation systems , notably by.	S-117
OWN	The notion of a lexical ` head ' of a phrase is central to these representations because they concentrate on relations between such lexical heads .	S-118
OWN	In our case , the dependency representation is monostratal in that the relations may include ones normally classified as belonging to syntax , semantics or pragmatics .	S-119
OWN	One salient property of our language model is that it is strongly lexical : it consists of statistical parameters associated with relations between lexical items and the number and ordering of dependents of lexical heads .	S-120
OWN	This lexical anchoring facilitates statistical training and sensitivity to lexical variation and collocations .	S-121
OWN	In order to gain the benefits of probabilistic modeling , we replace the task of developing large rule sets with the task of estimating large numbers of statistical parameters for the monolingual and translation models .	S-122
OWN	This gives rise to a new cost trade-off in human annotation / judgement versus barely tractable fully automatic training .	S-123
OWN	It also necessitates further research on lexical similarity and clustering,to improve parameter estimation from sparse data .	S-124
OWN	The model associates phrases with relation graphs .	S-125
OWN	A relation graph is a directed labeled graph consisting of a set of relation edges .	S-126
OWN	Each edge has the form of an atomic proposition	S-127
OWN	where r is a relation symbol ,is the lexical head of a phrase andis the lexical head of another phrase ( typically a subphrase of the phrase headed by) .	S-128
OWN	The nodesandare word occurrences representable by a word and an index , the indices uniquely identifying particular occurrences of the words in a discourse or corpus .	S-129
OWN	The set of relation symbols is open ended , but the first argument of the relation is always interpreted as the head and the second as the dependent with respect to this relation .	S-130
OWN	The relations in the models for the source and target languages need not be the same , or even overlap .	S-131
OWN	To keep the language models simple , we will mainly restrict ourselves here to dependency graphs that are trees with unordered siblings .	S-132
OWN	In particular , phrases will always be contiguous strings of words and dependents will always be heads of subphrases .	S-133
OWN	Ignoring algorithmic issues relating to compactly representing and efficiently searching the space of alternative hypotheses , the overall design of the quantitative system is as follows .	S-134
OWN	The speech recognizer produces a set of word-position hypotheses ( perhaps in the form of a word lattice ) corresponding to a set of string hypotheses for the input .	S-135
OWN	The source language model is used to compute a set of possible relation graphs , with associated probabilities , for each string hypothesis .	S-136
OWN	A probabilistic graph translation model then provides , for each source relation graph , the probabilities of deriving corresponding graphs with word occurrences from the target language .	S-137
OWN	These target graphs include all the words of possible translations of the utterance hypotheses but do not specify the surface order of these words .	S-138
OWN	Probabilities for different possible word orderings are computed according to ordering parameters which form part of the target language model .	S-139
TXT	In the following section we explain how the probabilities for these various processing stages are combined to select the most likely target word sequence .	S-140
OWN	This word sequence can then be handed to the speech synthesizer .	S-141
OWN	For tighter integration between generation and synthesis , information about the derivation of the target utterance can also be passed to the synthesizer .	S-142
OWN	The probabilities associated with phrases in the above description are computed according to the statistical models for analysis , translation , and generation .	S-143
TXT	In this section we show the relationship between these models to arrive at an overall statistical model of speech translation .	S-144
OWN	We are not considering training issues in this paper , though a number of now familiar techniques ranging from methods for maximum likelihood estimation to direct estimation using fully annotated data are applicable .	S-145
OWN	The objects involved in the overall model are as follows ( we omit target speech synthesis under the assumption that it proceeds deterministically from a target language word string ) :	S-146
OWN	: ( acoustic evidence for ) source language speech	S-147
OWN	: source language word string	S-148
OWN	: target language word string	S-149
OWN	: source language relation graph	S-150
OWN	: target language relation graph .	S-151
OWN	Given a spoken input in the source language , we wish to find a target language string that is the most likely translation of the input .	S-152
OWN	We are thus interested in the conditional probability ofgiven.	S-153
OWN	This conditional probability can be expressed as follows:	S-154
OWN	We now apply some simplifying independence assumptions concerning relation graphs .	S-155
OWN	Specifically , that their derivation from word strings is independent of acoustic information ; that their translation is independent of the original words and acoustics involved ; and that target word string generation from target relation edges is independent of the source language representations .	S-156
OWN	The extent to which these ( Markovian ) assumptions hold depend on the extent to which relation edges represent all the relevant information for translation .	S-157
OWN	In particular it means they should express aspects of surface relevant to meaning , such as topicalization , as well as predicate argument structure .	S-158
OWN	In any case , the simplifying assumptions give the following :	S-159
OWN	This can be rewritten with two applications of Bayes rule :	S-160
OWN	Sinceis given ,is a constant which can be ignored in finding the maximum of.	S-161
OWN	Determiningthat maximizestherefore involves the following factors :	S-162
OWN	: source language acoustics	S-163
OWN	: source language generation	S-164
OWN	: source content relations	S-165
OWN	: source to target transfer	S-166
OWN	: target language generation .	S-167
OWN	We assume that the speech recognizer provides acoustic scores proportional to( or logs thereof ) .	S-168
OWN	Such scores are normally computed by speech recognition systems , although they are usually also multiplied by word-based language model probabilitieswhich we do not require in this application context .	S-169
TXT	Our approach to language modeling , which covers the content analysis and language generation factors , is presented in sectionand the transfer probabilities fall under the translation model of section.	S-170
OWN	Finally note that by another application of Bayes rule we can replace the two factorsbywithout changing other parts of the model .	S-171
OWN	This latter formulation allows us to apply constraints imposed by the target language model to filter inappropriate possibilities suggested by analysis and transfer .	S-172
OWN	In some respects this is similar to's approach to word sense disambiguation using statistical associations in a second language .	S-173
OWN	Our language model can be viewed in terms of a probabilistic generative process based on the choice of lexical ` heads ' of phrases and the recursive generation of subphrases and their ordering .	S-174
OWN	For this purpose , we can define the head word of a phrase to be the word that most strongly influences the way the phrase may be combined with other phrases .	S-175
OTH	This notion has been central to a number of approaches to grammar for some time , including theories like dependency grammar,and HPSG.	S-176
OTH	More recently , the statistical properties of associations between words , and more particularly heads of phrases , has become an active area of research,.	S-177
OWN	The language model factors the statistical derivation of a sentence with word string W as follows :where C ranges over relation graphs .	S-178
OWN	The content model ,, and generation model ,, are components of the overall statistical model for spoken language translation given earlier .	S-179
OWN	This decomposition ofcan be viewed as first deciding on the content of a sentence , formulated as a set of relation edges according to a statistical model for, and then deciding on word order according to.	S-180
OWN	Of course , this decomposition simplifies the realities of language production in that real language is always generated in the context of some situation S ( real or imaginary ) , so a more comprehensive model would be concerned with, i.e. language production in context .	S-181
OWN	This is less important , however , in the translation setting since we producein the context of a source relation graphand we assume the availability of a model for.	S-182
OWN	The model for deriving the relation graph of a phrase is taken to consist of choosing a lexical headfor the phrase ( what the phrase is ` about ' ) followed by a series of ` node expansion ' steps .	S-183
OWN	An expansion step takes a node and chooses a possibly empty set of edges ( relation labels and ending nodes ) starting from that node .	S-184
OWN	Here we consider only the case of relation graphs that are trees with unordered siblings .	S-185
OWN	To start with , let us take the simplified case where a head word h has no optional or duplicated dependents ( i.e. exactly one for each relation ) .	S-186
OWN	There will be a set of edges	S-187
OWN	corresponding to the local tree rooted at h with dependent nodes.	S-188
OWN	The set of relation edges for the entire derivation is the union of these local edge sets .	S-189
OWN	To determine the probability of deriving a relation graph C for a phrase headed bywe make use of parameters ( ` dependency parameters ' )for the probability , given a node h and a relation r , that w is an r-dependent of h. Under the assumption that the dependents of a head are chosen independently from each other , the probability of deriving C is :	S-190
OWN	whereis the probability of choosingto start the derivation .	S-191
OWN	If we now remove the assumption made earlier that there is exactly one r-dependent of a head , we need to elaborate the derivation model to include choosing the number of such dependents .	S-192
OWN	We model this by parameters	S-193
OWN	that is , the probability that head h has n r-dependents .	S-194
OWN	We will refer to this probability as a ` detail parameter ' .	S-195
OWN	Our previous assumption amounted to stating that this was always 1 for n = 1 or for n = 0 .	S-196
OWN	Detail parameters allow us to model , for example , the number of adjectival modifiers of a noun or the ` degree ' to which a particular argument of a verb is optional .	S-197
OWN	The probability of an expansion of h giving rise to local edgesis now :	S-198
OWN	where r ranges over the set of relation labels and h hasr-dependents.	S-199
OWN	is a combinatoric constant for taking account of the fact that we are not distinguishing permutations of the dependents ( e.g. there arepermutations of the r-dependents of h if these dependents are all distinct ) .	S-200
OWN	So ifis the root of a tree C , we have	S-201
OWN	where heads ( C ) is the set of nodes in C andis the set of edges headed by h in C .	S-202
OWN	The above formulation is only an approximation for relation graphs that are not trees because the independence assumptions which allow the dependency parameters to be simply multiplied together no longer hold for the general case .	S-203
OWN	Dependency graphs with cycles do arise as the most natural analyses of certain linguistic constructions , but calculating their probabilities on a node by node basis as above may still provide probability estimates that are accurate enough for practical purposes .	S-204
OWN	We now return to the generation model.	S-205
OWN	As mentioned earlier , since C includes the words in W and a set of relations between them , the generation model is concerned only with surface order .	S-206
OWN	One possibility is to use ` bi-relation ' parameters for the probability that an- dependent immediately follows an- dependent .	S-207
OWN	This approach is problematic for our overall statistical model because such parameters are not independent from the ` detail ' parameters specifying the number of r-dependents of a head .	S-208
OWN	We therefore adopt the use of ` sequencing ' parameters , these being probabilities of particular orderings of dependents given that the multiset of dependency relations is known .	S-209
OWN	We let the identity relation e stand for the head itself .	S-210
OWN	Specifically , we have parameters	S-211
OWN	where s is a sequence of relation labels including an occurrence of e and M ( s ) is the multiset for this sequence .	S-212
OWN	For a head h in a relation graph C , letbe the sequence of dependent relations induced by a particular word string W generated from C .	S-213
OWN	We now have	S-214
OWN	where h ranges over all the heads in C , andis the number of occurrences of r in, assuming that all orderings of- dependents are equally likely .	S-215
OWN	We can thus use these sequencing parameters directly in our overall model .	S-216
OWN	To summarize , our monolingual models are specified by :	S-217
OWN	topmost head parameters	S-218
OWN	dependency parameters	S-219
OWN	detail parameters	S-220
OWN	sequencing parameters	S-221
OWN	The overall model splits the contributions of contentordering.	S-222
OWN	However , we may also want a model for, for example for pruning speech recognition hypotheses .	S-223
OWN	Combining our content and ordering models we get :	S-224
OWN	The parameterscan be derived by combining sequencing parameters with the detail parameters for h .	S-225
OWN	As already mentioned , the translation model defines mappings between relation graphsfor the source language andfor the target language .	S-226
OWN	A direct ( though incomplete ) justification of translation via relation graphs may be based on a simple referential view of natural language semantics .	S-227
OWN	Thus nominals and their modifiers pick out entities in a ( real or imaginary ) world , verbs and their modifiers refer to actions or events in which the entities participate in roles indicated by the edge relations .	S-228
OWN	Under this view , the purpose of the translation mapping is to determine a target language relation graph that provides the best approximation to the referential function induced by the source relation graph .	S-229
OWN	We call this approximating referential equivalence .	S-230
OWN	This referential view of semantics is not adequate for taking account of much of the complexity of natural language including many aspects of quantification , distributivity and modality .	S-231
OWN	This means it cannot capture some of the subtleties that a theory based on logical equivalence might be expected to .	S-232
OWN	On the other hand , when we proposed a logic based approach as our qualitative model , we had to restrict it to a simple first order logic anyway for computational reasons , and even then it did not appear to be practical .	S-233
OWN	Thus using the more impoverished lexical relations representation may not be costing us much in practice .	S-234
OWN	One aspect of the representation that is particularly useful in the translation application is its convenience for partial and / or incremental representation of content - we can refine the representation by the addition of further edges .	S-235
OWN	A fully specified denotation of the meaning of a sentence is rarely required for translation , and as we pointed out when discussing logic representations , a complete specification may not have been intended by the speaker .	S-236
OWN	Although we have not provided a denotational semantics for sets of relation edges , we anticipate that this will be possible along the lines developed in monotonic semantics.	S-237
OWN	To be practical , a model forneeds to decompose the source and target graphsandinto subgraphs small enough that subgraph translation parameters can be estimated .	S-238
OWN	We do this with the help of ` node alignment relations ' between the nodes of these graphs .	S-239
BAS	These alignment relations are similar in some respects to the alignments used byin their surface translation model .	S-240
OWN	The translation probability is then the sum of probabilities over different alignments f :	S-241
OWN	There are different ways to modelcorresponding to different kinds of alignment relations and different independence assumptions about the translation mapping .	S-242
OWN	For our quantitative design , we adopt a simple model in which lexical and relation ( structural ) probabilities are assumed to be independent .	S-243
OWN	In this model the alignment relations are functions from the word occurrence nodes ofto the word occurrences of.	S-244
OWN	The idea is thatmeans that the source word occurrence` gave rise ' to the target word occurrence.	S-245
OWN	The inverse relationneed not be a function , allowing different numbers of words in the source and target sentences .	S-246
OWN	We decomposeinto ` lexical ' and ` structural ' probabilities as follows :	S-247
OWN	whereandare the node sets forandrespectively , andis the set of edges for the target graph .	S-248
OWN	The first factoris the lexical component in that it does not take into account any of the relations in the source graph.	S-249
OWN	This lexical component is the product of alignment probabilities for each node of:	S-250
OWN	That is , the probability that f maps exactly the ( possibly empty ) subsetofto.	S-251
OWN	These sets are assumed to be disjoint for different source graph nodes , so we can replace the factors in the above product with parameters :	S-252
OWN	where w is a source language word and M is a multiset of target language words .	S-253
OWN	We will derive a target set of edgesofby k derivation steps which partition the set of source edgesinto subgraphs.	S-254
OWN	These subgraphs give rise to disjoint sets of relation edgeswhich together form.	S-255
OWN	The structural component of our translation model will be the sum of derivation probabilities for such an edge set.	S-256
OWN	For simplicity , we assume here that the source graphis a tree .	S-257
OWN	This is consistent with our earlier assumptions about the source language model .	S-258
OWN	We take our partitions of the source graph to be the edge sets for local trees .	S-259
OWN	This ensures that the the partitioning is deterministic so the probability of a derivation is the product of the probabilities of derivation steps .	S-260
OWN	More complex models with larger partitions rooted at a node are possible but these require additional parameters for partitioning .	S-261
OWN	For the simple model it remains to specify derivation step probabilities .	S-262
OWN	The probability of a derivation step is given by parameters of the form :	S-263
OWN	whereandare unlabeled graphs andis a node alignment function fromto.	S-264
OWN	Unlabeled graphs are just like our relation edge graphs except that the nodes are not labeled with words ( the edges still have relation labels ) .	S-265
OWN	To apply a derivation step we need a notion of graph matching that respects edge labels : g is an isomorphism ( modulo node labels ) from a graph G to a graph H if g is a one-one and onto function from the nodes of G to the nodes of H such that	S-266
OWN	The derivation step with parameteris applicable to the source edges, under the alignment f , giving rise to the target edgesif	S-267
OWN	there is an isomorphismfromto	S-268
OWN	there is an isomorphismfromto	S-269
OWN	for any node v ofit must be the case that	S-270
OWN	This last condition ensures that the target graph partitions join up in a way that is compatible with the node alignment f .	S-271
OWN	The factoring of the translation model into these lexical and structural components means that it will overgenerate because these aspects are not independent in translation between real natural languages .	S-272
OWN	It is therefore appropriate to filter translation hypotheses by rescoring according to the version of the overall statistical model that included the factorsso that the target language model constrains the output of the translation model .	S-273
OWN	Of course , in this case we need to model the translation relation in the ` reverse ' direction .	S-274
OWN	This can be done in a parallel fashion to the forward direction described above .	S-275
OWN	Our qualitative and quantitative models have a similar overall structure and there are clear parallels between the factoring of logical constraints and statistical parameters , for example monolingual postulates and dependency parameters , bilingual postulates and translation parameters .	S-276
OWN	The parallelism would have been closer if we had adopted ID / LP style rulesin the qualitative model .	S-277
OWN	However , we argued in sectionthat our qualitative model suffered from lack of robustness , from having only the crudest means for choosing between competing hypotheses , and from being computationally intractable for large vocabularies .	S-278
OWN	The quantitative model is in a much better position to cope with these problems .	S-279
OWN	It is less brittle because statistical associations have replaced constraints ( featural , selectional , etc. ) that must be satisfied exactly .	S-280
OWN	The probabilistic models give us a systematic and well motivated way of ranking alternative hypotheses .	S-281
OWN	Computationally , the quantitative model lets us escape from the undecidability of logic-based reasoning .	S-282
OWN	Because this model is highly lexical , we can hope that the input words will allow effective pruning by limiting the number of search paths having significantly high probabilities .	S-283
OWN	We retained some of the basic assumptions about the structure of language when moving to the quantitative model .	S-284
OWN	In particular , we preserved the notion of hierarchical phrase structure .	S-285
OWN	Relations motivated by dependency grammar made it possible to do this without giving up sensitivity to lexical collocations which underpin simple statistical models like N-grams .	S-286
OWN	The quantitative model also reduced overall complexity in terms of the sets of symbols used .	S-287
OWN	In addition to words , it only required symbols for dependency relations , whereas the qualitative model required symbol sets for linguistic categories and features , and a set of word sense symbols .	S-288
OWN	Despite their apparent importance to translation , the quantitative system can avoid the use of word sense symbols ( and the problems of granularity they give rise to ) by exploiting statistical associations between words in the target language to filter implicit sense choices .	S-289
OWN	Finally , here is a summary of our reasons for combining statistical methods with dependency representations in our language and translation models :	S-290
OWN	inherent lexical sensitivity of dependency representations , facilitating parameter estimation ;	S-291
OWN	quantitative preference based on probabilistic derivation and translation ;	S-292
OWN	incremental and / or partial specification of the content of utterances , particularly useful in translation ;	S-293
OWN	decomposition of complex utterances through recursive linguistic structure .	S-294
OWN	These factors suggest that dependency grammar will play an increasingly important role as language processing systems seek to combine both structural and collocational information .	S-295
